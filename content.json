{"meta":{"title":"七路灯","subtitle":"人的一生应当有许多停靠站，但愿每一个站台都有一盏雾中的灯。","description":"人的一生应当有许多停靠站，但愿每一个站台都有一盏雾中的灯。","author":"七路灯","url":"http://www.lights8080.com","root":"/"},"pages":[{"title":"about","date":"2021-05-26T12:06:18.000Z","updated":"2021-06-01T10:47:46.000Z","comments":true,"path":"about/index.html","permalink":"http://www.lights8080.com/about/index.html","excerpt":"","text":"从事互联网开发，信奉终身成长的技术人。分享知识和记录成长。"},{"title":"categories","date":"2021-05-26T12:04:49.000Z","updated":"2021-06-01T10:47:46.000Z","comments":true,"path":"categories/index.html","permalink":"http://www.lights8080.com/categories/index.html","excerpt":"","text":""},{"title":"contact","date":"2021-05-26T12:06:42.000Z","updated":"2021-06-01T10:47:46.000Z","comments":true,"path":"contact/index.html","permalink":"http://www.lights8080.com/contact/index.html","excerpt":"","text":""},{"title":"tags","date":"2021-05-26T12:05:59.000Z","updated":"2021-06-01T10:47:46.000Z","comments":true,"path":"tags/index.html","permalink":"http://www.lights8080.com/tags/index.html","excerpt":"","text":""}],"posts":[{"title":"2108-4-企业文化","slug":"weekly/2108-4-企业文化","date":"2021-08-26T16:00:00.000Z","updated":"2021-09-02T02:03:13.000Z","comments":true,"path":"p/7fc5d86f.html","link":"","permalink":"http://www.lights8080.com/p/7fc5d86f.html","excerpt":"如题","text":"如题 话题：企业文化相信大部分人都非常向往大厂的企业文化吧。比如：扁平化管理、没有考勤管理和休假制度、没有主管和考核、不建立统一技术规范、没有测试团队等。天啊！很难想象这样的制度，公司怎样运营？ 那大厂为什么会这样，且还能运营的很好？又是如何组织上万人朝着同一个目标前进呢。我觉得关键是公司里是一群什么样的人。大厂的竞争实际是人才的竞争，而公司的企业文化就是要保证吸引和留住人才。”“”对于程序型的工作，顶级员工的输出量是一般员工的2倍。对于创新型/创意型的工作，顶级员工的输出量是一般员工的10倍!”“” 工程师人天生带有一种工匠式的图腾崇拜精神，奉行达者为师，不迷信管理他们的人，但充分尊重能够指导他们的人。他们都非常的自律性，有目标，有责任感。企业里有这样一群优秀的人，还需要管理吗？ 优秀的人从来不怕麻烦，如果是一个好的方案，大家就会趋同。所以不设立统一的技术规范，反而进化迭代出更好的规范。 公司的目标靠产品驱动，落实到人，分配给团队，团队就需要对自己做出来的东西负责任。团队内有不同意见时，听谁的呢？那就是谁对这个产品负责就听谁的，一旦负责人决定了，团队内不同意见的人，要迅速调整。 能力差的，不认可公司文化和价值观的人，最终会被淘汰掉，或主动离职。所以企业文化和有这样一群人是密不可分的。有这样的文化，没有这样一群优秀的人，是支撑不了企业成长的；没有这样的文化，即使有这样一群优秀的人，最终也会人才流失。大厂为啥热衷于要搞开源？互联网企业技术就是核心价值，这不是帮助同业成长吗？核心还是建立行业标准和最佳实践，建立技术品牌，留住和吸引顶级工程师，并从共享生态中获得反馈输入受益。 多美好，相辅相成，但小厂照搬过来，大概率是行不通的。原因是小厂没有能力招到一群优秀的人。基本都是一个大牛，带一堆小兵。是小兵也就意味着能力水平和意识，都达不到水平，只能靠管理，靠规范，靠大牛的指导，才能朝着目标前进。 语录 如果你想造一艘船，先不要雇人去收集木头， 也不要给他们分配任何任务，而是去激发他们对浩瀚汪洋的渴望。","categories":[{"name":"weekly","slug":"weekly","permalink":"http://www.lights8080.com/categories/weekly/"}],"tags":[{"name":"weekly","slug":"weekly","permalink":"http://www.lights8080.com/tags/weekly/"}]},{"title":"2108-3-从NBA看职场","slug":"weekly/2108-3-从NBA看职场","date":"2021-08-19T16:00:00.000Z","updated":"2021-09-02T02:03:13.000Z","comments":true,"path":"p/418eb76d.html","link":"","permalink":"http://www.lights8080.com/p/418eb76d.html","excerpt":"如题","text":"如题 话题：从NBA看职场球员背景： 施罗德：市场上没有比施罗德更适合湖人的首发控卫了，从赛季中期湖人的报价来看，确实把施罗德当第三巨头来看待了。施罗德也正是看中了这点，要求球队首发和顶薪，拒绝湖人平均年薪2000w报价的资本。 霍华德：19-20赛季湖人获得NBA总冠军，霍华德在夺冠道路上也发挥了应有价值，选择续约湖人是自然的事情。湖人从市场签约了小加索尔和哈雷尔，账目实力远大于霍华德，霍华德上场时间被挤没了，只能离队最终一年签约76人。 库兹马+波普：库兹马、波普都处于合同年，因为19-20赛季在季后赛中发挥作用，提升了市场价值。湖人分别签了平均年薪超千万的长约。但上赛季表现中规中矩，没有达到预期。 我有时候会想如果我是湖人总经理，面对这个局面，我该如何操作呢？尤其是对施罗德的续约上，顶薪明显是溢价合同，但是市场上又没有更好的选择了。放走施罗德湖人后卫就是问题？ 下面来看看湖人总经理的操作：1。 果断放弃表现不好的球员（库兹马+波普+哈雷尔），打包交易换来即战力很强的威少，组成三巨头，从侧面直接解决了施罗德的难题。而施罗德，最终一年590w签约凯尔特人。 毫不犹豫的重新签回霍华德。从上赛季的表现来看，去年的操作是失败的，小加索尔和哈雷尔和球队的化学反应并不好。 依靠巨星光环和总冠军，吸引优质球员底薪签约。 语录 当你有选择的时候，选择更重要；而当你没有选择的时候，努力才重要。努力是为了让你有更好的选择。 人不是因为变老了才没有热情，而是因为你没有热情才变老了。 成年人面对的第一个挑战就是没时间，我们需要解决的问题就是，在资源匮乏的情况下，还能够把事情做成。","categories":[{"name":"weekly","slug":"weekly","permalink":"http://www.lights8080.com/categories/weekly/"}],"tags":[{"name":"weekly","slug":"weekly","permalink":"http://www.lights8080.com/tags/weekly/"}]},{"title":"2108-2-K12双减的深度思考","slug":"weekly/2108-2-K12双减的深度思考","date":"2021-08-12T16:00:00.000Z","updated":"2021-09-02T02:03:13.000Z","comments":true,"path":"p/197f72b3.html","link":"","permalink":"http://www.lights8080.com/p/197f72b3.html","excerpt":"如题","text":"如题 话题：K12双减的深度思考认真观察身边的朋友同事，有一个现象比较明显。肯吃苦努力工作的人基本都是穷人家庭出身，而那些对工作没有什么追求的人往往家庭条件比较优越。 都市剧《谁说我结不了婚》中有这样一段对话，我感触很深。”“”许海峰：至于这么拼吗？田雷：我没有光鲜的学历，也没有丰厚的背景，我从实习生做起，一步一步拼到现在，才走到现在的位置。我拼尽全力，也只是你的起点而已。”“” 我是农村出身，有时候真的会觉得很气，凭什么啊！低收入家庭要想跃迁到中产阶级需要付出非常大的努力，而且不一定就能成功。而中产阶级家的孩子可以很轻松的享受并维持这个水平。 这也是年轻人躺平的原因吧，没有晋升希望。 贫富差距大，穷人太多，晋升的机会小，从宏观角度看，这是资本分配的问题。 经济内循环，关键还是要看中产阶级力量。中产阶级才是消费主力军，内循环的驱动力。（有钱的人占比太少拉动不起来，低收入家庭没钱去拉动，中产阶级是有欲望和有能力消费的群体） 20年统计，国家6亿人每月收入1000元。实现内循环，现存的中产阶级还不够。根本的解决办法，是让穷人变富。如何变富？ - 靠自己。 国家层面要做的就是，尽可能保障赛道公平，让那些有天赋，肯吃苦，努力的人更容易实现阶层跃迁，给更多的低收入家庭以希望。不能因为出身家庭财富和其他后天的限制就剥夺了成功的机会。要让后来人看到希望，带动更多的人步入中产。 K12双减，明面上是配合三胎政策，降低养育成本，实则是对中产阶级的降维打击。强行拉平中产阶级家庭孩子和低收入家庭孩子到同一起跑线（低收入家庭没有经济能力上辅导班，那就让中产阶级家庭的孩子没有课外辅导班可上），降低底层民众晋升中产阶级的难度。 K12双减政策这只是国家层面调整社会资本结构的一部分。要求给外卖小哥和快递员上社保、加强反垄断等等，你细品其实都有其影子。 回到开头的所说的内容，年轻人躺平。国家正在极力扭转这种局面，尽可能降低底层民众晋升中产阶级的难度，靠自己的努力实现富裕，去卷中产阶级。促进中产阶级的孩子努力去卷更高的阶级。 生病了，药再苦也得吃啊，总不能等到无药可救吧。 要求给外卖小哥和快递员上社保，平台给骑手买社保，平台的利润减少了，最终肯定会转移到骑手和商家上。外卖提价，高端的店留在平台，低端的店退出；优秀的骑手收入会增加，大量混日子的骑手退出；有钱人外卖贵点也会点，没钱人没钱点外卖了，去线下小店吃吧。 反垄断，会让资本更多的流向中小企业，中小企业是吸纳就业的重要力量。","categories":[{"name":"weekly","slug":"weekly","permalink":"http://www.lights8080.com/categories/weekly/"}],"tags":[{"name":"weekly","slug":"weekly","permalink":"http://www.lights8080.com/tags/weekly/"}]},{"title":"凤凰架构-透明多层分流系统","slug":"读书笔记/凤凰架构/凤凰架构-透明多层分流系统","date":"2021-08-08T16:00:00.000Z","updated":"2021-08-10T01:40:00.000Z","comments":true,"path":"p/fd2df2c4.html","link":"","permalink":"http://www.lights8080.com/p/fd2df2c4.html","excerpt":"周志明《凤凰架构：构建可靠的大型分布式系统》https://icyfenix.cn/ 包括：域名解析，客户端缓存，传输链路，传输压缩，内容分发，负载均衡","text":"周志明《凤凰架构：构建可靠的大型分布式系统》https://icyfenix.cn/ 包括：域名解析，客户端缓存，传输链路，传输压缩，内容分发，负载均衡 透明多级分流系统的设计原则： 第一个原则是尽可能减少单点部件，如果某些单点是无可避免的，则应尽最大限度减少到达单点部件的流量。 第二个原则是奥卡姆剃刀原则，它更为关键。”如无必要，勿增实体“ 域名解析(DNS)以 www.icyfenix.com.cn 为例，介绍DNS把域名解析成IP地址的过程： 客户端先检查本地的 DNS 缓存，查看是否存在并且是存活着的该域名的地址记录。 客户端将地址发送给本机操作系统中配置的本地 DNS（Local DNS） 本地 DNS 收到查询请求后，会按照顺序依次查找地址记录，“ www.icyfenix.com.cn 的权威服务器”→“ icyfenix.com.cn 的权威服务器”→“ com.cn 的权威服务器”→“cn 的权威服务器”→“根域名服务器“。 现在假设本地 DNS 是全新的，上面不存在任何域名的权威服务器记录，当 DNS 查询请求一直查到根域名服务器之后，会得到“cn 的权威服务器”的地址记录，然后通过“cn 的权威服务器”，得到“com.cn 的权威服务器”的地址记录，以此类推，最后找到能够解释 www.icyfenix.com.cn 的权威服务器地址。 通过“www.icyfenix.com.cn 的权威服务器”，查询 www.icyfenix.com.cn 的地址记录。 DNS 的分级查询都有可能受到中间人攻击的威胁，产生被劫持的风险。 HTTPDNS（也称为 DNS over HTTPS，DoH）：它把原本的 DNS 解析服务开放为一个基于 HTTPS 协议的查询服务，替代基于 UDP 传输协议的 DNS 域名解析，通过程序代替操作系统直接从权威 DNS，或者可靠 Local DNS 获取解析数据，从而绕过传统 Local DNS。 “切勿浪费较多东西，去做‘用较少的东西，同样可以做好的事情’。”手上有个新锤子，看啥都是对口的钉子。 客户端缓存分为强制缓存和协商缓存，这两套机制是并行工作。当强制缓存超过时效或者被禁用，协商缓存也仍然可以正常工作。 强制缓存根据资源的修改时间进行检查 ExpiresExpires 是 HTTP/1.0 协议中开始提供的 Header，后面跟随了一个截止时间参数 HTTP/1.1 200 OKExpires: Wed, 8 Apr 2020 07:28:00 GMT 问题： 受限于客户端的本地时间 无法处理涉及到用户身份的私有资源 无法描述“不缓存”的语义 Cache-ControlCache-Control 是 HTTP/1.1 协议中定义的强制缓存 Header，与 Expires 同时存在，且语义存在冲突时，IETF 规定必须以 Cache-Control 为准。 HTTP/1.1 200 OKCache-Control: max-age=600 max-age：相对于请求时间多少秒内，缓存是有效的 s-maxage：“共享缓存”的有效时间，即允许被 CDN、代理等持有的缓存有效时间 public：意味着资源可以被代理、CDN 等缓存 private：就意味着只能由用户的客户端进行私有缓存 no-cache：表明该资源不应该被缓存，哪怕是同一个会话中对同一个 URL 地址的请求 no-store：不强制会话中是否重复获取相同的 URL 资源，禁止浏览器、CDN 等以任何形式保存该资源 no-transform：禁止资源以任何形式被修改，包括Content-Encoding、Content-Range、Content-Type min-fresh：仅用于客户端的请求 Header，用于建议服务器能返回一个不少于该时间的缓存资源 only-if-cached：表示客户端只会接受代理缓存，而不会接受源服务器的响应。如果代理缓存无效，就直接返回 503/Service Unavailable 错误 must-revalidate：表示在资源过期后，一定要从服务器中进行获取 proxy-revalidate：用于提示代理、CDN 等设备资源过期后的缓存行为，语义与 must-revalidate 完全一致 协商缓存根据资源唯一标识是否发生变化来进行检查 根据资源的修改时间进行检查Last-Modified：服务器的响应 Header，用来告诉客户端这个资源的最后修改时间If-Modified-Since：客户端再次请求时，会通过 If-Modified-Since，把之前收到的资源最后修改时间发送回服务端 服务端发现资源在该时间后没有被修改过，就只要返回一个 304/Not Modified 的响应即可 123HTTP/1.1 304 Not ModifiedCache-Control: public, max-age=600Last-Modified: Wed, 8 Apr 2020 15:31:30 GMT 根据资源唯一标识是否发生变化来进行检查Etag：是服务器的响应 Header，用于告诉客户端这个资源的唯一标识If-None-Match：当客户端需要再次请求时，就会通过 If-None-Match，把之前收到的资源唯一标识发送回服务端 传输链路前端网页的优化技巧 最少请求数量：TCP连接开销很大，解决手段有雪碧图，文件合并，媒体内联等 扩大并发请求数：现代浏览器一般支持6个并发请求，解决手段域名分片 启用压缩传输：减少网络传输内容的大小 避免页面重定向：页面发生了重定向，就会延迟整个文档的传输 按重要性调节资源优先级：对客户端展示影响大的资源，放在 HTML 的头部，以便优先下载 … 因为 HTTP 协议还在持续发展，这些优化技巧可能会成为反模式。HTTP/3以前是以 TCP 为传输层的应用层协，TCP 协议本身是面向长时间、大数据传输来设计的。而每个页面包含的资源（HTML、CSS、JS、图片等）的特征是，数量多，资源小。以至于 HTTP/1.x 时代，大量短而小的 TCP 连接导致了网络性能的瓶颈。 Keep-Alive 机制（HTTP/1.0 中不是默认开启的，HTTP/1.1 中变为默认）原理是让客户端对同一个域名长期持有一个或多个不会用完即断的 TCP 连接。客户端维护一个 FIFO 队列，每次取完数据之后的一段时间内，不自动断开连接，下一个资源时可以直接复用，避免创建 TCP 连接的成本。 副作用：队首阻塞，首个资源是一个复杂的请求，导致后面的请求必须阻塞等待。 HTTP/2 的多路复用技术在 HTTP/1.x 中，HTTP 请求就是传输过程中最小粒度的信息单位，如果将多个请求切碎，再混杂在一块传输，客户端难以分辨重组出有效信息。而在 HTTP/2 中，帧（Frame）才是最小粒度的信息单位。它可以用来描述各种数据，比如请求的 Headers、Body，或者是用来做控制标识，如打开流、关闭流。流（Stream）是一个逻辑上的数据通道概念，每个帧都附带有一个流 ID，以标识这个帧属于哪个流。 多路复用的支持，HTTP/2 就可以对每个域名只维持一个 TCP 连接，开发者也不用去考虑并发请求数限制，客户端就不需要再刻意压缩 HTTP 请求。 在 HTTP 传输中，Headers 占传输成本的比重是相当地大，HTTP/2 中专门考虑如何进行 Header 压缩的问题，同一个连接上产生的请求和响应越多，头部压缩效果也就越好。所以 HTTP/2 是单域名单连接的机制，合并资源和域名分片反而对性能提升不利。 与 HTTP/1.x 相反，HTTP/2 本身反而变得更适合传输小资源。 传输压缩当客户端可以接受压缩版本的资源时（请求的 Header 中包含 Accept-Encoding: gzip），就返回压缩后的版本（响应的 Header 中包含 Content-Encoding: gzip），否则就返回未压缩的原版。 静态预压缩在网络时代的早期，服务器的处理能力还很薄弱，为了启用压缩，会把静态资源预先压缩为.gz 文件的形式给存放起来。 即时压缩现代的 Web 服务器处理能力有了大幅提升，整个压缩过程全部在内存的数据流中完成，不必等资源压缩完成再返回响应，这样可以显著提高首字节时间，改善 Web 性能体验。 在 HTTP/1.0 时，资源结束判断的机制，只有根据 Content-Length 判断。但即时压缩时，服务器再也没有办法给出 Content-Length 这个响应 Header 了。所以，如果是 HTTP/1.0 的话，持久连接和即时压缩只能二选其一。在 HTTP/1.0 中这两者都支持，却默认都是不启用。 分块编码HTTP/1.1中，增加了另一种资源结束判断的机制，“分块传输编码”（Chunked Transfer Encoding）。 工作原理：在响应 Header 中加入“Transfer-Encoding: chunked”之后，就代表这个响应报文将采用分块编码。此时，报文中的 Body 需要改为用一系列“分块”来传输。每个分块包含十六进制的长度值和对应长度的数据内容，长度值独占一行，数据从下一行开始。最后以一个长度值为 0 的分块，来表示资源结束。 快速 UDP 网络连接想从根本上改进 HTTP，就必须直接替换掉 HTTP over TCP 的根基，即 TCP 传输协议。2018 年末，IETF 正式批准了 HTTP over QUIC 使用 HTTP/3 的版本号，它会以 UDP 协议作为基础。 内容分发网络(CDN)CDN 其实就是做“内容分销”工作的。 内容分发网络的工作过程，主要涉及到路由解析、内容分发、负载均衡和它所能支持的应用内容四个方面。 仅从网络传输的角度来看，一个互联网系统的速度快慢，主要取决于以下四点因素： 网站服务器接入网络运营商的链路所能提供的出口带宽。 用户客户端接入网络运营商的链路所能提供的入口带宽。 从网站到用户之间，经过的不同运营商之间互联节点的带宽。 从网站到用户之间的物理链路传输时延。 路由解析 内容分发无论是对用户还是服务器，内容分发网络都可以是完全透明的，CDN需要解决两个问题：“如何获取源站资源”和“如何管理（更新）资源”。 第一种：主动分发（Push）主动分发就是由源站主动发起，将内容从源站或者其他资源库推送到用户边缘的各个 CDN 缓存节点上。 通常需要源站、CDN 服务双方提供的程序 API 接口层面的配合。 第二种：被动回源（Pull）由用户访问所触发的全自动、双向透明的资源缓存过程。当某个资源首次被用户请求的时候，CDN 缓存节点如果发现自己没有该资源，就会实时从源站中获取。 可以做到完全的双向透明，不需要源站在程序上做任何的配合，使用起来非常方便。 如何管理（更新）资源对于Cache-Control的 s-maxage，是否遵循，完全取决于 CDN 本身的实现策略。 CDN 缓存的管理没有通用的准则，最常见的管理（更新）资源的做法是超时被动失效与手工主动失效相结合。 CDN 应用 加速静态资源 安全防御，DDoS攻击 协议升级，https，IPv6 状态缓存 修改资源，给源站不支持跨域的资源提供跨域能力 访问控制，QoS控制，referer防盗链 注入功能，Google Analytics等 负载均衡“负载均衡器”（Load Balancer）：承担了调度后方的多台机器，以统一的接口对外提供服务的技术组件。 从形式上来说都可以分为两种：四层负载均衡和七层负载均衡。四层负载均衡的优势是性能高，七层负载均衡的优势是功能强。 层 数据单元 功能 7 应用层 Application Layer 数据Data 提供为应用软件提供服务的接口，用于与其他应用软件之间的通信。典型协议：HTTP、HTTPS、FTP、Telnet、SSH、SMTP、POP3 等 6 表达层Presentation Layer 数据 Data 把数据转换为能与接收者的系统格式兼容并适合传输的格式。 5 会话层 Session Layer 数据 Data 负责在数据传输中设置和维护计算机网络中两台计算机之间的通信连接。 4 传输层 Transport Layer 数据段Segments 把传输表头加至数据以形成数据包。传输表头包含了所使用的协议等发送信息。典型协议：TCP、UDP、RDP、SCTP、FCP 等 3 网络层 Network Layer 数据包Packets 决定数据的传输路径选择和转发，将网络表头附加至数据段后以形成报文（即数据包）。典型协议：IPv4/IPv6、IGMP、ICMP、EGP、RIP 等 2 数据链路层 Data Link Layer 数据帧Frame 负责点对点的网络寻址、错误侦测和纠错。当表头和表尾被附加至数据包后，就形成数据帧（Frame）。典型协议：WiFi（802.11）、Ethernet（802.3）、PPP 等。 1 物理层Physical Layer 比特流Bit 在局域网上传送数据帧，它负责管理电脑通信设备和网络媒体之间的互通。包括了针脚、电压、线缆规范、集线器、中继器、网卡、主机接口卡等。 四层负载均衡四层负载均衡的工作模式都属于“转发”，即直接将承载着 TCP 报文的底层数据格式（IP 数据包或以太网帧），转发到真实服务器上，此时客户端到响应请求的真实服务器维持着同一条 TCP 通道。 数据链路层负载均衡数据链路层传输的内容是数据帧（Frame），只需要注意到“MAC 目标地址”和“MAC 源地址”两项即可。 工作原理：修改请求的数据帧中的 MAC 目标地址，让用户原本是发送给负载均衡器的请求的数据帧，被二层交换机根据新的 MAC 目标地址，转发到服务器集群中，对应的服务器的网卡上，这样真实服务器就获得了一个原本目标并不是发送给它的数据帧。 数据链路层负载均衡的工作模式是，只有请求会经过负载均衡器，而服务的响应不需要从负载均衡器原路返回，整个请求、转发、响应的链路形成了一个“三角关系”，又叫“三角传输模式”，“单臂模式”，“直接路由”。 二层负载均衡器工作原理决定了，它们必须位于同一个子网当中，无法跨 VLAN。这个优势（效率高）和劣势（不能跨子网）就共同决定了，数据链路层负载均衡最适合用来做数据中心的第一级均衡设备，用来连接其他的下级负载均衡器。 网络层负载均衡网络层传输的单位是分组数据包（Packets），只要知道在 IP 分组数据包的 Headers 带有源和目标的 IP 地址即可。 第一种：保持原来的数据包不变，新创建一个数据包。(IP隧道)优点： 并没有修改原有数据包中的任何信息，仍然具备三角传输特性 IP 隧道工作在网络层，所以可以跨越 VLAN 缺点： 真实服务器收到数据包后，必须在接收入口处，设计拆包机制。(几乎所有Linux系统都支持IP隧道协议) 必须保证所有的真实服务器与均衡器有着相同的虚拟 IP 地址 第二种：改变目标数据包，直接把数据包 Headers 中的目标地址改掉(NAT 模式)NAT模式：充当了家里、公司、学校的上网路由器的作用。 优点： 没有经过 IP 隧道的额外包装，无需再拆包了 彻底的透明，真实服务器连网关都不需要配置，均衡器在转发时不仅修改目标 IP 地址，连源 IP 地址也一起改了 缺点： 不具备三角传输特性，必须回到负载均衡，改回自己的IP，再发给客户端。流量压力比较大时，带来较大的性能损失 真实服务器处理请求时就无法拿到客户端的 IP 地址 应用层负载均衡工作在四层之后的负载均衡模式就无法再进行转发了，只能进行代理。此时正式服务器、负载均衡器、客户端三者之间，是由两条独立的 TCP 通道来维持通讯的。 分类： 正向代理就是我们通常简称的代理，意思就是在客户端设置的、代表客户端与服务器通讯的代理服务。它是客户端可知，而对服务器是透明的。 反向代理是指设置在服务器这一侧，代表真实服务器来与客户端通讯的代理服务。此时它对客户端来说是透明的。 透明代理是指对双方都透明的，配置在网络中间设备上的代理服务。比如，架设在路由器上的透明翻墙代理。 七层负载均衡器，不能去做下载站、视频站这种流量应用，起码不能作为第一级均衡器。缺点：网络性能比不过四层负载均衡器，多一轮TCP握手，还有 NAT 转发模式一样的带宽问题优点：可以感知应用层通讯的具体内容，往往能够做出更明智的决策 应用： 缓存方面的工作，比如静态资源缓存、协议升级、安全防护、访问控制等 更智能化的路由，比如Session 路由、亲和性集群、URL路由、根据用户路由等 某些安全攻击可以由七层负载均衡器来抵御，比如DDoS 手段是 SYN Flood 攻击 链路治理措施，比如服务降级、熔断、异常注入等 … 均衡策略与实现均衡策略与实现 轮循均衡 权重轮循均衡 随机均衡 权重随机均衡 一致性哈希均衡 响应速度均衡 最少连接数均衡 负载均衡器的实现有“软件均衡器”和“硬件均衡器”两类。 软件均衡器又分为直接建设在操作系统内核的均衡器和应用程序形式的均衡器两种。前者的代表是 LVS（Linux Virtual Server），后者的代表有 Nginx、HAProxy、KeepAlived，等等；前者的性能会更好，因为它不需要在内核空间和应用空间中来回复制数据包；而后者的优势是选择广泛，使用方便，功能不受限于内核版本。 硬件均衡器，往往会直接采用应用专用集成电路来实现。因为它有专用处理芯片的支持，可以避免操作系统层面的损耗，从而能够达到最高的性能。这类的代表就是著名的 F5 和 A10 公司的负载均衡产品。","categories":[{"name":"读书","slug":"读书","permalink":"http://www.lights8080.com/categories/%E8%AF%BB%E4%B9%A6/"},{"name":"技术","slug":"读书/技术","permalink":"http://www.lights8080.com/categories/%E8%AF%BB%E4%B9%A6/%E6%8A%80%E6%9C%AF/"}],"tags":[{"name":"凤凰架构","slug":"凤凰架构","permalink":"http://www.lights8080.com/tags/%E5%87%A4%E5%87%B0%E6%9E%B6%E6%9E%84/"}]},{"title":"2108-1-什么才是给孩子最好的礼物","slug":"weekly/2108-1-什么才是给孩子最好的礼物","date":"2021-08-05T16:00:00.000Z","updated":"2021-09-02T02:03:13.000Z","comments":true,"path":"p/f438d8cc.html","link":"","permalink":"http://www.lights8080.com/p/f438d8cc.html","excerpt":"如题","text":"如题 话题：什么才是给孩子最好的礼物如果说“给孩子最好的礼物是陪伴”，相信大部分人都不会反驳，且还能说出一大堆的理由来。 我试着脑补了一下，在孩子的不同时期，问他俩个问题：“”“孩子10岁，你问他什么最重要？他会回答是陪伴；20岁问他什么最重要，他会说钱；你再问快乐的童年重要吗，他会想想说也重要。30岁问他什么最重要，他会告诉你是环境和成长经历；快乐的童年重要吗，没那么重要。“”“ 如果在读的你现在已经三十几岁已经有孩子，对于上面这个回答并不排斥吧 为什么家长从小就给孩子报各种补习班？那是因为我们都知道相比于快乐的童年，孩子的未来更重要，孩子将来长大了一定不会怪我。 我们都很想时刻陪在孩子身边，但是总得想想孩子20岁，30岁时最需要的是什么。真的在乎童年的陪伴吗？ 我这样说，并不是我认为小时候对孩子的陪伴不重要。相反，我也认为很重要，但我想表达的是陪伴的重要性是要建立在生长环境、成长经历和财务支持等基础之上的。 如果你有能力把这些都平衡好，那真的很完美，对于普通人而言，必须有所取舍。 很遗憾没有给你美好的童年，如果现在有机会能让你在20岁，30岁时多一点点的选择，那就够了。 这个思考来源于，最近二姐一家五口驱车从天津出发开往西藏。一次说走就走的旅行，这一定是很多人羡慕和向往的生活方式。 在这之前说实话我对姐夫有一定的偏见，而且是很固执的，根本原因就是没有对家庭的陪伴。 “我也想天天在家里陪着他们，你知道仨孩子上学要多少钱，将来出国留学，结婚要多少钱。我不出去挣钱，日子怎么过，她们不理解我，咱俩是男的，我以为你能理解我”上面这句话是我俩单独聊天他说的（并非原话，意思一样）。 说实话，当时我并没有理解。但是随着年龄的增长，我越来越能get到他的点。抛去那些分支乱插，从现在的结果来看，我觉得他并没有敷衍，而且正在一步一步的践行自己的承诺。 初心未变，负重前行。","categories":[{"name":"weekly","slug":"weekly","permalink":"http://www.lights8080.com/categories/weekly/"}],"tags":[{"name":"weekly","slug":"weekly","permalink":"http://www.lights8080.com/tags/weekly/"}]},{"title":"凤凰架构-分布式事务","slug":"读书笔记/凤凰架构/凤凰架构-分布式事务","date":"2021-08-04T16:00:00.000Z","updated":"2021-08-05T10:55:49.000Z","comments":true,"path":"p/c901dee9.html","link":"","permalink":"http://www.lights8080.com/p/c901dee9.html","excerpt":"周志明《凤凰架构：构建可靠的大型分布式系统》https://icyfenix.cn/ 包括：CAP、BASE理论。实现分布式事务的三种解决方案，可靠消息队列、TCC、SAGA事务。 可靠消息队列：简单、无法解决隔离性问题TCC：编码实现业务隔离性，但要求技术可控性（预留资源接口），硬编码可以使用SeataSAGA事务：编码实现业务隔离性和补偿机制，不要求技术可控性，硬编码可以使用Seata","text":"周志明《凤凰架构：构建可靠的大型分布式系统》https://icyfenix.cn/ 包括：CAP、BASE理论。实现分布式事务的三种解决方案，可靠消息队列、TCC、SAGA事务。 可靠消息队列：简单、无法解决隔离性问题TCC：编码实现业务隔离性，但要求技术可控性（预留资源接口），硬编码可以使用SeataSAGA事务：编码实现业务隔离性和补偿机制，不要求技术可控性，硬编码可以使用Seata 分布式事务（Distributed Transaction）特指多个服务同时访问多个数据源的事务处理机制。 CAP分布式系统（distributed system）的最大难点，就是各个节点的状态如何同步。CAP 定理是这方面的基本定理。 一致性（Consistency）：代表在任何时刻、任何分布式节点中所看到的数据都是符合预期的。 可用性（Availability）：代表系统不间断地提供服务的能力。密切相关两个指标：可靠性（平均无故障时间：MTBF）和可维护性（平均可修复时间：MTTR）。 分区容忍性（Partition Tolerance）：代表分布式环境中部分节点因网络原因而彼此失联（即与其他节点形成“网络分区”）时，系统仍能正确地提供服务的能力。 如果放弃分区容忍性（CA without P），意味着我们将假设节点之间通信永远是可靠的。永远可靠的通信在分布式系统中必定不成立的。主流的 RDBMS（关系数据库管理系统）集群通常就是采用放弃分区容错性的工作模式。 如果放弃可用性（CP without A），意味着我们将假设一旦网络发生分区，节点之间的信息同步时间可以无限制地延长，此时，问题相当于退化到前面“全局事务”中讨论的一个系统使用多个数据源的场景之中，我们可以通过 2PC/3PC 等手段，同时获得分区容忍性和一致性。著名的 HBase 也是属于 CP 系统。 如果放弃一致性（AP without C），意味着我们将假设一旦发生分区，节点之间所提供的数据可能不一致。选择放弃一致性的 AP 系统目前是设计分布式系统的主流选择，因为 P 是分布式网络的天然属性，而 A 通常是建设分布式的目的。目前大多数 NoSQL 库和支持分布式的缓存框架都是 AP 系统，如Redis集群。 CAP、ACID 中讨论的一致性称为“强一致性”（Strong Consistency）。把牺牲了 C 的 AP 系统，叫做“最终一致性”（Eventual Consistency）。它是指，如果数据在一段时间内没有被另外的操作所更改，那它最终将会达到与强一致性过程相同的结果，有时候面向最终一致性的算法，也被称为“乐观复制算法”。 BASE理论BASE是对CAP中一致性和可用性权衡的结果，其来源于对大规模互联网系统分布式实践的结论，是基于CAP定理逐步演化而来的，其核心思想是即使无法做到强一致性，但每个应用都可以根据自身的业务特点，采用适当的方式来使系统达到最终一致性。 BASE 分别是基本可用性（Basically Available）、柔性事务（Soft State）和最终一致性（Eventually Consistent）的缩写。 基本可用性（Basically Available）：分布式系统在出现故障时，允许损失部分可用性，即保证核心可用。 柔性事务（Soft State）：允许系统存在中间状态，而该中间状态不会影响系统整体可用性。这里的中间状态就是 CAP 理论中的数据不一致。 最终一致性（Eventually Consistent）：系统中的所有数据副本经过一定时间后，最终能够达到一致的状态。 可靠消息队列前面这种靠着持续重试来保证可靠性的操作，在计算机中就非常常见，它有个专门的名字，叫做“最大努力交付”（Best-Effort Delivery），比如 TCP 协议中的可靠性保障，就属于最大努力交付。 支持分布式事务的消息框架，如 RocketMQ，原生就支持分布式事务操作。 TCC可靠消息队列虽然能保证最终的结果是相对可靠的，过程也足够简单（相对于 TCC 来说），但整个过程完全没有任何隔离性可言，有一些业务中隔离性是无关紧要的，但有一些业务中缺乏隔离性就会带来许多麻烦。譬如：超售。 TCC 是另一种常见的分布式事务机制，它是“Try-Confirm-Cancel”三个单词的缩写。 TCC 的实现过程分为了三个阶段： Try：尝试执行阶段，完成所有业务可执行性的检查（保障一致性），并且预留好全部需用到的业务资源（保障隔离性）。 Confirm：确认执行阶段，不进行任何业务检查，直接使用 Try 阶段准备的资源来完成业务处理。Confirm 阶段可能会重复执行，因此本阶段所执行的操作需要具备幂等性。 Cancel：取消执行阶段，释放 Try 阶段预留的业务资源。Cancel 阶段可能会重复执行，也需要满足幂等性。 它是一种业务侵入性较强的事务方案，要求业务处理过程必须拆分为“预留业务资源”和“确认 / 释放消费资源”两个子过程。 TCC 其实有点类似 2PC 的准备阶段和提交阶段，但 TCC 是位于用户代码层面，而不是在基础设施层面，这为它的实现带来了较高的灵活性，可以根据需要设计资源锁定的粒度。TCC 在业务执行时只操作预留资源，几乎不会涉及锁和资源的争用，具有很高的性能潜力。但是 TCC 并非纯粹只有好处，它也带来了更高的开发成本和业务侵入性，意味着有更高的开发成本和更换事务实现方案的替换成本。 通常我们并不会完全靠裸编码来实现 TCC，而是基于某些分布式事务中间件（譬如阿里开源的Seata）去完成，尽量减轻一些编码工作量。 SAGA事务TCC 事务具有较强的隔离性，避免了“超售”的问题，而且其性能一般来说是本篇提及的几种柔性事务模式中最高的，但它仍不能满足所有的场景。TCC 的最主要限制是它的业务侵入性很强，不止是它需要开发编码配合所带来的工作量，而更多的是指它所要求的技术可控性上的约束。譬如，网银支付，通常也就无法完成冻结款项、解冻、扣减这样的操作。 SAGA 事务基于数据补偿代替回滚的解决思路。大致思路是把一个大事务分解为可以交错运行的一系列子事务集合。每个子事务都应该是或者能被视为是原子行为；为每一个子事务设计对应的补偿动作。 两种恢复策略：正向恢复（Forward Recovery）：T1，T2，…，Ti（失败），Ti（重试）…，Ti+1，…，Tn反向恢复（Backward Recovery）：T1，T2，…，Ti（失败），Ci（补偿），…，C2，C1。 与 TCC 相比，SAGA 不需要为资源设计冻结状态和撤销冻结的操作，补偿操作往往要比冻结操作容易实现得多。 SAGA 事务通常也不会直接靠裸编码来实现，一般也是在事务中间件的基础上完成，前面提到的 Seata 就同样支持 SAGA 事务模式。","categories":[{"name":"读书","slug":"读书","permalink":"http://www.lights8080.com/categories/%E8%AF%BB%E4%B9%A6/"},{"name":"技术","slug":"读书/技术","permalink":"http://www.lights8080.com/categories/%E8%AF%BB%E4%B9%A6/%E6%8A%80%E6%9C%AF/"}],"tags":[{"name":"凤凰架构","slug":"凤凰架构","permalink":"http://www.lights8080.com/tags/%E5%87%A4%E5%87%B0%E6%9E%B6%E6%9E%84/"},{"name":"事务","slug":"事务","permalink":"http://www.lights8080.com/tags/%E4%BA%8B%E5%8A%A1/"}]},{"title":"凤凰架构-全局事务和共享事务","slug":"读书笔记/凤凰架构/凤凰架构-全局事务和共享事务","date":"2021-08-03T16:00:00.000Z","updated":"2021-08-05T10:55:49.000Z","comments":true,"path":"p/53149592.html","link":"","permalink":"http://www.lights8080.com/p/53149592.html","excerpt":"周志明《凤凰架构：构建可靠的大型分布式系统》https://icyfenix.cn/ XA、JTA、两阶段提交、三阶段提交","text":"周志明《凤凰架构：构建可靠的大型分布式系统》https://icyfenix.cn/ XA、JTA、两阶段提交、三阶段提交 全局事务在本节里，全局事务被限定为一种适用于单个服务使用多个数据源场景的事务解决方案。 为了解决分布式事务的一致性问题，X/Open组织提出了一套名为X/Open XA（XA 是 eXtended Architecture 的缩写）的处理事务架构，其核心内容是定义了全局的事务管理器（Transaction Manager，用于协调全局事务）和局部的资源管理器（Resource Manager，用于驱动本地事务）之间的通信接口。 XA 接口是双向的，能在一个事务管理器和多个资源管理器（Resource Manager）之间形成通信桥梁，通过协调多个数据源的一致动作，实现全局事务的统一提交或者统一回滚。 JTA（Java Transaction API）是基于 XA 模式在 Java 语言中的实现了全局事务处理的标准。 XA 将事务提交拆分成为两阶段过程，两阶段式提交（2 Phase Commit，2PC）： 准备阶段（又叫作投票阶段，对于数据库来说，它与本地事务中真正提交的区别只是暂不写入最后一条 Commit Record 而已，并不立即释放隔离性，即仍继续持有锁） 提交阶段（又叫作执行阶段，对于数据库来说，这个阶段的提交操作应是很轻量的，仅仅是持久化一条 Commit Record 而已，通常能够快速完成） 缺点： 单点问题：协调者宕机，所有参与者都必须一直等待 性能问题：两段提交过程中，所有参与者相当于被绑定成为一个统一调度的整体，期间要经过两次远程服务调用，三次数据持久化（准备阶段写重做日志，协调者做状态持久化，提交阶段在日志写入 Commit Record） 一致性风险：前面已经提到，两段式提交的成立是有前提条件的，当网络稳定性和宕机恢复能力的假设不成立时，仍可能出现一致性问题。 为了缓解两段式提交协议的一部分缺陷，具体地说是协调者的单点问题和准备阶段的性能问题，后续又发展出了“三段式提交”（3 Phase Commit，3PC）协议。三段式提交把原本的两段式提交的准备阶段再细分为两个阶段，分别称为 CanCommit、PreCommit，把提交阶段改称为 DoCommit 阶段。 共享事务共享事务（Share Transaction）是指多个服务共用同一个数据源。这里有必要再强调一次“数据源”与“数据库”的区别：数据源是指提供数据的逻辑设备，不必与物理设备一一对应。 如果直接将不同数据源视为不同的数据库，那完全可以用全局事务或者下一讲要学习的分布式事务来实现。 如果针对每个数据源连接的都是同一个物理数据库的特例，一种理论可行的方案是，直接让各个服务共享数据库连接。同一个应用进程中共享数据库连接并不困难，但不同服务节点共享数据库连接很难做到，为了实现共享事务，就必须新增一个中间角色。 这在分布式的场景下是个伪需求，你有充足理由让多个微服务去共享数据库，那就必须找到更加站得住脚的理由，来向团队解释拆分微服务的目的是什么。","categories":[{"name":"读书","slug":"读书","permalink":"http://www.lights8080.com/categories/%E8%AF%BB%E4%B9%A6/"},{"name":"技术","slug":"读书/技术","permalink":"http://www.lights8080.com/categories/%E8%AF%BB%E4%B9%A6/%E6%8A%80%E6%9C%AF/"}],"tags":[{"name":"凤凰架构","slug":"凤凰架构","permalink":"http://www.lights8080.com/tags/%E5%87%A4%E5%87%B0%E6%9E%B6%E6%9E%84/"},{"name":"事务","slug":"事务","permalink":"http://www.lights8080.com/tags/%E4%BA%8B%E5%8A%A1/"}]},{"title":"大数据中台架构","slug":"技术/大数据/大数据中台架构","date":"2021-08-01T16:00:00.000Z","updated":"2021-08-04T03:15:03.000Z","comments":true,"path":"p/27c771b.html","link":"","permalink":"http://www.lights8080.com/p/27c771b.html","excerpt":"以奈学教育大数据架构师训练营为大纲，关于数据仓库的学习笔记","text":"以奈学教育大数据架构师训练营为大纲，关于数据仓库的学习笔记 大数据中台架构 大数据中台演进三阶段： 定位和价值： 降低数据使用门槛 提升决策利用效率 数据驱动业务增长 第一阶段： 定位：快速描述业务事实，提供数据分析的原材料 价值：为决策提供支持，评估业务效果 形式：数据提取-&gt;数据计算-&gt;数据仓库建设-&gt;指标体系-&gt;报表 第二阶段： 定位：波动根因分析 价值：辅助/诊断业务 形式：专题分析-&gt;分析框架-&gt;分析工具-&gt;多维分析模型 第三阶段： 定位：数据化运营实践落地 价值：业务数据统计分析-&gt;数据驱动运营闭环-&gt;良性循环 形式：分析报告 大数据仓库中台建设目标： 响应：监视事件计量标准，与历史数据相关联，制定及时的应对策略 分析：分析盈利价值链，内部运营的状况和风险，外部市场变化 决策：大数据中找出价值信息，对管理和分析人员精准及时的报告 计划：预测和趋势分析，感知风险 大数据仓库中台建设实现功能： 逻辑分层：数据采集层-&gt;原始数据层-&gt;数据仓库层-&gt;数据集市层-&gt;数据应用层-&gt;数据应用层 数仓规范：开发标准，命名规范，开放的数据存储、建模、计算能力 主题域划分：一致性维度与事实；业务数据矩阵（明确各业务分析的主题模块、业务过程所属的数据域） 数据建模：构建维度和事实总线矩阵，维度和事实模型设计；明确统计指标，指标结果表设计；维度总线矩阵（明确维度和业务过程之间的关系） 数据治理：元数据管理，数据安全，数据治理，数据生命周期管理 智能分析：数据挖掘（精细化运营、竞对抓取分析、商业广告），实时预测分析，多维根因分析 逻辑分层 ODS（操作数据层，Operational Data Store）：将原始数据几乎无处理地存放在数据仓库系统中，看数据量和存储预算决定保留时间 DM（Data Warehouse）/CDM（数据公共层，Common Dimenions Model）：包括DIM、DWD、DWM、DWS，采取更多的宽表化手段构建公共指标数据层，提升公共指标的复用性，减少重复加工 DIM（维表，Dimension）：以维度作为建模驱动 DWD（明细事实表，Data Warehouse Detail）：以业务过程作为建模驱动，基于每个具体的业务过程特点，构建最细粒度的明细事实表 DWM（轻度汇总层，Data WareHouse Middle） DM（数据集市/宽表，Data Market）/DWS（汇总数据层，Data Warehouse Summary）：以分析的主题对象作为建模驱动 ADS（应用数据层，Application Data Store）：存放数据产品个性化的统计指标数据 事实表又叫事实数据表，主要特点是含有大量的数据，并且这些数据是可以汇总，并被记录的。 表格里存储了能体现实际数据或详细数值，一般由维度编码和事实数据组成。事实表作为数据仓库维度建模的核心，紧紧围绕着业务过程进行设计。 事实数据表不应该包含描述性的信息，也不应该包含除数字度量字段及使事实与纬度表中对应项的相关索引字段之外的任何数据。 事实表数据列组成部分： 键值列 度量值（分为可以累计的度量值，非累计的度量值） 事实表分类： 事务型事实表 周期型快照事实表 累积型快照事实表 维度表数据仓库中的表，其条目描述事实数据表中的数据。 表格里存放了具有独立属性和层次结构的数据，一般由维度编码和对应的维度说明组成。 维度表可以看作是用户来分析数据的窗口，纬度表中包含事实数据表中事实记录的特性，有些特性提供描述性信息，有些特性指定如何汇总事实数据表数据，以便为分析者提供有用的信息，维度表包含帮助汇总数据的特性的层次结构。 粒度粒度就是业务流程中对度量的单位，比如商品是按件记录度量，还是按批记录度量。 度量度量是业务流程节点上的一个数值。比如销量，价格，成本等等。 宽表通常是指业务主题相关的指标、维度、属性关联在一起的一张数据库表。 逻辑分层(业务架构) 数仓数据分层好处： 【易维护】高效的数据组织形式，清洗和过滤，规范化，血缘追踪 【高性能】时间价值，数据集合，维度汇总，查询效率 【简单化】集成价值，多维度数据整合，多角度多层次的数据分析 【历史性】历史数据，回溯历史，预测未来 数据建模DWD层需构建维度模型：选择业务过程→声明粒度→确认维度→确认事实 维度总线矩阵 Skynet调度中台 Lambda VS Kappa架构https://www.cnblogs.com/xiaodf/p/11642555.html 参考阿里云-数仓分层数据仓库 |1.3 数仓分层| 建模数据仓库| 1.4 ODS&amp; DWD&amp; DWS&amp;DWT&amp; ADS数据仓库事实表和维表","categories":[{"name":"技术","slug":"技术","permalink":"http://www.lights8080.com/categories/%E6%8A%80%E6%9C%AF/"}],"tags":[{"name":"架构","slug":"架构","permalink":"http://www.lights8080.com/tags/%E6%9E%B6%E6%9E%84/"},{"name":"中台","slug":"中台","permalink":"http://www.lights8080.com/tags/%E4%B8%AD%E5%8F%B0/"},{"name":"大数据","slug":"大数据","permalink":"http://www.lights8080.com/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"},{"name":"数据仓库","slug":"数据仓库","permalink":"http://www.lights8080.com/tags/%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/"}]},{"title":"凤凰架构-本地事务","slug":"读书笔记/凤凰架构/凤凰架构-本地事务","date":"2021-07-29T16:00:00.000Z","updated":"2021-08-05T09:49:21.000Z","comments":true,"path":"p/4bfa4595.html","link":"","permalink":"http://www.lights8080.com/p/4bfa4595.html","excerpt":"周志明《凤凰架构：构建可靠的大型分布式系统》https://icyfenix.cn/ 包括：ACID、如何实现原子性和持久性、如何实现隔离性、事务隔离级别、多版本并发控制-MVCC、悲观锁和乐观锁","text":"周志明《凤凰架构：构建可靠的大型分布式系统》https://icyfenix.cn/ 包括：ACID、如何实现原子性和持久性、如何实现隔离性、事务隔离级别、多版本并发控制-MVCC、悲观锁和乐观锁 事务处理 一致性（Consistency）：保证系统中所有的数据都是符合期望的，且相互关联的数据之间不会产生矛盾。 原子性（Atomic）：在同一项业务处理过程中，事务保证了对多个数据的修改，要么同时成功，要么同时被撤销。 隔离性（Isolation）：在不同的业务处理过程中，事务保证了各自业务正在读、写的数据互相独立，不会彼此影响。 持久性（Durability）：事务应当保证所有成功被提交的数据修改都能够正确地被持久化，不丢失数据。 原子性，隔离性和持久性是数据库的属性，是手段，而一致性（在ACID意义上）是应用程序的属性，是目的。应用可能依赖数据库的原子性和隔离属性来实现一致性，但这并不仅取决于数据库。 本地事务是最基础的一种事务处理方案，通常只适用于单个服务使用单个数据源的场景，它是直接依赖于数据源（通常是数据库系统）本身的事务能力来工作的。 如何实现原子性和持久性实现原子性和持久性的最大困难是“写入磁盘”这个操作并不是原子的（比如要写入到不同的磁盘块上），不仅有“写入”与“未写入”状态，还客观地存在着“正在写”的中间状态。 未提交事务，写入后崩溃，要将已经修改过的数据从磁盘中恢复成没有改过的样子，以保证原子性。已提交事务，写入前崩溃，要将没来得及写入磁盘的数据重新写入，以保证持久性。 由于写入中间状态与崩溃都是无法避免的，为了保证原子性和持久性，就只能在崩溃后采取恢复的补救措施，这种数据恢复操作被称为“崩溃恢复”。 为了能够顺利地完成崩溃恢复，会以日志的形式先记录到磁盘，日志记录全部落盘后为Commit Record，表示事务成功。根据日志上的信息对真正的数据进行修改，修改完成后为End Record，表示事务已完成持久化。这种事务实现方法被称为Commit Logging。 Commit Logging的原理很清晰，但缺陷是，即使磁盘 I/O 有足够空闲、即使某个事务修改的数据量非常庞大，都不允许在事务提交前就修改磁盘上的真实数据，这对提升数据库的性能是否不利。为了解决这个问题，ARIES提出所谓“提前写入”（Write-Ahead），就是允许在事务提交之前，提前写入变动数据的意思。 按照事务提交时点为界，划分为 FORCE 和 STEAL 两类情况： FORCE：当事务提交后，要求变动数据必须同时完成写入。 NO-FORCE：当事务提交后，不强制变动数据必须同时完成写入。 STEAL：在事务提交前，允许变动数据提前写入。 NO-STEAL：在事务提交前，不允许变动数据提前写入。(从优化磁盘 I/O 性能考虑，没有必要强制数据写入立即进行) 从优化磁盘 I/O 性能考虑，没有必要强制数据写入立即进行。允许数据提前写入（NO-STEAL），有利于利用空闲 I/O 资源，也有利于节省数据库缓存区的内存。从优化磁盘 I/O 的角度看，NO-FORCE + STEAL 组合的性能无疑是最高的；从算法实现与日志的角度看 NO-FORCE + STEAL 组合的复杂度无疑也是最高的。 Write-Ahead Logging 允许 NO-FORCE，也允许 STEAL。解决办法是增加了另一种被称为 Undo Log 的日志类型，当变动数据写入磁盘前，必须先记录 Undo Log，以便在事务回滚或者崩溃恢复时根据 Undo Log 对提前写入的数据变动进行擦除。崩溃恢复时会执行以下三个阶段的操作：分析阶段（Analysis）、重做阶段（Redo）、回滚阶段（Undo）。 如何实现隔离性隔离性保证了每个事务各自读、写的数据互相独立，不会彼此影响。 串行的数据访问，具有天然的隔离性，现代数据库都提供了以下三种锁： 写锁（Write Lock，也叫做排他锁，简写为 X-Lock）：只有持有写锁的事务才能对数据进行写入操作，数据加持着写锁时，其他事务不能写入数据，也不能施加读锁。 读锁（Read Lock，也叫做共享锁，简写为 S-Lock）：多个事务可以对同一个数据添加多个读锁，数据被加上读锁后就不能再被加上写锁，所以其他事务不能对该数据进行写入，但仍然可以读取。对于持有读锁的事务，如果该数据只有一个事务加了读锁，那可以直接将其升级为写锁，然后写入数据。 范围锁（Range Lock）：对于某个范围直接加排他锁，在这个范围内的数据不能被读取，也不能被写入。 本地事务的四种隔离级别 可串行化（Serializable）：隔离性级别最高，对事务所有读、写的数据全都加上读锁、写锁和范围锁即可（这种可串行化的实现方案称为 Two-Phase Lock - 2PL）。 可重复读（Repeatable Read）：对事务所涉及到的数据加读锁和写锁，并且一直持续到事务结束，但不再加范围锁。可重复读比可串行化弱化的地方在于幻读问题 读已提交（Read Committed）：对事务涉及到的数据加的写锁，会一直持续到事务结束，但加的读锁在查询操作完成后就马上会释放。读已提交比可重复读弱化的地方在于不可重复读问题 读未提交（Read Uncommitted）：对事务涉及到的数据只加写锁，这会一直持续到事务结束，但完全不加读锁。读未提交比读已提交弱化的地方在于脏读问题 事务隔离级别解决的问题 幻读问题：事务执行的过程中，两个完全相同的范围查询得到了不同的结果集 不可重复读问题：在事务执行过程中，对同一行数据的两次查询得到了不同的结果 脏读问题：在事务执行的过程中，一个事务读取到了另一个事务未提交的数据 多版本并发控制”（Multi-Version Concurrency Control，MVCC）这种“一个事务读 + 另一个事务写”的隔离问题，有一种名为“多版本并发控制”的无锁优化方案被主流的商业数据库广泛采用。 MVCC 的基本思路是对数据库的任何修改都不会直接覆盖之前的数据，而是产生一个新版副本与老版本共存，以此达到读取时可以完全不加锁的目的。 隔离级别是可串行化：可串行化本来的语义就是要阻塞其他事务的读取操作，与MVCC无锁优化冲突。 隔离级别是可重复读：总是读取 CREATE_VERSION 小于或等于当前事务 ID 的记录，在这个前提下，如果数据仍有多个版本，则取最新（事务 ID 最大）的。 隔离级别是读已提交：总是取最新的版本即可，即最近被 Commit 的那个版本的数据记录。 隔离级别是读未提交：直接修改原始数据即可，其他事务查看数据的时候立刻可以查看到，无需版本控制。 MVCC 是只针对“读 + 写”场景的优化，如果是两个事务同时修改数据，即“写 + 写”的情况，那就没有多少优化的空间了，加锁几乎是唯一可行的解决方案，分为悲观锁和乐观锁。 “悲观锁（悲观并发控制）”：认为事务之间数据存在竞争是必然情况，竞争越剧烈，性能越好。（即基于锁的并发控制，比如2PL） “乐观锁（乐观并发控制）”：认为事务之间数据存在竞争是偶然情况，竞争越剧烈，性能越差。（基本思路是提交事务前检查有没有更改，如果有就放弃修改并重试）","categories":[{"name":"读书","slug":"读书","permalink":"http://www.lights8080.com/categories/%E8%AF%BB%E4%B9%A6/"},{"name":"技术","slug":"读书/技术","permalink":"http://www.lights8080.com/categories/%E8%AF%BB%E4%B9%A6/%E6%8A%80%E6%9C%AF/"}],"tags":[{"name":"凤凰架构","slug":"凤凰架构","permalink":"http://www.lights8080.com/tags/%E5%87%A4%E5%87%B0%E6%9E%B6%E6%9E%84/"},{"name":"事务","slug":"事务","permalink":"http://www.lights8080.com/tags/%E4%BA%8B%E5%8A%A1/"}]},{"title":"凤凰架构-RPC vs REST","slug":"读书笔记/凤凰架构/凤凰架构-RPC vs REST","date":"2021-07-25T16:00:00.000Z","updated":"2021-08-03T01:47:54.000Z","comments":true,"path":"p/dcd4b9cd.html","link":"","permalink":"http://www.lights8080.com/p/dcd4b9cd.html","excerpt":"周志明《凤凰架构：构建可靠的大型分布式系统》https://icyfenix.cn/ RPC发展史、REST风格的面向资源编程思想","text":"周志明《凤凰架构：构建可靠的大型分布式系统》https://icyfenix.cn/ RPC发展史、REST风格的面向资源编程思想 RPCRPC出现的最初目的，就是为了让计算机能够跟调用本地方法一样去调用远程方法。 进程间通信（Inter-Process Communication，IPC）方法： 管道（Pipe）：类似于两个进程间的桥梁，传递少量的字符流和字节流。如：ps -ef | grep java 信号（Signal）：通知目标进程有某种事件发生。如：kill -9 pid 信号量（Semaphore）：信号量用于两个进程之间同步协作手段。如：wait(),notify() 消息队列（Message Queue）：以上三种方式只适合传递传递少量信息，消息队列用于进程间数据量较多的通信。 共享内存（Shared Memory）：允许多个进程访问同一块公共的内存空间，这是效率最高的进程间通信形式。 套接字接口（Socket）：以上两种方式只适合单机多进程间的通信，套接字接口是更为普适的进程间通信机制，可用于不同机器之间的进程通信。 通信的成本最后一种基于套接字接口的通信方式（IPC Socket），它不仅适用于本地相同机器的不同进程间通信，由于 Socket 是网络栈的统一接口，它也理所当然地能支持基于网络的跨机器的进程间通信。 由于 Socket 是各个操作系统都有提供的标准接口，完全有可能把远程方法调用的通信细节隐藏在操作系统底层，从应用层面上看来可以做到远程调用与本地的进程间通信在编码上完全一致。但这种透明的调用形式却反而造成了程序员误以为通信是无成本的假象。 在“透明的 RPC 调用”一度成为主流范式的时候，Andrew Tanenbaum教授对此提出了一系列质问。论文的中心观点是：本地调用与远程调用当做一样处理，这是犯了方向性的错误，把系统间的调用做成透明，反而会增加程序员工作的复杂度。 透明通信的支持者有之，反对者有之，经过此后几年的发展，逐渐证明了Andrew Tanenbaum教授的预言是正确的。最终大佬们共同总结了通过网络进行分布式运算的八宗罪。 潜台词就是如果远程服务调用要弄透明化的话，就必须为这些罪过买单。至此，RPC 应该是一种高层次的或者说语言层次的特征，而不是像 IPC 那样，是低层次的或者说系统层次的特征成为工业界、学术界的主流观点。 RPC三个基本问题 如何表示数据：就是序列化与反序列化。Web Service 的XML Serialization 如何传递数据：通常指的是应用层协议，实际传输一般是基于标准的 TCP、UDP 等标准的传输层协议来完成的。Web Service 的Simple Object Access Protocol（SOAP） 如何确定方法：一套语言无关的接口描述语言。Web Service 的Web Service Description Language（WSDL） RPC的统一和分裂统一CORBA本身设计得实在是太过于啰嗦繁琐，制定的规范晦涩难懂脱离实际，没有把握住统一 RPC 的大好机遇。 后来XML 1.0 发布，并成为W3C的推荐标准，随后SOAP 1.0规范的发布，它代表着一种被称为“Web Service”的全新的 RPC 协议的诞生。随后提交给 W3C 投票成为国际标准，所以也被称为W3C Web Service。Web Service 采用了 XML 作为远程过程调用的序列化、接口描述、服务发现等所有编码的载体。 Web Service 的一大缺点是它那过于严格的数据和接口定义所带来的性能问题，XML本身信息密度就相对低下，Web Service又是跨语言的 RPC 协议，一个简单的字段为了不会产生歧义，XML严格描述的话，往往比原来多出几十倍的空间。另外一点是，它希望在一套协议上一揽子解决分布式计算中可能遇到的所有问题，除它本身包括的 SOAP、WSDL、UDDI 协议外，还有一堆WS-*命名的、用于解决事务、一致性、事件、通知、业务描述、安全、防重放等子功能协议，这对开发者造成了非常沉重的学习负担。 分裂由于一直没有一个同时满足以上三点的“完美 RPC 协议”出现。今时今日，任何一款具有生命力的 RPC 框架，都不再去追求大而全的“完美”，而是有自己的针对性特点作为主要的发展方向。 朝着面向对象发展：RMI（Sun/Oracle）、.NET Remoting 朝着性能发展，代表为 gRPC（Google）、Thrift（Facebook/Apache） 朝着简化发展，代表为 JSON-RPC 到了最近几年，RPC 框架有明显的朝着更高层次（不仅仅负责调用远程服务，还管理远程服务）与插件化方向发展的趋势，不再追求独立地解决 RPC 的全部三个问题（表示数据、传递数据、表示方法），而是将一部分功能设计成扩展点，让用户自己去选择。框架聚焦于提供核心的、更高层次的能力，譬如提供负载均衡、服务注册、可观察性等方面的支持。这一类框架的代表有 Facebook 的 Thrift 与阿里的 Dubbo。 RESTREST并不是一种远程服务调用协议，它甚至就不是一种协议。虽然它有一些指导原则，但实际上并不受任何强制的约束。经常会有人批评说，某个系统接口“设计得不够 RESTful”，其实这句话本身就有些争议。因为 REST 只能说是一种风格。 REST是“表征状态转移”（Representational State Transfer）的缩写。可以理解为是“HTT”（Hyper Text Transfer，超文本传输）的进一步抽象，它们就像是接口与实现类之间的关系。 REST中关键概念：（以阅读文章为例） 资源（Resource）：可以将其视作是某种信息、数据。如：文章的内容，无论是网页还是报纸，你阅读的仍是同一个“资源” 表征（Representation）：指信息与用户交互时的表示形式。如：文章的PDF、Markdown等表现形式 状态（State）：在特定语境中才能产生的上下文信息就被称为“状态”。如：请求“下一篇”文章，依赖当前正在阅读的文章 转移（Transfer）：服务器通过某种方式，把“用户当前阅读的文章”转变成“下一篇文章”，这就被称为“表征状态转移” RESTful的系统REST风格的系统应该满足以下六大原则 服务端与客户端分离（Client-Server） 无状态（Stateless） 可缓存（Cacheability） 分层系统（Layered System） 统一接口（Uniform Interface） 按需代码（Code-On-Demand） REST以资源为主体进行服务设计的风格，带来了什么好处： 降低的服务接口的学习成本 资源天然具有集合与层次结构 REST 绑定于 HTTP 协议 RMM成熟度模型 0级. The Swamp of Plain Old XML：完全不REST，是RPC的风格 1级. Resources：引入资源的概念 2级. HTTP Verbs：引入统一接口，映射到HTTP协议的方法上（目前大部分的系统能够达到的REST界别） 3级. Hypermedia Controls：“超文本驱动”，除了第一个请求是地址栏输入驱动以外，后续请求应该自己描述清楚后续可能发生的状态转移，由超文本自身来驱动。 编程思想的立场不同： 面向过程编程时，为什么要以算法和处理过程为中心，输入数据，输出结果？当然是为了符合计算机世界中主流的交互方式。 面向对象编程时，为什么要将数据和行为统一起来、封装成对象？当然是为了符合现实世界的主流的交互方式。 面向资源编程时，为什么要将数据（资源）作为抽象的主体，把行为看作是统一的接口？当然是为了符合网络世界的主流的交互方式。 RPC vs RESTREST 与 RPC 在思想上差异的核心是抽象的目标不一样，即面向资源的编程思想与面向过程的编程思想两者之间的区别。 至于使用范围，REST 与 RPC 作为主流的两种远程调用方式，在使用上是确有重合的，但重合的区域有多大就见仁见智了。 RPC一些发展方向，如分布式对象、提升调用效率、简化调用复杂性。 分布式对象：这一条线的应用与 REST 可以说是毫无关联； 提升调用效率：REST提升传输效率的潜力有限，对于传输协议、序列化器这两点都不会有什么选择的权力 简化调用复杂性：追求简化调用的场景，众多 RPC 里也就 JSON-RPC 有机会与 REST 竞争 我们今天再去看这两种编程思想，虽然它们出现的时间有先后，但在人类使用计算机语言来处理数据的工作中，无论用哪种思维来抽象问题都是合乎逻辑的。","categories":[{"name":"读书","slug":"读书","permalink":"http://www.lights8080.com/categories/%E8%AF%BB%E4%B9%A6/"},{"name":"技术","slug":"读书/技术","permalink":"http://www.lights8080.com/categories/%E8%AF%BB%E4%B9%A6/%E6%8A%80%E6%9C%AF/"}],"tags":[{"name":"凤凰架构","slug":"凤凰架构","permalink":"http://www.lights8080.com/tags/%E5%87%A4%E5%87%B0%E6%9E%B6%E6%9E%84/"},{"name":"RPC","slug":"RPC","permalink":"http://www.lights8080.com/tags/RPC/"}]},{"title":"凤凰架构-向微服务迈进","slug":"读书笔记/凤凰架构/凤凰架构-向微服务迈进","date":"2021-07-22T16:00:00.000Z","updated":"2021-08-03T01:47:54.000Z","comments":true,"path":"p/70598b0f.html","link":"","permalink":"http://www.lights8080.com/p/70598b0f.html","excerpt":"周志明《凤凰架构：构建可靠的大型分布式系统》https://icyfenix.cn/ 向微服务迈进，目的-&gt;前提-&gt;边界-&gt;治理","text":"周志明《凤凰架构：构建可靠的大型分布式系统》https://icyfenix.cn/ 向微服务迈进，目的-&gt;前提-&gt;边界-&gt;治理 软件研发中任何一项技术、方法、架构都不可能是银弹。 假如只能用一个词来形容微服务解决问题的核心思想，笔者给的答案就是“分治”，这即是微服务的基本特征，也是微服务应对复杂性的手段。 目的：微服务的驱动力硬件的成本能够持续稳定地下降，而软件开发的成本则不可能。微服务最主要的目的是对系统进行有效的拆分，实现物理层面的隔离，微服务的核心价值就是拆分之后的系统能够让局部的单个服务有可能实现敏捷地卸载、部署、开发、升级，局部的持续更迭。 外部因素： 当意识到没有什么技术能够包打天下：java、python、golang不同的语言擅长做不同的事情 当个人能力因素成为系统发展的明显制约：在单体架构下，系统中“整体”与“部分”的关系没有物理的划分，难以阻止大量螺丝钉式的程序员或外包人员在不起眼的地方犯错并产生全局的影响 当遇到来自外部商业层面对内部技术层面提出的要求：甲方招投标文件技术规范明文要求 内部因素： 变化发展特别快的创新业务系统往往会自主地向微服务架构靠近：需求、开发、运维肯定都是很乐意接受微服务 大规模的、业务复杂的、历史包袱沉重的系统也可能主动向微服务架构靠近 前提：微服务需要的条件 决策者与执行者都能意识到康威定律在软件设计中的关键作用，沟通决定设计，技术层面和组织层面不一致，会造成沟通成本上升或管理成本上升 组织中具备一些的对微服务有充分理解、有一定实践经验的技术专家，微服务架构对普通开发者友善，对架构者满满的恶意 系统应具有以自治为目标的自动化与监控度量能力，三个前提：环境预置，基础监控，快速部署 复杂性已经成为制约生产力的主要矛盾，接受演进式架构 边界：微服务的粒度“识别微服务的边界”其实已取得了较为一致的观点，也找到了指导具体实践的方法论，即领域驱动设计（Domain-Driven Design，DDD）。 但系统设计是一种创作，而不是应试，不可能每一位架构师设计的服务粒度全都相同，微服务的大小、边界不应该只有唯一正确的答案或绝对的标准，但是应该有个合理的范围，笔者称其为微服务粒度的上下界（下界：表示微服务粒度太小，上界：表示微服务粒度太大）。 微服务粒度的下界是它至少应满足 独立：能够独立发布、独立部署、独立运行与独立测试 内聚：强相关的功能与数据在同一个服务中处理 完备：一个服务包含至少一项业务实体与对应的完整操作 微服务的上界并非受限于技术，而是受限于人，更准确地说，受限于人与人之间的社交协作 微服务粒度的上界是一个 2 Pizza Team 能够在一个研发周期内完成的全部需求范围 治理：理解系统复杂性 治理就是让产品能够符合预期地稳定运行，并能够持续保持在一定的质量水平上。 静态的治理复杂性的来源： 复杂性来自认知负荷：分布式系统带来更高的认知负荷，面向资源，异步通讯，容错处理，去中心化等。 复杂性来自协作成本：协作的沟通复杂度，沟通成本= n×(n-1)/2，n 为参与项目的人数 结论：软件规模小时微服务的复杂度高于单体系统，规模大时则相反。这里的原因就是微服务的认知负荷较高，但是协作成本较低。 假如只能用一个词来形容微服务解决问题的核心思想，笔者给的答案就是“分治”，这即是微服务的基本特征，也是微服务应对复杂性的手段。 发展的治理架构腐化是软件动态发展中出现的问题，任何静态的治理方案都只能延缓，不能根治，必须在发展中才能寻找到彻底解决的办法。治理架构腐化唯一有效的办法是演进式的设计。","categories":[{"name":"读书","slug":"读书","permalink":"http://www.lights8080.com/categories/%E8%AF%BB%E4%B9%A6/"},{"name":"技术","slug":"读书/技术","permalink":"http://www.lights8080.com/categories/%E8%AF%BB%E4%B9%A6/%E6%8A%80%E6%9C%AF/"}],"tags":[{"name":"凤凰架构","slug":"凤凰架构","permalink":"http://www.lights8080.com/tags/%E5%87%A4%E5%87%B0%E6%9E%B6%E6%9E%84/"},{"name":"微服务","slug":"微服务","permalink":"http://www.lights8080.com/tags/%E5%BE%AE%E6%9C%8D%E5%8A%A1/"}]},{"title":"凤凰架构-架构演进","slug":"读书笔记/凤凰架构/凤凰架构-架构演进","date":"2021-07-22T16:00:00.000Z","updated":"2021-08-03T01:47:54.000Z","comments":true,"path":"p/a289ccb9.html","link":"","permalink":"http://www.lights8080.com/p/a289ccb9.html","excerpt":"周志明《凤凰架构：构建可靠的大型分布式系统》https://icyfenix.cn/ 架构演进，原始分布式时代-&gt;单体系统时代-&gt;SOA时代-&gt;微服务时代-&gt;后微服务时代-&gt;无服务时代","text":"周志明《凤凰架构：构建可靠的大型分布式系统》https://icyfenix.cn/ 架构演进，原始分布式时代-&gt;单体系统时代-&gt;SOA时代-&gt;微服务时代-&gt;后微服务时代-&gt;无服务时代 架构并不是被“发明”出来的，而是持续进化的结果。 整个“演进中的架构”这部分，一条重要的逻辑线索就是软件工业对如何拆分业务、隔离技术复杂性的探索。从最初的不拆分，到通过越来越复杂的技术手段逐渐满足了业务的拆分与协作，再到追求隔离掉这些复杂技术手段，将它们掩埋于基础设施之中，到未来（有可能的）重新回到无需考虑算力、无需拆分的云端系统。 原始分布式时代 Unix的分布式设计哲学：保持接口与实现的简单性，比系统的任何其他属性，包括准确性、一致性和完整性，都来得更加重要。 20世纪70年代末期到80年代初，计算机科学刚经历了从以大型机为主向以微型机为主的蜕变。当时计算机硬件局促的运算处理能力，已直接妨碍到了在单台计算机上信息系统软件能够达到的最大规模。为突破硬件算力的限制，各个高校、研究机构、软硬件厂商开始分头探索，寻找使用多台计算机共同协作来支撑同一套软件系统运行的可行方案。 当时研究这些技术都带着浓厚的UNIX设计风格，有一个预设的重要原则是使分布式环境中的服务调用、资源访问、数据存储等操作尽可能透明化、简单化，使开发人员不必过于关注他们访问的方法或其他资源是位于本地还是远程。 但是“调用远程方法”与“调用本地方法”两者的复杂度就完全不可同日而语，一旦要考虑性能上的差异，那远程和本地的鸿沟是无比深刻的，两者的速度往往有着数量级上的差距，完全不可调和。 在那个时代的机器硬件条件下，为了让程序在运行效率上可被用户接受，开发者在某些场景只能被迫将几个原本毫无关系的方法打包到一个方法体内，一块进行远程调用，以提升性能。这本身与期望的分布式相矛盾，另外开发者需要时刻注意是在编写分布式程序。最终导致设计向性能做出的妥协，本地与远程无论是编码、设计、部署还是运行效率角度上看，都有着天壤之别。 原始分布式时代的教训：某个功能能够进行分布式，并不意味着它就应该进行分布式，强行追求透明的分布式操作，只会自寻苦果。 以上结论是有违UNIX设计哲学的，却是当时现实情况下不得不做出的让步。摆在计算机科学面前有两条通往更大规模软件系统的道路，一条是尽快提升单机的处理能力，以避免分布式带来的种种问题；另一条路是找到更完美的解决如何构筑分布式系统的解决方案。 20世纪80年代正是摩尔定律开始稳定发挥作用的黄金时期，硬件算力束缚软件规模的链条很快变得松动，信息系统进入了以单台或少量几台计算机即可作为服务器来支撑大型信息系统运作的单体时代，且在很长的一段时间内，单体都将是软件架构的绝对主流。 单体系统时代 单体意味着自包含。单体应用描述了一种由同一技术平台的不同组件构成的单层软件。 单体架构是出现时间最早、应用范围最广、使用人数最多、统治历史最长的一种架构风格。但“单体”这个名称，却是从微服务开始流行之后，才“事后追认”所形成的概念。在这之前，并没有多少人会把“单体”看成一种架构。 优点： 易于开发、易于测试、易于部署 进程内的高效交互 缺点： 性能要求难以超越单机 开发人员规模难以超过“2 Pizza Teams” 所有代码运行在同一个进程空间之内，某一个功能一旦出问题（内存泄漏、线程爆炸、阻塞、死循环等），都将会影响到整个程序的运行 代码无法隔离，无法做到单独停止、更新、升级某一部分代码 无法技术异构（技术异构是说允许系统的每个模块，自由选择不一样的程序语言、不一样的编程框架等技术栈去实现） 单体系统的真正缺陷实际上并不在于要如何拆分，而在于拆分之后，它会存在隔离与自治能力上的欠缺。 单体架构并不会消失，因其架构简单，易于开发测试和部署，适用于项目初始阶段，便于业务的快速上线。 SOA时代SOA架构模式之前，三种有代表性服务拆分架构模式 烟筒式架构：系统独立，信息孤岛 微内核架构：共享公共主数据，子系统无法直接互通 事件驱动架构：建立事件队列管道，子系统通过管道解耦交互 “面向服务的架构”（Service Oriented Architecture，SOA）的概念最早1994年提出，2006成立OSOA联盟来联合制定和推进 SOA 相关行业标准，2007在OASIS的倡议与支持下，共同新成立了Open CSA组织，管理制定SOA的行业标准。 SOA最根本的目标，就是希望能够总结出一套自上而下的软件研发方法论，它有完善的理论和工具，让企业只需要跟着它的思路，就能够一揽子解决掉软件开发过程中的全套问题。如真如此可以大幅提升整个社会实施信息化的效率。 但遗憾的是，因为SOA架构过于严谨精密的流程与理论，给架构带来了过度的复杂性，需要有懂得复杂概念的专业人员才能够驾驭。它可以实现多个异构大型系统之间的复杂集成交互，却很难作为一种具有广泛普适性的软件架构风格来推广。 微服务时代 “Micro-Web-Service”，指的是一种专注于单一职责的、与语言无关的、细粒度的 Web 服务。 微服务是一种通过多个小型服务的组合，来构建单个应用的架构风格，这些服务会围绕业务能力而非特定的技术标准来构建。各个服务可以采用不同的编程语言、不同的数据存储技术、运行在不同的进程之中。服务会采取轻量级的通讯机制和自动化的部署机制，来实现通讯与运维。 微服务的九个核心的业务与技术特征 围绕业务能力构建：康威定律(有怎样的结构、规模和能力的团队，就会产生出对应结构、规模、能力的产品) 分散治理：谁家孩子谁来管，技术异构 通过服务来实现独立自治的组件：独立、自治 产品化思维：把服务看做持续改进、提升的产品 数据去中心化：数据应该按领域来分散管理、更新、维护和存储 轻量级通讯机制：处理事务、一致性、认证授权等一系列工作应该在服务自己的Endpoint上解决 容错性设计：接受服务总会出错的现实 演进式设计：承认服务会被报废淘汰 基础设施自动化：CI/CD，大大降低了构建、发布、运维工作的复杂性 微服务追求的是更加自由的架构风格，它摒弃了SOA中几乎所有可以抛弃的约束和规定，提倡以“实践标准”代替“规范标准”。 服务的注册发现、跟踪治理、负载均衡、故障隔离、认证授权、伸缩扩展、传输通讯、事务处理等问题，在微服务中，都不再会有统一的解决方案。 Spring Cloud这样的胶水式的全家桶工具集，通过一致的接口、声明和配置，进一步屏蔽了源自于具体工具、框架的复杂性，降低了在不同工具、框架之间切换的成本。 后微服务时代/云原生时代微服务时代，注册发现、跟踪治理、负载均衡、传输通讯等，这些问题我们都需要在应用服务层面处理，而不是基础设施层面去解决这些分布式问题，完全是因为由硬件构成的基础设施，跟不上由软件构成的应用服务的灵活性。随着虚拟化和容器化技术的发展，这些问题好像都可以解决了。 针对同一个分布式服务的问题，对比下 Spring Cloud 中提供的应用层面的解决方案，以及 Kubernetes 中提供的基础设施层面的解决方案 Kubernetes 的确提供了一条全新的、前途更加广阔的解题思路。 当虚拟化的基础设施，开始从单个服务的容器发展到由多个容器构成的服务集群，以及集群所需的所有通讯、存储设施的时候，软件与硬件的界限就开始模糊了。原来只能从软件层面解决的分布式架构问题，于是有了另外一种解法：应用代码与基础设施软硬一体，合力应对。 无服务时代“无服务“与“微服务”和“云原生”并没有继承替代关系，“无服务比微服务更加先进“是的错误想法。 无服务架构简单，分为两块内容 后端设施：是指数据库、消息队列、日志、存储等这一类用于支撑业务逻辑运行，但本身无业务含义的技术组件。这些后端设施都运行在云中，也就是无服务中的“后端即服务”（Backend as a Service，BaaS） 函数：业务逻辑代码。这里函数的概念与粒度，都已经和程序编码角度的函数非常接近了，区别就在于，无服务中的函数运行在云端，不必考虑算力问题和容量规划 无服务的愿景是让开发者只需要纯粹地关注业务 不用考虑技术组件，因为后端的技术组件是现成的，可以直接取用，没有采购、版权和选型的烦恼 不需要考虑如何部署，因为部署过程完全是托管到云端的，由云端自动完成 不需要考虑算力，因为有整个数据中心的支撑，算力可以认为是无限的 不需要操心运维，维护系统持续地平稳运行是云服务商的责任，而不再是开发者的责任 无服务中短期内的发展局限性 与单体架构、微服务架构不同，无服务架构天生的一些特点，比如冷启动、 无状态、运行时间有限制等等，决定了它不是一种具有普适性的架构模式 擅长短链接、无状态、适合事件驱动的交互形式，不适用于具有业务逻辑复杂、依赖服务端状态、响应速度要求较高、需要长连接等特征的应用 如果说微服务架构是分布式系统这条路当前所能做到的极致，那无服务架构，也许就是“不分布式”的云端系统这条路的起点。","categories":[{"name":"读书","slug":"读书","permalink":"http://www.lights8080.com/categories/%E8%AF%BB%E4%B9%A6/"},{"name":"技术","slug":"读书/技术","permalink":"http://www.lights8080.com/categories/%E8%AF%BB%E4%B9%A6/%E6%8A%80%E6%9C%AF/"}],"tags":[{"name":"凤凰架构","slug":"凤凰架构","permalink":"http://www.lights8080.com/tags/%E5%87%A4%E5%87%B0%E6%9E%B6%E6%9E%84/"}]},{"title":"凤凰架构-程序员发展观和价值观","slug":"读书笔记/凤凰架构/凤凰架构-程序员发展观和价值观","date":"2021-07-21T16:00:00.000Z","updated":"2021-08-03T01:47:54.000Z","comments":true,"path":"p/1b7e41a.html","link":"","permalink":"http://www.lights8080.com/p/1b7e41a.html","excerpt":"周志明《凤凰架构：构建可靠的大型分布式系统》https://icyfenix.cn/ 作者阐明程序员的发展观和价值观","text":"周志明《凤凰架构：构建可靠的大型分布式系统》https://icyfenix.cn/ 作者阐明程序员的发展观和价值观 程序员发展观无论日后你的职业目标是永远做一名程序员，还是架构师，抑或是成为一名研发管理者，都不要轻易地离开技术领域的一线前沿。离开技术一线前沿，你对代码、技术、产品状态与团队研发状态的理解，渐渐和团队成员产生了偏差错位，丧失了细节上给予指导的能力，丧失了专业问题上提出接地气解决方案的能力，只能在短期难以校验对错的大战略方向提意见，在会议、流程及团队管理措施上下功夫，在职业经理人式的宣讲与汇报上寻找存在感。 程序员的性质： 工作的过程无法标准化和流水线化 编码的产出指标与质量指标都很难量化地衡量和对比 写代码这种工作还是一种创造性的脑力劳动，性质决定了程序员必须是一群能独立思考，带有一点天生洁癖，有一点习惯性找茬纠错抬杠的人 程序员的特点： 相对单纯，不必琢磨复杂人际心思的职场群体 天生带有一种工匠式的图腾崇拜精神 奉行达者为师，不迷信管理他们的人，但充分尊重能够指导他们的人 带着些许理工钢铁直男式的直线思维，爱讲逻辑爱讲道理 程序员价值观价值 = (技能收益 + 知识收益) × 提升空间 / 投入成本 技能收益：正视技能收益的意义在于避免自己变得过度浮躁，以“兴趣不合”、“发展不符”为借口去过度挑剔。先把本分工作做对做好，再追求兴趣选择和机遇发展，这才是对多数人的最大的公平。 知识收益：知识的收益往往是间接的，最终会体现在缩减模型中的“投入成本”因素，即降低认知负荷（Cognitive Load）上。目的是要将自己的知识点筑成体系，将大量的不同的零散的知识点、通过内化、存储、整理、归档、输出等方式组合起来，以点成线、以线成面，最终形成系统的、有序的、清晰的脉络结构，这就是知识体系。 提升空间：如果一项工作对你来说是个全新的领域，甚至能称为是一项挑战，那风险的背后往往也蕴含有更高的收益。目的是为了规避舒适区的陷阱，去做已经完全得心应手的事情，没有价值，是因为提升空间是可以下降至零，但投入成本不可能为零，因为成本中不仅包括精力，还包括有时间。 投入成本：“权衡”，“凡事不能只讲收益不谈成本”。收益大小也是必须在确定的成本下才有衡量比较的意义。这里的成本，既包括你花费的时间、金钱与机会，也包括你投入的知识、精神与毅力。","categories":[{"name":"读书","slug":"读书","permalink":"http://www.lights8080.com/categories/%E8%AF%BB%E4%B9%A6/"},{"name":"技术","slug":"读书/技术","permalink":"http://www.lights8080.com/categories/%E8%AF%BB%E4%B9%A6/%E6%8A%80%E6%9C%AF/"}],"tags":[{"name":"凤凰架构","slug":"凤凰架构","permalink":"http://www.lights8080.com/tags/%E5%87%A4%E5%87%B0%E6%9E%B6%E6%9E%84/"}]},{"title":"凤凰架构-介绍","slug":"读书笔记/凤凰架构/凤凰架构-介绍","date":"2021-07-20T16:00:00.000Z","updated":"2021-08-03T01:47:54.000Z","comments":true,"path":"p/caa830c3.html","link":"","permalink":"http://www.lights8080.com/p/caa830c3.html","excerpt":"周志明《凤凰架构：构建可靠的大型分布式系统》https://icyfenix.cn/ 作者，如何理解”可靠“的软件系统，架构演进的驱动力是什么。","text":"周志明《凤凰架构：构建可靠的大型分布式系统》https://icyfenix.cn/ 作者，如何理解”可靠“的软件系统，架构演进的驱动力是什么。 关于作者周志明 程序员 研究员 计算机技术作家 技术布道师 职业是上市公司的高级管理人员，实际是一名兼职一些管理与研究工作的程序员。 做技术不仅要去看、去读、去想、去用，更要去写、去说。 终身学习让自己有个年轻的灵魂，终身锻炼让自己有个年轻的身体。 如何构建一个大规模但依然可靠的软件系统一套“靠谱”的软件系统，尤其是大型的、分布式的软件系统，很难指望只依靠团队成员的个人能力水平，或者依靠质量管理流程来达成。 根据“墨菲定律”和在“大规模”这个前提下，在做软件开发时，你一定会遇到各种不靠谱的人员、代码、硬件、网络等因素。如果一项工作，要经过多个“不靠谱”的过程相互协作来完成，其中的误差应该会不断地累积叠加，导致最终结果必然不能收敛稳定。 在软件工程世界里，把出错看作是正常、甚至是必须的发展过程，只要出了问题能够兜底，能重回正轨就好。 作为一名架构师，在软件研发的过程中，最难的事儿，其实并不是如何解决具体某个缺陷、如何提升某段代码的性能，而是如何才能让一系列来自不同开发者、不同厂商、不同版本、不同语言、质量也良莠不齐的软件模块，在不同的物理硬件和拓扑结构随时变动的网络环境中，依然能保证可靠的运行质量。 架构演进的驱动力软件架构风格从大型机（Mainframe），发展到了多层单体架构（Monolithic），到分布式（Distributed），到微服务（Microservices），到服务网格（Service Mesh），到无服务（Serverless）……你能发现，在技术架构上确实呈现出“从大到小”的发展趋势。 架构演变最重要的驱动力，或者说产生这种“从大到小”趋势的最根本的驱动力，始终都是为了方便某个服务能够顺利地“死去”与“重生”而设计的。个体服务的生死更迭，是关系到整个系统能否可靠续存的关键因素。 整个“演进中的架构”这部分，一条重要的逻辑线索就是软件工业对如何拆分业务、隔离技术复杂性的探索。从最初的不拆分，到通过越来越复杂的技术手段逐渐满足了业务的拆分与协作，再到追求隔离掉这些复杂技术手段，将它们掩埋于基础设施之中，到未来（有可能的）重新回到无需考虑算力、无需拆分的云端系统。 流水不腐，有老朽、有消亡、有重生、有更迭，才是正常生态的运作合理规律。","categories":[{"name":"读书","slug":"读书","permalink":"http://www.lights8080.com/categories/%E8%AF%BB%E4%B9%A6/"},{"name":"技术","slug":"读书/技术","permalink":"http://www.lights8080.com/categories/%E8%AF%BB%E4%B9%A6/%E6%8A%80%E6%9C%AF/"}],"tags":[{"name":"凤凰架构","slug":"凤凰架构","permalink":"http://www.lights8080.com/tags/%E5%87%A4%E5%87%B0%E6%9E%B6%E6%9E%84/"}]},{"title":"阿里云网络故障分析","slug":"技术/其他/阿里云网络故障分析","date":"2021-07-18T16:00:00.000Z","updated":"2021-08-03T01:47:54.000Z","comments":true,"path":"p/17b3cb62.html","link":"","permalink":"http://www.lights8080.com/p/17b3cb62.html","excerpt":"","text":"mtr链路测试和分析工具一：mtr（My traceroute）几乎是所有Linux发行版本预装的网络测试工具。其将ping和traceroute的功能合并，所以功能更强大。mtr默认发送ICMP数据包进行链路探测。您也可以通过-u参数来指定使用UDP数据包进行探测。相对于traceroute只会做一次链路跟踪测试，mtr会对链路上的相关节点做持续探测并给出相应的统计信息。所以，mtr能避免节点波动对测试结果的影响，所以其测试结果更正确，建议优先使用。 请用mtr测试，把结果发给我们帮您分析。具体的请参考以下链接：Windows下的mtr工具使用方法您参考：https://help.aliyun.com/knowledge_detail/40573.html#WinMTR 工具（建议优先使用）Linux下的mtr工具使用方法您参考：https://help.aliyun.com/knowledge_detail/40573.html#mtr 命令行工具（建议优先使用） 请用mtr -r 加访问域名测试 1234# 1. 获取本地网络对应的公网IP# 2. 正向链路测试（ping和mtr）# 3. 反向链路测试（ping和mtr）# 4. 测试结果分析 工具二：traceroute也是几乎所有Linux发行版本预装的网络测试工具，用于跟踪Internet协议（IP）数据包传送到目标地址时经过的路径。 tracetcp端口可用性探测请用tracetcp来测试一下看在那个设备有端口过滤，可以参考：https://help.aliyun.com/knowledge_detail/40572.html 12345678# 1. 实例安全组检查# 2. 端口相关服务检查netstat -ntpl|grep [$Port]# 3. 检查目标服务器的防火墙配置iptables -nL# 4. 通过探测工具进行检查traceroute [-n] -T -p [$Port] [$Host]traceroute -n -T -p 22 223.5.5.5 探测结果如下，目标端口在第11跳之后就没有数据返回。 12345678910111213141516171819202122[root@mycentos ~]# traceroute -T -p 135 www.baidu.comtraceroute to www.baidu.com (111.13.100.92), 30 hops max, 60 byte packets 1 * * * 2 1X.X.X.X (1X.X.X.X) 4.115 ms 4.397 ms 4.679 ms 3 2X.X.X.X (2X.X.X.X) 901.921 ms 902.762 ms 902.338 ms 4 3X.X.X.X (3X.X.X.X) 2.187 ms 1.392 ms 2.266 ms 5 * * * 6 5X.X.X.X (5X.X.X.X) 1.688 ms 1.465 ms 1.475 ms 7 6X.X.X.X (6X.X.X.X) 27.729 ms 27.708 ms 27.636 ms 8 * * * 9 * * *10 111.13.98.249 (111.13.98.249) 28.922 ms 111.13.98.253 (111.13.98.253) 29.030 ms 28.916 ms11 111.13.108.22 (111.13.108.22) 29.169 ms 28.893 ms 111.13.108.33 (111.13.108.33) 30.986 ms12 * * *13 * * *14 * * *15 * * *16 * * *17 * * *18 * * *19 * * *20 * * * tcpdump抓包请您在故障复现的时候抓包，您可以使用下面的方法抓包看一下。能帮助您查找问题，Windows： 在Wireshark 界面中，选择 Capture -》 Interface ，选择对应连接的内网网卡后 -》 Option -》 在 File 输入框中输入要保存的文件 1.cap，然后点击 start 开始抓包。 复现问题。 问题复现后，停止抓包。 将1.cap 压缩为zip格式提供给我们Linux: 打开一个到ECS的ssh连接，并以root身份登录。在该窗口运行下列命令tcpdump host 服务器的域名或地址 -w /var/tmp/1.cap 复现问题。 使用 ctrl + c 终止窗口1 的 tcpdump 命令。 下载 /var/tmp/1.cap 并压缩成zip格式上传到工单中。请参考：https://help.aliyun.com/knowledge_detail/40564.html 12345678# 执行以下命令，抓取eth0网卡22端口的交互数据tcpdump -s 0 -i eth0 port 22# 抓取eth1网卡发送给22端口的交互数据，并在控制台输出详细交互信息tcpdump -s 0 -i eth1 -vvv port 22# 抓取eth1网卡发送至指定IP地址的PING交互数据，并输出详细交互数据tcpdump -s 0 -i eth1 -vvv dst 223.xx.xx.5 and icmp# 抓取系统内所有接口数据并保存到指定文件tcpdump -i any -s 0 -w test.cap 总结出问题是先用tcpdump打开两端抓包，再用mtr和tracetcp测试，再复现问题，最后用ctrl + c 终止窗口，把信息发给我们。 您好，通过您发来的包分析，目前是中间设备拦截导致的。1、 问题复现的时候 ：IP_A访问IP_B的8080端口，出现rst的情况。根据抓包看： 当IP_B收到这个rst报文，对方并没有始发该报文。并且通过该rst报文的 ttl 和 ip序列号， 更证明了该报文不是阿里云发出的。 是中间公网运营商设备伪造发送的rst报文。2、由于公网运营商网络，尤其您这个是跨境网络的情况下， 怀疑是运营商公网出口进行了rst报文的构造发送。","categories":[{"name":"技术","slug":"技术","permalink":"http://www.lights8080.com/categories/%E6%8A%80%E6%9C%AF/"}],"tags":[{"name":"网络","slug":"网络","permalink":"http://www.lights8080.com/tags/%E7%BD%91%E7%BB%9C/"},{"name":"阿里云","slug":"阿里云","permalink":"http://www.lights8080.com/tags/%E9%98%BF%E9%87%8C%E4%BA%91/"}]},{"title":"MinIO对象存储","slug":"工具/MinIO对象存储","date":"2021-07-11T16:00:00.000Z","updated":"2021-08-03T01:47:54.000Z","comments":true,"path":"p/ca17c92b.html","link":"","permalink":"http://www.lights8080.com/p/ca17c92b.html","excerpt":"MinIO 是一个基于Apache License v2.0开源协议的对象存储服务。它兼容亚马逊S3云存储服务接口，非常适合于存储大容量非结构化的数据，例如图片、视频、日志文件、备份数据和容器/虚拟机镜像等，而一个对象文件可以是任意大小，从几kb到最大5T不等。MinIO是一个非常轻量的服务,可以很简单的和其他应用的结合，类似 NodeJS, Redis 或者 MySQL。","text":"MinIO 是一个基于Apache License v2.0开源协议的对象存储服务。它兼容亚马逊S3云存储服务接口，非常适合于存储大容量非结构化的数据，例如图片、视频、日志文件、备份数据和容器/虚拟机镜像等，而一个对象文件可以是任意大小，从几kb到最大5T不等。MinIO是一个非常轻量的服务,可以很简单的和其他应用的结合，类似 NodeJS, Redis 或者 MySQL。 http://docs.minio.org.cn/docs/ MinIO 是一个基于Apache License v2.0开源协议的对象存储服务。它兼容亚马逊S3云存储服务接口，非常适合于存储大容量非结构化的数据，例如图片、视频、日志文件、备份数据和容器/虚拟机镜像等，而一个对象文件可以是任意大小，从几kb到最大5T不等。 MinIO是一个非常轻量的服务,可以很简单的和其他应用的结合，类似 NodeJS, Redis 或者 MySQL。 安装和启动1234wget http://dl.minio.org.cn/server/minio/release/darwin-amd64/miniochmod 755 minio./minio server /datanohup ./minio server /home/minio/data &gt; /home/minio/data/minio.log 2&gt;&amp;1 &amp;","categories":[{"name":"工具","slug":"工具","permalink":"http://www.lights8080.com/categories/%E5%B7%A5%E5%85%B7/"}],"tags":[{"name":"开源产品","slug":"开源产品","permalink":"http://www.lights8080.com/tags/%E5%BC%80%E6%BA%90%E4%BA%A7%E5%93%81/"}]},{"title":"IDEA快捷键(Mac)","slug":"工具/IDEA快捷键(mac)","date":"2021-07-08T16:00:00.000Z","updated":"2021-08-03T01:47:54.000Z","comments":true,"path":"p/a73ed2ff.html","link":"","permalink":"http://www.lights8080.com/p/a73ed2ff.html","excerpt":"IDEA快捷键(Mac)","text":"IDEA快捷键(Mac) 菜单栏功能Edit - Find - Find Usages 查看一个Java类、方法或变量的直接使用情况右键 - Diagrams - Show Disgram… 查看类关系图 查找FindCommand + R 替换文本Command + F 查找文本Ctrl + Shift + F 全局文件中查找Ctrl + Shift + R 全局文件中替换Alt + F7 查找当前变量的使用，并列表显示Command + Alt + F7 查找当前变量的使用，并直接对话框显示Command + F7 查询当前元素在当前文件中的引用，然后按F3可以选择Command + Shift + F7 高亮显示所有该文本，按Esc高亮消失Command + F3 逐个往下查找相同文本 导航NavigateCommand + N 查找类文件Command + Shift + N 查找文件Command + E 最近的文件Command + Shift + E 最近修改的文件Command + Alt + B 跳转到方法实现处Command + Shift + Backspace 跳转到上次编辑的地方Command + G 快速定位文件到某行Command + B 定位至变量定义的位置Command + F12 显示当前文件的文件结构Command + Alt + F12 显示当前文件的路径，并可以方便的将相关父路径打开F2 定位至下一个错误处（高亮错误或警告快速定位）Shift + F2 定位至前一个错误处F3 查找下一个 重构Command + Alt + Shift + T 弹出重构菜单Shift + F6 重命名Command + F6 变更函数签名Command + Alt + V 可以引入变量Command + Alt + O 优化导入的类和包Command + Alt + T 可以把代码包在一个块内，例如：try/catchCommand + Alt + L 格式化代码Command + Alt + I 将选中的代码进行自动缩进编排Command + O 重写方法F6 移动F5 复制 提示Command + P 参数提示Alt + F1 查找代码所在位置 快捷操作Command + Y/X 删除当前行Command + D 复制行Alt + Shift + 向下箭头 将光标所在的行向下整体移动Alt + Shift + 向上箭头 将光标所在的行向上整体移动Command + Shift + 向下箭头 将光标所在的代码块向下整体移动Command + Shift + 向上箭头 将光标所在的代码块向上整体移动Command + Shift + U 大小写转化Command + Shift + C 复制路径Command + / 注解Command + W 选中整个单词Command + [ 快速跳转至代码块的开始处Command + ] 快速跳转至代码块的结尾处Command + Shift + Enter 代码快速补全Command + Alt + Enter 在当前行上方插入新行Shift + Enter 在当前行的下方开始新行 折叠FoldingCommand + = 展开代码Command + - 折叠代码Command + Shift + = 展开所有代码Command + Shift + - 折叠所有代码 窗口Command + Shift + F12 隐藏/恢复所有窗口Alt + 1 项目Alt + 2 收藏Alt + 6 TODOAlt + 7 结构 调试Command + F2，停止Alt + Shift + F9 选择 DebugAlt + Shift + F10 选择 RunCommand + Shift + F8 查看断点F8 步过F7 步入 Jetbrains IDEA 激活https://zhile.io/2018/08/25/jetbrains-license-server-crack.html","categories":[{"name":"工具","slug":"工具","permalink":"http://www.lights8080.com/categories/%E5%B7%A5%E5%85%B7/"}],"tags":[{"name":"IDEA","slug":"IDEA","permalink":"http://www.lights8080.com/tags/IDEA/"},{"name":"for Mac","slug":"for-Mac","permalink":"http://www.lights8080.com/tags/for-Mac/"}]},{"title":"Apache common-pool","slug":"技术/JAVA/Apache common-pool","date":"2021-07-08T16:00:00.000Z","updated":"2021-08-03T01:47:54.000Z","comments":true,"path":"p/d1ef64cd.html","link":"","permalink":"http://www.lights8080.com/p/d1ef64cd.html","excerpt":"对象池介绍","text":"对象池介绍 对象生命周期 Config详解 maxActive: 链接池中最大连接数,默认为8. maxIdle: 链接池中最大空闲的连接数,默认为8. minIdle: 连接池中最少空闲的连接数,默认为0. maxWait: 当连接池资源耗尽时，调用者最大阻塞的时间，超时将跑出异常。单位，毫秒数;默认为-1.表示永不超时. minEvictableIdleTimeMillis: 连接空闲的最小时间，达到此值后空闲连接将可能会被移除。负值(-1)表示不移除。 softMinEvictableIdleTimeMillis: 连接空闲的最小时间，达到此值后空闲链接将会被移除，且保留“minIdle”个空闲连接数。默认为-1. numTestsPerEvictionRun: 对于“空闲链接”检测线程而言，每次检测的链接资源的个数。默认为3. testOnBorrow: 向调用者输出“链接”资源时，是否检测是有有效，如果无效则从连接池中移除，并尝试获取继续获取。默认为false。建议保持默认值. testOnReturn: 向连接池“归还”链接时，是否检测“链接”对象的有效性。默认为false。建议保持默认值. testWhileIdle: 向调用者输出“链接”对象时，是否检测它的空闲超时；默认为false。如果“链接”空闲超时，将会被移除。建议保持默认值. timeBetweenEvictionRunsMillis: “空闲链接”检测线程，检测的周期，毫秒数。如果为负值，表示不运行“检测线程”。默认为-1. whenExhaustedAction: 当“连接池”中active数量达到阀值时，即“链接”资源耗尽时，连接池需要采取的手段, 默认为1： 0 : 抛出异常， 1 : 阻塞，直到有可用链接资源 2 : 强制创建新的链接资源 代码说明 1. 整个方案从ObjectPool，PooledObjectFactory和PooledObject三个接口展开， 2. 其中ObjectPool定义了对象池要实现的功能【比如怎么存取，怎么过期】； 3. PooledObjectFactory定义了被池化的对象的创建，初始化，激活，钝化以及销毁功能； 4. PooledObject定了一被池化对象的一些附加信息【创建时间，池中状态】； 5. 大概流程就是由PooledObjectFactory创建的对象经过PooledObject的包装然后放到ObjectPool里面来。 ObjectPool12345678910111213141516//从池中获取对象T borrowObject() throws Exception, NoSuchElementException,IllegalStateException;//将对象放回池中void returnObject(T obj) throws Exception;//废弃对象void invalidateObject(T obj) throws Exception;//添加对象void addObject() throws Exception, IllegalStateException,UnsupportedOperationException;//获取对象个数int getNumIdle();//获取活跃对象个数int getNumActive();//清除池，池可用void clear() throws Exception, UnsupportedOperationException;//关闭池，池不可用void close(); PooledObjectFactory12345678910//创建一个新对象;当对象池中的对象个数不足时,将会使用此方法来&quot;输出&quot;一个新的&quot;对象&quot;,并交付给对象池管理PooledObject&lt;T&gt; makeObject() throws Exception;//销毁对象,如果对象池中检测到某个&quot;对象&quot;idle的时间超时,或者操作者向对象池&quot;归还对象&quot;时检测到&quot;对象&quot;已经无效,那么此时将会导致&quot;对象销毁&quot;;&quot;销毁对象&quot;的操作设计相差甚远,但是必须明确:当调用此方法时,&quot;对象&quot;的生命周期必须结束.如果object是线程,那么此时线程必须退出;如果object是socket操作,那么此时socket必须关闭;如果object是文件流操作,那么此时&quot;数据flush&quot;且正常关闭.void destroyObject(PooledObject&lt;T&gt; p) throws Exception;//检测对象是否&quot;有效&quot;;Pool中不能保存无效的&quot;对象&quot;,因此&quot;后台检测线程&quot;会周期性的检测Pool中&quot;对象&quot;的有效性,如果对象无效则会导致此对象从Pool中移除,并destroy;此外在调用者从Pool获取一个&quot;对象&quot;时,也会检测&quot;对象&quot;的有效性,确保不能讲&quot;无效&quot;的对象输出给调用者;当调用者使用完毕将&quot;对象归还&quot;到Pool时,仍然会检测对象的有效性.所谓有效性,就是此&quot;对象&quot;的状态是否符合预期,是否可以对调用者直接使用;如果对象是Socket,那么它的有效性就是socket的通道是否畅通/阻塞是否超时等.boolean validateObject(PooledObject&lt;T&gt; p);// &quot;激活&quot;对象,当Pool中决定移除一个对象交付给调用者时额外的&quot;激活&quot;操作,比如可以在activateObject方法中&quot;重置&quot;参数列表让调用者使用时感觉像一个&quot;新创建&quot;的对象一样;如果object是一个线程,可以在&quot;激活&quot;操作中重置&quot;线程中断标记&quot;,或者让线程从阻塞中唤醒等;如果object是一个socket,那么可以在&quot;激活操作&quot;中刷新通道,或者对socket进行链接重建(假如socket意外关闭)等.void activateObject(PooledObject&lt;T&gt; p) throws Exception;//&quot;钝化&quot;对象,当调用者&quot;归还对象&quot;时,Pool将会&quot;钝化对象&quot;;钝化的言外之意,就是此&quot;对象&quot;暂且需要&quot;休息&quot;一下.如果object是一个socket,那么可以passivateObject中清除buffer,将socket阻塞;如果object是一个线程,可以在&quot;钝化&quot;操作中将线程sleep或者将线程中的某个对象wait.需要注意的时,activateObject和passivateObject两个方法需要对应,避免死锁或者&quot;对象&quot;状态的混乱.void passivateObject(PooledObject&lt;T&gt; p) throws Exception; PooledObject123456789101112131415161718192021222324T getObject();long getCreateTime();long getActiveTimeMillis();long getIdleTimeMillis();long getLastBorrowTime();long getLastReturnTime();long getLastUsedTime();int compareTo(PooledObject&lt;T&gt; other);boolean equals(Object obj);int hashCode();String toString();//后台清理线程boolean startEvictionTest();boolean endEvictionTest(Deque&lt;PooledObject&lt;T&gt;&gt; idleQueue);boolean allocate();boolean deallocate();void invalidate()void setLogAbandoned(boolean logAbandoned);void use();void printStackTrace(PrintWriter writer);PooledObjectState getState();//自动补偿功能void markAbandoned();void markReturning(); 方案提供了三种类型的pool: GenericObjectPool是一般意义上的对象池； SoftReferenceObjectPool是弱引用的对象池； GenericKeyedObjectPool是具备分组的对象池。 参考开源项目剖析之apache-common-poolapache-common pool的使用","categories":[{"name":"技术","slug":"技术","permalink":"http://www.lights8080.com/categories/%E6%8A%80%E6%9C%AF/"}],"tags":[{"name":"线程池","slug":"线程池","permalink":"http://www.lights8080.com/tags/%E7%BA%BF%E7%A8%8B%E6%B1%A0/"},{"name":"Java","slug":"Java","permalink":"http://www.lights8080.com/tags/Java/"}]},{"title":"SSH免密登录","slug":"技术/其他/SSH免密登录","date":"2021-07-08T16:00:00.000Z","updated":"2021-08-03T01:47:54.000Z","comments":true,"path":"p/baa1a241.html","link":"","permalink":"http://www.lights8080.com/p/baa1a241.html","excerpt":"SSH免密码登录","text":"SSH免密码登录 ###server-1 12345678$ ssh-keygen -t rsa$ cat ~/id_rsa.pub &gt;&gt; ～/.ssh/authorized_keys$ chmod 700 ~/.ssh$ chmod 600 ~/.ssh/authorized_keys$ ssh localhost ###server-2 12345678910$ ssh-keygen -t rsa$ cat ~/id_rsa.pub &gt;&gt; ～/.ssh/authorized_keys$ chmod 700 ~/.ssh$ chmod 600 ~/.ssh/authorized_keys$ ssh localhost# 将server-2公钥粘贴到server-1节点的authorized_keys中$ ssh server-1 &quot;cat &gt;&gt; ~/.ssh/authorized_keys&quot; &lt; ~/.ssh/id_rsa.pub","categories":[{"name":"技术","slug":"技术","permalink":"http://www.lights8080.com/categories/%E6%8A%80%E6%9C%AF/"}],"tags":[{"name":"SSH","slug":"SSH","permalink":"http://www.lights8080.com/tags/SSH/"}]},{"title":"Git学习笔记","slug":"技术/其他/Git学习笔记","date":"2021-07-07T16:00:00.000Z","updated":"2021-08-17T07:42:19.000Z","comments":true,"path":"p/a91eaa72.html","link":"","permalink":"http://www.lights8080.com/p/a91eaa72.html","excerpt":"Git","text":"Git https://codewords.recurse.com/issues/two/git-from-the-inside-out这篇文章解释了 Git 是如何工作的。它假设您对 Git 足够了解，可以使用它来对您的项目进行版本控制。 https://blog.csdn.net/qq_32452623/article/details/78905181Git三大特色之WorkFlow(工作流) https://nvie.com/posts/a-successful-git-branching-model/A successful Git branching model 分支管理Production 分支也就是我们经常使用的Master分支，这个分支最近发布到生产环境的代码，最近发布的Release， 这个分支只能从其他分支合并，不能在这个分支直接修改 Develop 分支这个分支是我们是我们的主开发分支，包含所有要发布到下一个Release的代码，这个主要合并与其他分支，比如Feature分支 Feature 分支这个分支主要是用来开发一个新的功能，一旦开发完成，我们合并回Develop分支进入下一个Release Release分支当你需要一个发布一个新Release的时候，我们基于Develop分支创建一个Release分支，完成Release后，我们合并到Master和Develop分支 Hotfix分支当我们在Production发现新的Bug时候，我们需要创建一个Hotfix, 完成Hotfix后，我们合并回Master和Develop分支，所以Hotfix的改动会进入下一个Release 工作区（Working Directory） git add命令实际上是把文件添加到Stagegit commit就是往master分支上提交更改 常用命令 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950git revert：撤销某次操作，此次操作之前和之后的commit和history都会保留，并且把这次撤销git clean：删除工作目录中所有没有tracked过的文件git reset：返回到某个节点git clone：命令用于从远程主机克隆一个版本库。Git支持https和ssh协议，https每次需要输入密码速度慢git pull：命令用于取回远程主机某个分支的更新，与本地的指定分支合并git push：命令用于将本地分支的更新，推送到远程主机git remote：远程仓库管理git fetch：命令用于将远程主机版本库中更新内容取回本地，可指定分支git log：显示提交历史git merge：合并分支###（分支管理）#基于远程分支“master”，创建本地分支dev$git checkout -b dev origin/master#创建分支并切换。相当于执行git branch dev和git checkout dev两条命令$git checkout -b dev#切换到master分支$git checkout mastergit reset：返回到某个节点#（默认方式）回退到某个版本，只保留源码，取消了commit ，取消了add$git reset --mixed HASH#回到当前分支最新的版本，不保留修改（不可逆）$git reset --hard [HASH]#返回到某个节点。回退了commit的信息，保留修改$git reset --soft HASH#从add中返回到之前的状态，保留修改$git reset -q 文件#列出Git配置$git config --list#设置用户名$git config --global user.name &quot;Jerry Mouse&quot;#设置邮箱ID$git config --global user.email &quot;jerry@yiibai.com&quot;(如果省略--global选项，那么配置是具体当前的Git存储库)git stash #储藏当前工作现场，等以后恢复现场继续工作git stash list #查询保存的工作现场列表git stash apply stash@&#123;0&#125; #恢复指定的stash# -----# 强行拉取远程分支覆盖本地$git reset --hard origin/master# 把本地的dev 分支强制推送到远端 mastergit push origin dev:master -f","categories":[{"name":"技术","slug":"技术","permalink":"http://www.lights8080.com/categories/%E6%8A%80%E6%9C%AF/"}],"tags":[{"name":"Git","slug":"Git","permalink":"http://www.lights8080.com/tags/Git/"}]},{"title":"Linux 2>&1","slug":"技术/Linux/Linux 2>&1","date":"2021-07-06T16:00:00.000Z","updated":"2021-08-03T01:47:54.000Z","comments":true,"path":"p/2c9a2ea4.html","link":"","permalink":"http://www.lights8080.com/p/2c9a2ea4.html","excerpt":"2&gt;&amp;1说明","text":"2&gt;&amp;1说明 2&gt;&amp;1看下这个命令nohup command&gt;/dev/null 2&gt;&amp;1 &amp; nohup 表示可以在你退出帐户之后继续运行相应的进程 &gt; 代表重定向到哪里，例如：echo “123” &gt; /home/123.txt /dev/null 表示空设备文件 2&gt;&amp;1 表示把标准错误重定向到标准输出 0 表示stdin标准输入 1 表示stdout标准输出，系统默认值是1，所以”&gt;/dev/null”等同于 “1&gt;/dev/null” 2 表示stderr标准错误 &amp; 表示该命令以后台的job的形式运行 command&gt;a 2&gt;a VS command&gt;a 2&gt;&amp;1 command&gt;a 2&gt;&amp;1：打开了一次文件a，&amp;1的含义就可以理解为用标准输出的引用 command&gt;a 2&gt;a：打开文件两次，并导致stdout被stderr覆盖 &gt;/dev/null 2&gt;&amp;1 VS 2&gt;&amp;1 &gt;/dev/null &gt;/dev/null 2&gt;&amp;1：标准输出(丢弃)，错误输出(丢弃) 2&gt;&amp;1 &gt;/dev/null：标准输出(丢弃)，错误输出(屏幕)","categories":[{"name":"技术","slug":"技术","permalink":"http://www.lights8080.com/categories/%E6%8A%80%E6%9C%AF/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://www.lights8080.com/tags/Linux/"}]},{"title":"同步订单到ES","slug":"技术/脚本/同步订单到ES","date":"2021-07-06T16:00:00.000Z","updated":"2021-08-03T01:47:54.000Z","comments":true,"path":"p/fb058b43.html","link":"","permalink":"http://www.lights8080.com/p/fb058b43.html","excerpt":"从Mysql-&gt;logstash-&gt;Elasticsearch。支持时间字段时区设置，防止重复执行，错误订单记录。","text":"从Mysql-&gt;logstash-&gt;Elasticsearch。支持时间字段时区设置，防止重复执行，错误订单记录。 sync_order_fix2.sh12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364#!/bin/bashes_script_path=&quot;/opt/sync_elasticsearch/&quot;file_suffix=&quot;order_fix2&quot;LOCK=$(cat $es_script_path/.lock_$file_suffix)if [ $LOCK -eq 1 ];then echo &quot;`date &quot;+%Y-%m-%d %H:%M:%S&quot;` locked&quot; &amp;&amp; exit 1fiecho &quot;1&quot; &gt; $es_script_path/.lock_$file_suffix#DATE_BEGIN=$(cat $es_script_path/.lasttime_$file_suffix)#DATE_END=`date +&quot;%Y-%m-%d %H:%M:%S&quot;`DATE_BEGIN=&quot;2021-01-01 00:00:00&quot;DATE_END=&quot;2021-04-01 00:00:00&quot;ORDERS_SQL=&quot;&quot;&quot;select order_no from db.order_main where gmt_create BETWEEN &#x27;$DATE_BEGIN&#x27; and &#x27;$DATE_END&#x27;&quot;&quot;&quot;successStr=&#x27;&quot;status&quot;:0&#x27;declare -i succ_cdeclare -i fail_cdeclare -i order_cdeclare -i run_csucc_c=0fail_c=0run_c=0ORDERS_RESULT=$(mysql -u xxx -pxxx -h 127.0.0.1 db -e &quot;$ORDERS_SQL&quot;);order_c=$(echo $ORDERS_RESULT|wc -w)if [ $order_c -gt 0 ];then order_c=$order_c-1fiecho &quot;sync order begin. time:$DATE_BEGIN ~ $DATE_END, count:$order_c&quot;for LINE in $ORDERS_RESULTdo if [ &#x27;order_no&#x27; != &quot;$LINE&quot; ];then run_c=$run_c+1 orderdetail_result=$(curl -XPOST -H &quot;Content-Type: application/json&quot; -H &quot;username: _system_sync&quot; -H &quot;Authorization: &quot; -d &#x27;&#123;&quot;orderNo&quot;:&quot;&#x27;$LINE&#x27;&quot;&#125;&#x27; http://10.88.2.1:80/u2c-order-service/orderDetail -s) if [[ $orderdetail_result =~ $successStr ]];then echo &quot;$orderdetail_result&quot; &gt; $es_script_path/.data_$file_suffix cat $es_script_path/.data_$file_suffix |jq &#x27;.orderInfo&#x27; -c &gt;$es_script_path/.data_1_$file_suffix sed -i &#x27;s/[0-9]\\&#123;4\\&#125;-[0-9]\\&#123;2\\&#125;-[0-9]\\&#123;2\\&#125; [0-9]\\&#123;2\\&#125;:[0-9]\\&#123;2\\&#125;:[0-9]\\&#123;2\\&#125;/&amp; +0800/g&#x27; $es_script_path/.data_1_$file_suffix logstash_result=$(curl -XPOST -H &quot;Content-Type: application/json&quot; http://10.88.2.1:4000 -d@$es_script_path/.data_1_$file_suffix -s) if [ &quot;ok&quot; == $logstash_result ];then echo &quot;$order_c/$run_c $LINE ok&quot; succ_c=$succ_c+1 else echo &quot;$LINE&quot; &gt;&gt; $es_script_path/failure_$file_suffix echo &quot;$order_c/$run_c $LINE fail -&gt; $logstash_result&quot; fail_c=$fail_c+1 fi else echo &quot;$LINE&quot; &gt;&gt; $es_script_path/failure_$file_suffix echo &quot;$order_c/$run_c $LINE fail -&gt; $orderdetail_result&quot; fail_c=$fail_c+1 fi fidoneecho &quot;sync order end. time:$DATE_BEGIN ~ $DATE_END, count:$order_c, succ:$succ_c, fail:$fail_c&quot;echo $DATE_END &gt; $es_script_path/.lasttime_$file_suffixecho &quot;0&quot; &gt; $es_script_path/.lock_$file_suffix","categories":[{"name":"技术","slug":"技术","permalink":"http://www.lights8080.com/categories/%E6%8A%80%E6%9C%AF/"}],"tags":[{"name":"shell","slug":"shell","permalink":"http://www.lights8080.com/tags/shell/"},{"name":"脚本","slug":"脚本","permalink":"http://www.lights8080.com/tags/%E8%84%9A%E6%9C%AC/"}]},{"title":"ELK-实践20210629","slug":"技术/ELK/ELK-实践20210629","date":"2021-06-28T16:00:00.000Z","updated":"2021-08-03T01:47:54.000Z","comments":true,"path":"p/3da25304.html","link":"","permalink":"http://www.lights8080.com/p/3da25304.html","excerpt":"复合聚合按计数排序，TSVB使用脚本字段，滚动升级，Elastic中国社区官方博客","text":"复合聚合按计数排序，TSVB使用脚本字段，滚动升级，Elastic中国社区官方博客 1. 复合聚合按计数排序123456789101112131415161718192021222324252627282930313233343536373839404142434445GET /service-exception/_search&#123; &quot;size&quot;: 0, &quot;query&quot;: &#123; &quot;range&quot;: &#123; &quot;@timestamp&quot;: &#123; &quot;gte&quot;: &quot;2021-05-06 00:00:00&quot;, &quot;lte&quot;: &quot;2021-05-06 23:59:59&quot;, &quot;format&quot;: &quot;yyyy-MM-dd HH:mm:ss&quot;, &quot;time_zone&quot;:&quot;+08:00&quot; &#125; &#125; &#125;, &quot;aggs&quot;: &#123; &quot;group_by_fields&quot;: &#123; &quot;composite&quot;:&#123; &quot;sources&quot;: [ &#123; &quot;service_name&quot;: &#123; &quot;terms&quot;: &#123; &quot;field&quot;: &quot;service_name&quot; &#125; &#125; &#125;,&#123; &quot;error_message&quot;: &#123; &quot;terms&quot;: &#123; &quot;field&quot;: &quot;error_message&quot; &#125; &#125; &#125; ], &quot;size&quot;: 50 &#125;, &quot;aggs&quot;: &#123; &quot;top_bucket_sort&quot;:&#123; &quot;bucket_sort&quot;: &#123; &quot;sort&quot;: [ &#123; &quot;_count&quot;: &#123; &quot;order&quot;: &quot;desc&quot; &#125; &#125; ] &#125; &#125; &#125; &#125; &#125;&#125; 2. TSVB使用脚本字段（[TSVB] Visualize Scripted Fields）在7.13版本中支持 https://github.com/elastic/kibana/issues/13928https://github.com/elastic/kibana/issues/11999https://github.com/elastic/kibana/issues/82438 3. 滚动升级（Rolling Upgrade Elasticsearch）https://www.elastic.co/guide/en/elasticsearch/reference/current/setup-upgrade.html 注意事项： 不支持在升级期间以外在同一集群中运行多个版本的Elasticsearch，因为不能将分片从升级的节点复制到运行旧版本的节点。 升级时将集群的节点划分为两组并按顺序升级组（第一组：Nodes that are not Master-eligible，第二组：Master-eligible nodes），可以确保所有Master-ineligible节点都能够加入集群，无论符合Master-eligible节点是否已经升级。 一旦您开始将集群升级到指定版本，您就必须完成升级，它可能对其内部状态进行无法恢复的更改 升级步骤： 禁用分片分配 停止非必要的索引和执行同步刷新(可选) 暂时停止与活动机器学习作业和数据源相关的任务(可选) 关闭单个节点 升级关闭的节点(如果没有使用外部数据目录，请将旧数据目录复制到新安装) 升级任何插件 启动升级后的节点 重新启用分片分配 等待节点恢复 对于需要更新的每个节点重复这些步骤 重新启动机器学习作业 检查 123456789101112131415161718192021222324252627282930313233# 升级时将集群的节点划分为两组并按顺序升级组GET /_nodes/_all,master:falseGET /_nodes/master:true# 禁用分片分配PUT _cluster/settings&#123; &quot;persistent&quot;: &#123; &quot;cluster.routing.allocation.enable&quot;: &quot;primaries&quot; &#125;&#125;# 停止非必要的索引和执行同步刷新POST _flush/synced# 重新启用分片分配PUT _cluster/settings&#123; &quot;persistent&quot;: &#123; &quot;cluster.routing.allocation.enable&quot;: null &#125;&#125;# 等待节点恢复GET _cat/health?v=true# 对于需要更新的每个节点重复这些步骤GET /_cat/health?v=trueGET /_cat/nodes?h=ip,name,version&amp;v=true# 重新启动机器学习作业POST _ml/set_upgrade_mode?enabled=false 4. 数据增强数据增强有很多种方式 数据源处理 自定义脚本 Logstash Filter ES pipeline 5. Elastic中国社区官方博客Kibana：如何让用户匿名访问 Kibana 中的 DashboardElasticsearch：运用 Python 来实现对搜索结果的分页","categories":[{"name":"技术","slug":"技术","permalink":"http://www.lights8080.com/categories/%E6%8A%80%E6%9C%AF/"}],"tags":[{"name":"ELK","slug":"ELK","permalink":"http://www.lights8080.com/tags/ELK/"}]},{"title":"JAVA面向对象编程思想","slug":"技术/JAVA/JAVA面向对象编程思想","date":"2021-06-23T16:00:00.000Z","updated":"2021-08-03T01:47:54.000Z","comments":true,"path":"p/392c8f8d.html","link":"","permalink":"http://www.lights8080.com/p/392c8f8d.html","excerpt":"翻看之前总结的文本，整理时在重新理解这些概念，发现编程思想真的非常重要，是怎么把复杂的逻辑梳理结构化的过程。","text":"翻看之前总结的文本，整理时在重新理解这些概念，发现编程思想真的非常重要，是怎么把复杂的逻辑梳理结构化的过程。 如何理解面向对象编程只定义业务的框架，而不去实现具体业务功能。就像盖楼房，有楼梯、电梯、门的位置等；只提供了一个框架和公共设施，不去管住户室内的设计，让住户按照自己的需求去实现。如Struts：提供了MVC的流程架构，具体的展示和业务实现交给用户处理。 面向接口编程：实现应用层与实现层的分离，提高系统维护和扩展。通过接口约束对象的属性和行为，是面向对象的一部分。 面向对象编程优点：1）易维护：代码可读性高，维护成本低2）易扩展：由于封装、继承、多态的特征设计出高内聚、低耦合的系统架构，提高系统灵活性和扩展性。 耦合：模块之间的关联强度应该是比较弱的，即低耦合。内聚：模块内的各个元素的联系时紧密的，即高内聚。面向对象编程是一种思想。主要特征是：封装、继承、多态、抽象。5个设计原则。 面向对象编程特征： 封装：把过程和数据包围起来，通过已定义的界面访问数据。提高代码重用性。 继承：对象的一个新类可以从现有的类中派生，这个过程叫做继承。新类继承了原有类的属性和方法。提高代码重用性。 多态：允许不同类的对象对同一消息做出响应。提高灵活性和重用性。 抽象：分离对象的行为和实现。 面向对象设计原则： 单一职责原则(Single Responsibility Principle-SRP)：一个对象只有一种单一的职责。过多的职责会使代码牵一发而动全身。 开放封闭原则(Open-Closed Principle-OCP)：对扩展开放，对修改封闭。是面向对象所有原则的核心。 依赖倒置原则(Dependency Inversion Principle-DIP)：核心思想是依赖于抽象，不依赖于具体。是面向对象设计的精髓。抽象的稳定性决定了系统的稳定性，因为抽象是不变的。 接口隔离原则(Interface Segregation Principle-ISP)：定义许多特定的接口好过定义一个通用的接口。 里氏替换原则(Liskvo Substitution Principle-LSP)：程序中子类必须能够替换其父类，这是保证继承复用的基础。是关于继承机制的设计原则。保证系统具有良好的拓展性，同时实现基于多态的抽象机制，能够减少代码冗余，避免运行期的类型判别。 迪米特法则(Law of Demeter or Least Knowlegde Principle-LoD or LKP)：一个对象应该尽可能少的去了解其他对象。松耦合原则。","categories":[{"name":"技术","slug":"技术","permalink":"http://www.lights8080.com/categories/%E6%8A%80%E6%9C%AF/"}],"tags":[{"name":"JAVA","slug":"JAVA","permalink":"http://www.lights8080.com/tags/JAVA/"},{"name":"编程思想","slug":"编程思想","permalink":"http://www.lights8080.com/tags/%E7%BC%96%E7%A8%8B%E6%80%9D%E6%83%B3/"}]},{"title":"Maven学习笔记","slug":"技术/JAVA/Maven学习笔记","date":"2021-06-23T16:00:00.000Z","updated":"2021-08-03T01:47:54.000Z","comments":true,"path":"p/be02a930.html","link":"","permalink":"http://www.lights8080.com/p/be02a930.html","excerpt":"依赖范围、依赖原则、Maven插件和生命周期、插件说明、Maven模板、Maven生成archetype项目","text":"依赖范围、依赖原则、Maven插件和生命周期、插件说明、Maven模板、Maven生成archetype项目 依赖范围Maven在编译、测试、运行环境下都需要独立的一套classpath。 compile：（默认）编译依赖范围，对于编译、测试、运行三种classpath都有效。典型的是spring-core，三种环境都使用。 test：测试依赖范围，只对于测试classpath有效。典型的是Junit，测试时使用。 provided：已提供依赖范围，对于编译和测试classpath有效。典型的是servlet-api，容器已提供。 runtime：运行时依赖范围，对于测试和运行classpath有效。典型的是JDBC驱动实现，编译时JDK提供了JDBC接口。 system：系统依赖范围，往往与本地系统绑定，造成不可移植性，谨慎使用。 依赖原则传递性依赖：A项目–依赖于–&gt;B项目–依赖于–&gt;C项目。A项目对于C项目是传递性依赖。依赖调解：A–&gt;B–&gt;C–&gt;X(1.0)，A–&gt;D–&gt;X(2.0)。传递性依赖有可能造成依赖问题。这时候依赖路径上有两个X版本，该依赖那条路径下的X项目呢？路径选择原则，第一原则：路径最近者优先，第二原则：第一声明者优先。可选依赖：A–&gt;B，B–&gt;X(可选)，B–&gt;Y(可选)。可选依赖只会对当前项目B产生影响，依赖不会被传递，A不会有任何影响。例如：X为mysql驱动包，Y为oracle驱动包，这种情况只能依赖一种数据库驱动包。POM代码：true。推荐采用单一职责原则。为mysql和oracle分别建立Maven项目。就不存在可选依赖了。 Maven插件和生命周期Maven插件执行详解： maven-clean-plugin:2.5:clean (default-clean) 删除target目录 maven-resources-plugin:2.6:resources (default-resources) 拷贝主程序配置文件到target/classes目录 maven-compiler-plugin:2.5.1:compile (default-compile) 编译主程序目录java文件到target/classes目录 maven-resources-plugin:2.6:testResources (default-testResources) 拷贝测试程序配置文件到target/classes目录 maven-compiler-plugin:2.5.1:testCompile (default-testCompile) 编译测试程序目录java文件到target/classes目录 maven-surefire-plugin:2.12.4:test (default-test) 运行测试用例 maven-jar-plugin:2.4:jar (default-jar) 将项目主代码打包成jar文件（不包含测试程序和测试配置文件） maven-install-plugin:2.4:install (default-install) 将项目输出的jar文件安装到Maven本地仓库中 mvn命令执行过程： mvn clean default-clean mvn compile default-resources–&gt; default-compile mvn test mvn compile–&gt; default-testResources–&gt; default-testCompile–&gt; default-test mvn package mvn test–&gt; default-jar mvn install mvn package–&gt; default-install mvn deploy 项目构建输出的构件部署到对应的远程仓库 mvn clean install-U 强制让Maven检查更新 mvn help:describe -Dplugin=compiler 获取插件compiler的描述信息 mvn dependency:tree 获得项目依赖树 插件说明1、设置源文件编码方式2、解决资源文件的编码问题3、配置多个源文件夹4、打包source文件为jar文件5、拷贝依赖的jar包到目录6、生成源代码包7、打包jar文件时，配置manifest文件，加入lib包的jar依赖8、编译java文件使用依赖本地jar包9、打war包配置项目webContext相对路径、排除文件10、生成可执行jar文件111、生成可执行jar文件212、编译错误（错误: 不兼容的类型）增加配置13、跳过测试 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201&lt;!-- 解决资源文件的编码问题 --&gt;&lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-resources-plugin&lt;/artifactId&gt; &lt;version&gt;2.3&lt;/version&gt; &lt;configuration&gt; &lt;encoding&gt;UTF-8&lt;/encoding&gt; &lt;/configuration&gt;&lt;/plugin&gt;&lt;!-- 打包source文件为jar文件 --&gt;&lt;plugin&gt; &lt;artifactId&gt;maven-source-plugin&lt;/artifactId&gt; &lt;version&gt;2.1&lt;/version&gt; &lt;configuration&gt; &lt;attach&gt;true&lt;/attach&gt; &lt;encoding&gt;UTF-8&lt;/encoding&gt; &lt;/configuration&gt; &lt;executions&gt; &lt;execution&gt; &lt;phase&gt;compile&lt;/phase&gt; &lt;goals&gt; &lt;goal&gt;jar&lt;/goal&gt; &lt;/goals&gt; &lt;/execution&gt; &lt;/executions&gt;&lt;/plugin&gt;&lt;!-- 拷贝依赖的jar包到目录 --&gt;&lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-dependency-plugin&lt;/artifactId&gt; &lt;executions&gt; &lt;execution&gt; &lt;id&gt;copy&lt;/id&gt; &lt;phase&gt;package&lt;/phase&gt; &lt;goals&gt; &lt;goal&gt;copy-dependencies&lt;/goal&gt; &lt;/goals&gt; &lt;configuration&gt; &lt;outputDirectory&gt; $&#123;project.build.directory&#125;/dependency &lt;/outputDirectory&gt; &lt;includeScope&gt;runtime&lt;/includeScope&gt; &lt;excludeScope&gt;provided&lt;/excludeScope&gt; &lt;/configuration&gt; &lt;/execution&gt; &lt;/executions&gt;&lt;/plugin&gt;&lt;!-- 设置源文件编码方式 --&gt;&lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-compiler-plugin&lt;/artifactId&gt; &lt;configuration&gt; &lt;defaultLibBundleDir&gt;lib&lt;/defaultLibBundleDir&gt; &lt;source&gt;1.6&lt;/source&gt; &lt;target&gt;1.6&lt;/target&gt; &lt;encoding&gt;UTF-8&lt;/encoding&gt; &lt;/configuration&gt;&lt;/plugin&gt;&lt;!-- 打包jar文件时，配置manifest文件，加入lib包的jar依赖 --&gt;&lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-jar-plugin&lt;/artifactId&gt; &lt;configuration&gt; &lt;archive&gt; &lt;manifest&gt; &lt;addClasspath&gt;true&lt;/addClasspath&gt; &lt;classpathPrefix&gt;lib/&lt;/classpathPrefix&gt; &lt;mainClass&gt;.....MonitorMain&lt;/mainClass&gt; &lt;/manifest&gt; &lt;/archive&gt; &lt;/configuration&gt;&lt;/plugin&gt;&lt;!-- 配置多个源文件夹 --&gt;&lt;plugin&gt; &lt;groupId&gt;org.codehaus.mojo&lt;/groupId&gt; &lt;artifactId&gt;build-helper-maven-plugin&lt;/artifactId&gt; &lt;executions&gt; &lt;execution&gt; &lt;phase&gt;generate-sources&lt;/phase&gt; &lt;goals&gt; &lt;goal&gt;add-source&lt;/goal&gt; &lt;/goals&gt; &lt;configuration&gt; &lt;sources&gt; &lt;source&gt;src/utils&lt;/source&gt; &lt;source&gt;src/platform&lt;/source&gt; &lt;/sources&gt; &lt;/configuration&gt; &lt;/execution&gt; &lt;/executions&gt;&lt;/plugin&gt;&lt;!-- 编译java文件使用依赖本地jar包 --&gt;&lt;plugin&gt; &lt;artifactId&gt;maven-compiler-plugin&lt;/artifactId&gt; &lt;configuration&gt; &lt;source&gt;1.6&lt;/source&gt; &lt;target&gt;1.6&lt;/target&gt; &lt;encoding&gt;UTF-8&lt;/encoding&gt; &lt;compilerArguments&gt; &lt;extdirs&gt;src\\main\\webapp\\WEB-INF\\lib或$&#123;basedir&#125;/WebContent/WEB-INF/lib&lt;/extdirs&gt; &lt;/compilerArguments&gt; &lt;/configuration&gt;&lt;/plugin&gt;&lt;!-- 打war包配置项目webContext相对路径、排除文件 --&gt;&lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-war-plugin&lt;/artifactId&gt; &lt;version&gt;2.4&lt;/version&gt; &lt;configuration&gt; &lt;!--包含空文件夹--&gt; &lt;includeEmptyDirectories&gt;true&lt;/includeEmptyDirectories&gt; &lt;warSourceDirectory&gt;WebContent&lt;/warSourceDirectory&gt; &lt;!-- 排除文件 --&gt; &lt;packagingExcludes&gt;WEB-INF/classes/**/*.*,WEB-INF/lib/**/*&lt;/packagingExcludes&gt; &lt;!--将类文件打成jar包--&gt; &lt;archiveClasses&gt;true&lt;/archiveClasses&gt; &lt;/configuration&gt;&lt;/plugin&gt;&lt;!-- 生成源代码包 --&gt;&lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-source-plugin&lt;/artifactId&gt; &lt;version&gt;2.1.1&lt;/version&gt; &lt;executions&gt; &lt;execution&gt; &lt;id&gt;attach-sources&lt;/id&gt; &lt;phase&gt;verify&lt;/phase&gt; &lt;goals&gt; &lt;goal&gt;jar-no-fork&lt;/goal&gt; &lt;/goals&gt; &lt;/execution&gt; &lt;/executions&gt;&lt;/plugin&gt;&lt;!-- 生成可执行jar文件1 --&gt;&lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-jar-plugin&lt;/artifactId&gt; &lt;configuration&gt; &lt;archive&gt; &lt;manifest&gt; &lt;addClasspath&gt;true&lt;/addClasspath&gt; &lt;classpathPrefix&gt;lib/&lt;/classpathPrefix&gt; &lt;mainClass&gt;com.abc.ABCTest&lt;/mainClass&gt;&lt;!-- 入口类名 --&gt; &lt;/manifest&gt; &lt;/archive&gt; &lt;/configuration&gt;&lt;/plugin&gt;&lt;!-- 生成可执行jar文件2 执行命令：mvn clean install --&gt;&lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-shade-plugin&lt;/artifactId&gt; &lt;version&gt;1.2.1&lt;/version&gt; &lt;executions&gt; &lt;execution&gt; &lt;phase&gt;package&lt;/phase&gt; &lt;goals&gt; &lt;goal&gt;shade&lt;/goal&gt; &lt;/goals&gt; &lt;configuration&gt; &lt;transformers&gt; &lt;transformer implementation=&quot;org.apache.maven.plugins.shade.resource.ManifestResourceTransformer&quot;&gt; &lt;mainClass&gt;com.lihp.mvnbook.helloworld.HelloWorld&lt;/mainClass&gt; &lt;/transformer&gt; &lt;/transformers&gt; &lt;/configuration&gt; &lt;/execution&gt; &lt;/executions&gt;&lt;/plugin&gt;&lt;!-- 编译错误（错误: 不兼容的类型）增加配置 --&gt;&lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-compiler-plugin&lt;/artifactId&gt; &lt;version&gt;3.1&lt;/version&gt; &lt;configuration&gt; &lt;source&gt;1.7&lt;/source&gt; &lt;target&gt;1.7&lt;/target&gt; &lt;encoding&gt;UTF-8&lt;/encoding&gt; &lt;compilerId&gt;csharp&lt;/compilerId&gt; &lt;compilerArguments&gt; &lt;extdirs&gt;$&#123;lib.dir&#125;&lt;/extdirs&gt; &lt;/compilerArguments&gt; &lt;/configuration&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.codehaus.plexus&lt;/groupId&gt; &lt;artifactId&gt;plexus-compiler-csharp&lt;/artifactId&gt; &lt;version&gt;1.6&lt;/version&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;/plugin&gt;&lt;!-- 跳过测试 --&gt;&lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-surefire-plugin&lt;/artifactId&gt; &lt;version&gt;2.5&lt;/version&gt; &lt;configuration&gt; &lt;skipTests&gt;true&lt;/skipTests&gt; &lt;/configuration&gt; &lt;/plugin&gt; Maven模板 mvn archetype:crawl （创建本地模板库） mvn archetype:generate（使用模板创建项目） 123456789101112# groupId、artifactId、version - 为新建项目的定位信息# archetypeGroupId、archetypeArtifactId、archetypeVersion - 为模板项目的定位信息# archetypeCatalog=local 从本地仓库选取模板项目mvn archetype:generate \\-DgroupId=com.lights \\-DartifactId=lights-order \\-Dversion=2.0.0 \\-DarchetypeCatalog=local \\-DarchetypeGroupId=com.lights \\-DarchetypeArtifactId=lights-springcloud-archetype \\-DarchetypeVersion=2.0.0 \\-DinteractiveMode=false Maven生成archetype项目 进入工程目录执行命令： mvn archetype:create-from-project 进入target/generated-sources/archetype 编辑pom.xml，添加远程发布的Nexus地址 执行命令：mvn deploy 命令创建项目：mvn archetype:generate -DarchetypeCatalog=local 前提配置~/.m2/settings.xml 12345678910&lt;server&gt; &lt;id&gt;releases&lt;/id&gt; &lt;username&gt;admin&lt;/username&gt; &lt;password&gt;admin1212&lt;/password&gt;&lt;/server&gt;&lt;server&gt; &lt;id&gt;snapshots&lt;/id&gt; &lt;username&gt;admin&lt;/username&gt; &lt;password&gt;admin1212&lt;/password&gt;&lt;/server&gt; 前提配置pom.xml 123456789101112&lt;distributionManagement&gt; &lt;repository&gt; &lt;id&gt;releases&lt;/id&gt; &lt;name&gt;Internal Releases&lt;/name&gt; &lt;url&gt;http://192.168.1.23:8081/nexus/content/repositories/releases&lt;/url&gt; &lt;/repository&gt; &lt;snapshotRepository&gt; &lt;id&gt;snapshots&lt;/id&gt; &lt;name&gt;Internal Releases&lt;/name&gt; &lt;url&gt;http://192.168.1.23:8081/nexus/content/repositories/snapshots&lt;/url&gt; &lt;/snapshotRepository&gt;&lt;/distributionManagement&gt;","categories":[{"name":"技术","slug":"技术","permalink":"http://www.lights8080.com/categories/%E6%8A%80%E6%9C%AF/"}],"tags":[{"name":"Java","slug":"Java","permalink":"http://www.lights8080.com/tags/Java/"},{"name":"Maven","slug":"Maven","permalink":"http://www.lights8080.com/tags/Maven/"}]},{"title":"字符集和字符编码","slug":"技术/JAVA/字符集和字符编码","date":"2021-06-23T16:00:00.000Z","updated":"2021-08-03T01:47:54.000Z","comments":true,"path":"p/438d1de3.html","link":"","permalink":"http://www.lights8080.com/p/438d1de3.html","excerpt":"UTF-8、GBK、ASCII、Unicode的区别","text":"UTF-8、GBK、ASCII、Unicode的区别 字符集和字符编码字符集（Charset）：是一个系统支持的所有抽象字符的集合。字符是各种文字和符号的总称，包括各国家文字、标点符号、图形符号、数字等。 字符编码（Character Encoding）：是一套法则，使用该法则能够对自然语言的字符的一个集合（如字母表或音节表），与其他东西的一个集合（如号码或电脉冲）进行配对。 常见字符集名称：ASCII字符集、GB2312字符集、BIG5字符集、GB18030字符集、Unicode字符集等。计算机要准确的处理各种字符集文字，需要进行字符编码，以便计算机能够识别和存储各种文字。 ASCII字符集&amp;编码ASCII（American Standard Code for Information Interchange，美国信息交换标准代码）是基于拉丁字母的一套电脑编码系统。ASCII字符集使用7位（bits）表示一个字符，共128字符，ASCII扩展字符集使用8位（bits）表示一个字符，共256字符。 GBXXXX字符集&amp;编码为了显示中文，必须设计一套编码规则用于将汉字转换为计算机可以接受的数字系统的数。规定：一个小于127的字符的意义与原来相同，但两个大于127的字符连在一起时，就表示一个汉字。连在ASCII里本来就有的数字、标点、字母都统统重新编了两个字节长的编码，这就是常说的”全角”字符，而原来在127号以下的那些就叫”半角”字符了。 GB2312或GB2312-80是中国国家标准简体中文字符集，全称《信息交换用汉字编码字符集·基本集》，又称GB0，由中国国家标准总局发布。对于人名、古汉语等方面出现的罕用字，GB2312不能处理，这导致了后来GBK及GB18030汉字字符集的出现。 UnicodeUnicode编码系统为表达任意语言的任意字符而设计，是国际组织制定的可以容纳所有文字和符号的字符编码方案。它使用4字节的数字来表达每个字母、符号，或者表意文字(ideograph)。其用数字来映射字符，浏览器不能直接展示，只能转换成其他格式的编码（UTF-8、GBK）。Unicode用数字0-0x10FFFF来映射这些字符，最多可以容纳1114112（16^4 * 17） 个字符。 GBK与UTF-8之间必须通过Unicode编码才能相互转换 UTF-8UTF-8（8-bit Unicode Transformation Format）是一种针对Unicode的可变长度字符编码，也是一种前缀码。它可以用来表示Unicode标准中的任何字符，且其编码中的第一个字节仍与ASCII兼容，这使得原来处理ASCII字符的软件无须或只须做少部份修改，即可继续使用。因此，它逐渐成为电子邮件、网页及其他存储或传送文字的应用中，优先采用的编码。互联网工程工作小组（IETF）要求所有互联网协议都必须支持UTF-8编码。 UTF-8：用来解决国际上字符的一种多字节编码，包含全世界所有国家需要用到的字符。通用性比较好。示例：使用UTF-8编码，如果外国人用英文IE访问中文网站，也能显示中文，不需要中文语言包支持。避免乱码问题 UTF-8是ASCII的超集。不需要转换和修改就能和UTF-8一起使用。 GBK与UTF-8之间必须通过Unicode编码才能相互转换。 UTF-8对于英文使用8位（一个字节）编码，对于中文使用24位（三个字节）编码。 一个汉字GBK编码占两个字节，UTF-8编码占三个字节，所以UTF-8对中文来说占用空间要大。","categories":[{"name":"技术","slug":"技术","permalink":"http://www.lights8080.com/categories/%E6%8A%80%E6%9C%AF/"}],"tags":[{"name":"Java","slug":"Java","permalink":"http://www.lights8080.com/tags/Java/"},{"name":"字符集","slug":"字符集","permalink":"http://www.lights8080.com/tags/%E5%AD%97%E7%AC%A6%E9%9B%86/"}]},{"title":"阻塞、非阻塞、异步","slug":"技术/JAVA/阻塞、非阻塞、异步","date":"2021-06-23T16:00:00.000Z","updated":"2021-08-03T01:47:54.000Z","comments":true,"path":"p/f43c958b.html","link":"","permalink":"http://www.lights8080.com/p/f43c958b.html","excerpt":"","text":"阻塞调用请求过来，要建立连接，然后再接收数据，接收数据后，再发送数据。具体到系统底层，就是读写事件，而当读写事件没有准备好时，必然不可操作，如果不用非阻塞的方式来调用，那就得阻塞调用了，事件没有准备好，那就只能等了，等事件准备好了，你再继续吧。cpu空闲没人用，cpu利用率自然上不去了，更别谈高并发了。好吧，你说加进程数，这跟apache的线程模型有什么区别，注意，别增加无谓的上下文切换。 非阻塞调用非阻塞就是，事件没有准备好，马上返回EAGAIN，告诉你，事件还没准备好呢，你慌什么，过会再来吧。好吧，你过一会，再来检查一下事件，直到事件准备好了为止，在这期间，你就可以先去做其它事情，然后再来看看事件好了没。虽然不阻塞了，但你得不时地过来检查一下事件的状态，你可以做更多的事情了，但带来的开销也是不小的。 异步非阻塞调用它们提供了一种机制，让你可以同时监控多个事件，调用他们是阻塞的，但可以设置超时时间，在超时时间之内，如果有事件准备好了，就返回。拿epoll为例(在后面的例子中，我们多以epoll为例子，以代表这一类函数)，当事件没准备好时，放到epoll里面，事件准备好了，我们就去读写，当读写返回EAGAIN时，我们将它再次加入到epoll里面。这样，只要有事件准备好了，我们就去处理它，只有当所有事件都没准备好时，才在epoll里面等着。这样，我们就可以并发处理大量的并发了，当然，这里的并发请求，是指未处理完的请求，线程只有一个，所以同时能处理的请求当然只有一个了，只是在请求间进行不断地切换而已，切换也是因为异步事件未准备好，而主动让出的。这里的切换是没有任何代价，你可以理解为循环处理多个准备好的事件，事实上就是这样的。与多线程相比，这种事件处理方式是有很大的优势的，不需要创建线程，每个请求占用的内存也很少，没有上下文切换，事件处理非常的轻量级。并发数再多也不会导致无谓的资源浪费（[link]上下文切换 ）。 异步非阻塞的理解http://blog.csdn.net/feitianxuxue/article/details/8936802 阻塞，非阻塞：关注的是程序在等待调用结果（消息或返回值）时的状态。进程/线程要访问的数据是否就绪，进程/线程是否需要等待； 同步，异步：关注的是消息通信机制。访问数据的方式，同步需要主动读写数据，在读写数据的过程中还是会阻塞；异步只需要I/O操作完成的通知，并不主动读写数据，由操作系统内核完成数据的读写。 故事(老张爱喝茶)1234567知乎老张爱喝茶，废话不说，煮开水。出场人物：老张，水壶两把（普通水壶，简称水壶；会响的水壶，简称响水壶）。1 老张把水壶放到火上，立等水开。（同步阻塞）老张觉得自己有点傻2 老张把水壶放到火上，去客厅看电视，时不时去厨房看看水开没有。（同步非阻塞）老张还是觉得自己有点傻，于是变高端了，买了把会响笛的那种水壶。水开之后，能大声发出嘀~~~~的噪音。3 老张把响水壶放到火上，立等水开。（异步阻塞）老张觉得这样傻等意义不大4 老张把响水壶放到火上，去客厅看电视，水壶响之前不再去看它了，响了再去拿壶。（异步非阻塞）老张觉得自己聪明了。所谓同步异步，只是对于水壶而言。普通水壶，同步；响水壶，异步。虽然都能干活，但响水壶可以在自己完工之后，提示老张水开了。这是普通水壶所不能及的。同步只能让调用者去轮询自己（情况2中），造成老张效率的低下。所谓阻塞非阻塞，仅仅对于老张而言。立等的老张，阻塞；看电视的老张，非阻塞。情况1和情况3中老张就是阻塞的，媳妇喊他都不知道。虽然3中响水壶是异步的，可对于立等的老张没有太大的意义。所以一般异步是配合非阻塞使用的，这样才能发挥异步的效用。","categories":[{"name":"技术","slug":"技术","permalink":"http://www.lights8080.com/categories/%E6%8A%80%E6%9C%AF/"}],"tags":[{"name":"Java","slug":"Java","permalink":"http://www.lights8080.com/tags/Java/"},{"name":"多线程","slug":"多线程","permalink":"http://www.lights8080.com/tags/%E5%A4%9A%E7%BA%BF%E7%A8%8B/"}]},{"title":"Linux使用总结","slug":"技术/Linux/Linux使用总结","date":"2021-06-23T16:00:00.000Z","updated":"2021-08-03T01:47:54.000Z","comments":true,"path":"p/9c47b2af.html","link":"","permalink":"http://www.lights8080.com/p/9c47b2af.html","excerpt":"","text":"常用命令12345678910111213141516171819202122232425262728293031# 查找目录下的所有文件中是否含有某个字符串,只打印出文件名称find . | xargs grep -ri &#x27;string&#x27; -l# 文件字符串替换sed -i &#x27;s/oldStr/newStr/g&#x27; demo# 将worker-5的异常前后20行保存到文件中cat all.log.2015-08-26 | grep &#x27;schedulerFactoryBean_Worker-5] - 异常信息&#x27; -A 20 -B 20 &gt; worker-5-exception.log# 查询文件中时间段的内容sed -n &#x27;/2014-05-16/,/2014-05-17/p&#x27; catalina.out &gt; 20140516tomcat.loggrep &quot;2014-07-23 13:[00-59]&quot; 20140514tomcat.log &gt;05-14.log# 正则匹配egrep &#x27;(-开始)|(-结束)&#x27; p.log &gt; begin-end.log# 昨天这个时候的时间date -d&quot;yesterday&quot; +&quot;%F %H:%M:%S&quot;# linux 查询端口占用情况netstat -natp|grep 8080# 创建软连接ln -s 目标文件 连接文件#查看CPU性能vmstat 1#显示CPU数，ALL为索引mpstat -P ALL 1#查看I/O性能iostat -m -x 1 Statistics Examples1234567891011121314151617181920212223242526272829### 每小时的访问量统计awk &#x27;&#123;print $5&#125;&#x27; /data/log/nginxlog/nginx.log|awk -F: &#x27;&#123;s[$1&quot;:&quot;$2]++&#125;END&#123;for(a in s)&#123;print a,s[a]&#125;&#125;&#x27;|sort### 每小时状态码统计awk &#x27;&#123;print $5&quot;:&quot;$10&#125;&#x27; /data/log/nginxlog/nginx.log|awk -F: &#x27;&#123;s[$1&quot;:&quot;$2&quot;:&quot;$5]++&#125;END&#123;for(a in s)&#123;print a,s[a]&#125;&#125;&#x27;|sort### 统计search服务每分钟处理时间大于7秒的统计grep &#x27;2021-02-02 00&#x27; /data/log/service/service.2021-02-02.log |grep &#x27;ServiceImpl&#x27;| awk &#x27;&#123;print $1,$2&quot;:&quot;strtonum($12)&#125;&#x27; |awk -F: &#x27;&#123;if($4&gt;7000)&#123;s[$1&quot;:&quot;$2]++&#125;&#125;END&#123;for(a in s)&#123;print a,s[a]&#125;&#125;&#x27;|sort### 每小时统计[hour,gds_ms,jaxb_ms,parser_ms,spendms,count,spendms/1000]grep &#x27;INFO c.f.s.s.api.service.impl.ServiceImpl &#x27; /data/log/service/service.2019-04-24.log \\| awk &#x27;&#123;split($2,time,&quot;:&quot;);split($9,gds,&quot;:&quot;);split($10,jaxb,&quot;:&quot;);split($11,parse,&quot;:&quot;);split($13,count,&quot;:&quot;);&#123;print $1 &quot;-&quot; time[1] &quot; &quot; strtonum(gds[2]) &quot; &quot; strtonum(jaxb[2]) &quot; &quot; strtonum(parse[2]) &quot; &quot; strtonum(count[2])&#125;&#125;&#x27; \\| awk &#x27;&#123;gds[$1]=gds[$1]+$2;jaxb[$1]=jaxb[$1]+$3;parser[$1]=parser[$1]+$4;count[$1]=count[$1]+$5;size[$1]=size[$1]+1&#125;;END&#123;for(i in gds)print i,gds[i],jaxb[i],parser[i],(gds[i]+jaxb[i]+parser[i]),count[i],size[i],(gds[i]+jaxb[i]+parser[i])/1000 | &quot;sort&quot;&#125;&#x27;### 按航线统计grep &#x27;INFO c.f.s.s.api.service.impl.ServiceImpl &#x27; /data/log/service/service.2019-04-24.log \\| awk &#x27;&#123;split($2,time,&quot;:&quot;);split($9,gds,&quot;:&quot;);split($10,jaxb,&quot;:&quot;);split($11,parse,&quot;:&quot;);split($13,count,&quot;:&quot;);&#123;if(strtonum(gds[2])&gt;7000)&#123;print $1 &quot;-&quot; time[1] &quot;#&quot; $7 &quot; &quot; strtonum(gds[2]) &quot; &quot; strtonum(jaxb[2]) &quot; &quot; strtonum(parse[2]) &quot; &quot; strtonum(count[2])&#125;&#125;&#125;&#x27; \\| awk &#x27;&#123;gds[$1]+=$2;jaxb[$1]+=$3;parser[$1]+=$4;count[$1]+=$5;size[$1]+=1&#125;;END&#123;for(i in gds)print (gds[i]+jaxb[i]+parser[i]),(gds[i]+jaxb[i]+parser[i])/size[i],size[i],i | &quot;sort -nr&quot;&#125;&#x27; \\| head -n 100 &gt;&gt; stat### 统计每小时查询量grep &#x27;INFO&#x27; /data/log/service/service.2019-11-11.log|awk -F: &#x27;&#123;sum[$1]+=1&#125;END&#123;for(c in sum)&#123;print c,sum[c]&#125;&#125;&#x27;|sort### 统计单个小时航线查询量grep &#x27;INFO&#x27; /data/log/service/service.2019-11-11.log|awk -F: &#x27;&#123;split($6,fromto,&quot;|&quot;);&#123;print $1,fromto[1]&#125;&#125;&#x27;|awk -F: &#x27;&#123;sum[$1]+=1&#125;END&#123;for(c in sum)&#123;print c,sum[c]&#125;&#125;&#x27;|sort -k 4 -nr|head### 统计每小时超时数grep &#x27;INFO&#x27; /data/log/service/service.2019-11-11.log|awk -F: &#x27;&#123;split($5,detail,&quot;,&quot;);&#123;print $1,strtonum(detail[1])&#125;&#125;&#x27;| awk &#x27;&#123;if(strtonum($3)&gt;7000)&#123;print $1,$2,&quot;timeout&quot;&#125;else&#123;print $1,$2,&quot;ok&quot;&#125;&#125;&#x27;|awk -F: &#x27;&#123;sum[$1]+=1&#125;END&#123;for(c in sum)&#123;print c,sum[c]&#125;&#125;&#x27;|sort","categories":[{"name":"技术","slug":"技术","permalink":"http://www.lights8080.com/categories/%E6%8A%80%E6%9C%AF/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://www.lights8080.com/tags/Linux/"}]},{"title":"Mysql使用总结","slug":"技术/其他/Mysql使用总结","date":"2021-06-23T16:00:00.000Z","updated":"2021-08-03T01:47:54.000Z","comments":true,"path":"p/28b8e966.html","link":"","permalink":"http://www.lights8080.com/p/28b8e966.html","excerpt":"新建用户及授权、性能指标、数据库容量、导入导出、存储过程和事件、sql总结","text":"新建用户及授权、性能指标、数据库容量、导入导出、存储过程和事件、sql总结 新建用户及授权12345678910111213141516-- 添加用户CREATE USER &#x27;username&#x27;@&#x27;host&#x27; IDENTIFIED BY &#x27;password&#x27;;-- 授权GRANT [select,update,delete,create,drop] privileges ON databasename.tablename TO &#x27;username&#x27;@&#x27;host&#x27;;-- 授权示例GRANT all privileges ON *.* TO &#x27;user1&#x27;@&#x27;%&#x27; identified by &#x27;123456&#x27;;-- 刷新系统权限表flush privileges;-- 设置或更改用户密码SET PASSWORD FOR &#x27;username&#x27;@&#x27;host&#x27; = PASSWORD(&#x27;newpassword&#x27;);-- 撤销用户权限REVOKE privilege ON databasename.tablename FROM &#x27;username&#x27;@&#x27;host&#x27;;-- 查看授权信息SHOW GRANTS FOR &#x27;username&#x27;@&#x27;host&#x27;;-- 删除用户DROP USER &#x27;username&#x27;@&#x27;host&#x27;; Examples123456grant all privileges on *.* to root@&#x27;%&#x27; identified by &#x27;123456&#x27;;flush privileges;CREATE USER &#x27;lihaipeng&#x27;@&#x27;%&#x27; IDENTIFIED BY &#x27;123456&#x27;;GRANT all privileges ON *.* TO &#x27;lihaipeng&#x27;@&#x27;%&#x27; identified by &#x27;123456&#x27;;flush privileges; 性能指标1234567891011121314151617181920212223242526-- sql性能分析explain select * from xxx-- 显示正在执行语句show PROCESSLIST;-- 查询表连接数和锁表数show open tables;-- max_connections 最大连接数-- max_user_connections 单用户的最大连接数-- thread_cache_size 线程缓存最大数show variables like &#x27;%max_connections%&#x27;;-- Threads_cached 当前缓存中空闲的连接数量-- Threads_connected 当前打开的连接数量-- Threads_running 不在睡眠的线程数量show status like &#x27;Threads%&#x27;;-- Max_used_connections 同时使用的连接的最大数目-- Connections 试图连接到MySQL(不管是否连接成功)的连接数show status like &#x27;Max_used_connections&#x27;;-- 显示group_concat长度限制show session variables LIKE &#x27;%group_concat_max_len%&#x27;;-- 设置group_concat长度限制SET SESSION group_concat_max_len=512000; 事务和锁1234567891011-- 查看数据库当前的进程show processlist;select * from information_schema.processlist;-- 当前运行的所有事务SELECT * FROM information_schema.INNODB_TRX;-- 查看正在锁的事务SELECT * FROM information_schema.INNODB_LOCKS;-- 查看等待锁的事务SELECT * FROM information_schema.INNODB_LOCK_WAITS;-- 结束线程KILL &lt;trx_mysql_thread_id&gt;; 连接数和超时时间12345678910111213-- 查看服务器状态信息(分为全局和会话，支持like)SHOW global status LIKE &#x27;slow_queries&#x27;;-- 查看系统变量及其值(分为全局和会话，支持like)show global variables LIKE &#x27;%slow_queries%&#x27;;-- 更改全局变量，必须具有SUPER权限-- 最大连接数set global max_connections=1500;-- 关闭一个非交互的连接之前等待秒数 28800set global wait_timeout=600;-- 关闭一个交互的连接之前等待秒数 28800set global interactive_timeout=600;-- 锁等待时间 31536000set global lock_wait_timeout=600; 查看数据库容量123456789101112131415161718192021222324252627282930313233343536373839-- 查看所有数据库容量大小selecttable_schema as &#x27;数据库&#x27;,sum(table_rows) as &#x27;记录数&#x27;,sum(truncate(data_length/1024/1024, 2)) as &#x27;数据容量(MB)&#x27;,sum(truncate(index_length/1024/1024, 2)) as &#x27;索引容量(MB)&#x27;from information_schema.tablesgroup by table_schemaorder by sum(data_length) desc, sum(index_length) desc;-- 查看所有数据库各表容量大小selecttable_schema as &#x27;数据库&#x27;,table_name as &#x27;表名&#x27;,table_rows as &#x27;记录数&#x27;,truncate(data_length/1024/1024, 2) as &#x27;数据容量(MB)&#x27;,truncate(index_length/1024/1024, 2) as &#x27;索引容量(MB)&#x27;from information_schema.tablesorder by data_length desc, index_length desc;-- 查看指定数据库容量大小selecttable_schema as &#x27;数据库&#x27;,sum(table_rows) as &#x27;记录数&#x27;,sum(truncate(data_length/1024/1024, 2)) as &#x27;数据容量(MB)&#x27;,sum(truncate(index_length/1024/1024, 2)) as &#x27;索引容量(MB)&#x27;from information_schema.tableswhere table_schema=&#x27;mysql&#x27;;-- 查看指定数据库各表容量大小selecttable_schema as &#x27;数据库&#x27;,table_name as &#x27;表名&#x27;,table_rows as &#x27;记录数&#x27;,truncate(data_length/1024/1024, 2) as &#x27;数据容量(MB)&#x27;,truncate(index_length/1024/1024, 2) as &#x27;索引容量(MB)&#x27;from information_schema.tableswhere table_schema=&#x27;mysql&#x27;order by data_length desc, index_length desc; 导入/导出12345678910-- 导出指定库所有表结构mysqldump -u root -p234234 -h 192.168.1.75 -d [数据库] &gt; a.sql-- 导出指定数据库所有结构和数据mysqldump -u root -p234234 -h 192.168.1.75 [数据库] &gt; a.sql-- 导出指定数据库，指定表结构mysqldump -u root -p234234 -h 192.168.1.75 -d [数据库] [表] &gt; a.sql-- 导出指定数据库，指定表结构和数据mysqldump -u root -p234234 -h 192.168.1.75 [数据库] [表] &gt; a.sql-- 导入mysql -u root -proot test &lt; cid-fromto-1.sql to csv12345SELECT order_id,product_name,qty FROM ordersINTO OUTFILE &#x27;/tmp/orders.csv&#x27;FIELDS TERMINATED BY &#x27;,&#x27;ENCLOSED BY &#x27;&quot;&#x27;LINES TERMINATED BY &#x27;\\n&#x27; to txt12SELECT order_id,product_name,qty FROM ordersINTO OUTFILE &#x27;/tmp/orders.txt&#x27; Sql统计：mysql中每10分钟统计1234SELECT concat(date_format(createtime,&#x27;%Y-%m-%d %H:&#x27;) , floor( date_format(createtime, &#x27;%i&#x27;)/10)) AS c, count( id )FROM `qm_log`WHERE createtime BETWEEN &#x27;2015-11-24 07:00:00&#x27; and &#x27;2015-11-24 08:59:59&#x27;GROUP BY c 关联表更新1234update fesf_order.order_detail od set od.child_policy_no =(select pg.policy_no from fesf_commission.policy_general pg where pg.id = substring_index(od.child_policy_id,&quot;|&quot;,-1)); 数据库表迁移1234567891011-- 复制表create table database1.table1 like database2.table1;-- 复制表数据insert into database1.table1 select * from database2.table1;-- 数据库改名方案-- 1. 创建目标库CREATE SCHEMA `unififi_security` DEFAULT CHARACTER SET utf8;-- 2. 生成改表名的sqlselect concat(&#x27;rename table &#x27;,TABLE_SCHEMA,&#x27;.&#x27;,TABLE_NAME,&#x27; to target_schema.&#x27;,TABLE_NAME,&#x27;;&#x27;) from information_schema.TABLES where TABLE_SCHEMA=&#x27;origin_schema&#x27;; 数据库改名脚本rename_schema.sh 1234567891011#!/bin/bash# 假设将sakila数据库名改为new_sakila# MyISAM直接更改数据库目录下的文件即可mysql -uroot -p123456 -e &#x27;create database if not exists new_sakila&#x27;list_table=$(mysql -uroot -p123456 -Nse &quot;select table_name from information_schema.TABLES where TABLE_SCHEMA=&#x27;sakila&#x27;&quot;)for table in $list_tabledo mysql -uroot -p123456 -e &quot;rename table sakila.$table to new_sakila.$table&quot;done 库表操作12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970-- 创建库CREATE SCHEMA `lights` DEFAULT CHARACTER SET utf8;-- 删除/创建表drop table `lights`.`scheduler_log`;CREATE TABLE `lights`.`scheduler_log` ( `id` bigint(20) NOT NULL AUTO_INCREMENT, `task_id` varchar(64) DEFAULT NULL COMMENT &#x27;任务ID&#x27;, `task_type` varchar(10) DEFAULT NULL COMMENT &#x27;任务类型 once:一次性任务 cron:定时任务&#x27;, `task_module` varchar(100) DEFAULT NULL COMMENT &#x27;任务模块&#x27;, `callback_url` varchar(500) DEFAULT NULL COMMENT &#x27;回调URL&#x27;, `callback_request` varchar(3000) DEFAULT NULL COMMENT &#x27;回调请求内容&#x27;, `callback_response` varchar(3000) DEFAULT NULL COMMENT &#x27;回调响应内容&#x27;, `callback_status` int(11) DEFAULT &#x27;0&#x27; COMMENT &#x27;回调响应状态 0:成功&#x27;, `callback_spendms` int(11) DEFAULT &#x27;0&#x27; COMMENT &#x27;回调响应时间（毫秒）&#x27;, `gmt_create` datetime NOT NULL DEFAULT CURRENT_TIMESTAMP, `gmt_modified` datetime NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP, PRIMARY KEY (`id`), KEY `idx_query` (`gmt_create`) USING BTREE, KEY `idx_task_module` (`task_module`) USING HASH) ENGINE=InnoDB AUTO_INCREMENT=1 DEFAULT CHARSET=utf8 ROW_FORMAT=DYNAMIC COMMENT=&#x27;任务执行日志表&#x27;;-- 修改表alter table `lights`.`scheduler_log` add column `test` longtext COMMENT &#x27;test&#x27; after callback_spendms;-- 创建索引create UNIQUE index uk_orderno on lights.order_main (order_no);create index idx_query on lights.order_main (gmt_create, status) USING BTREE;-- 创建存储过程drop procedure if exists `lights`.`proc_report`;create procedure `lights`.`proc_report`(IN report_day date) begin declare end_day date; declare usd_rmbratio decimal(18, 6) default 1; set usd_rmbratio = 6.890000; set end_day = date_add(date_add(report_day, interval 1 day), interval -1 microsecond); -- 可以执行delete、update、insert操作 select * from table; end;-- 调用存储过程call `lights`.proc_report(&#x27;2019-01-02&#x27;);select * from lights.proc_report;-- 创建事件drop event if exists `lights`.`proc_report`;CREATE EVENT `lights`.`event_call_proc_report` on schedule EVERY 1 DAY STARTS DATE_ADD(DATE_ADD(CURDATE(), INTERVAL 1 DAY), INTERVAL 1 HOUR) ON COMPLETION PRESERVEDO BEGIN call `lights`.`proc_report`(DATE_ADD(CURDATE(), INTERVAL -1 DAY)); END; -- 创建函数delimiter $$CREATE FUNCTION fesf_accounting.accounting_status(cc_status int, iata_status int) RETURNS intBEGIN declare status int default null; if iata_status = 1 || cc_status = 1 then set status = 5; elseif iata_status = 6 || cc_status = 6 then set status = 6; else set status = null; end if; return status;END $$","categories":[{"name":"技术","slug":"技术","permalink":"http://www.lights8080.com/categories/%E6%8A%80%E6%9C%AF/"}],"tags":[{"name":"Mysql","slug":"Mysql","permalink":"http://www.lights8080.com/tags/Mysql/"}]},{"title":"Linux Shell","slug":"技术/Linux/Linux Shell","date":"2021-06-17T16:00:00.000Z","updated":"2021-08-03T01:47:54.000Z","comments":true,"path":"p/706c2122.html","link":"","permalink":"http://www.lights8080.com/p/706c2122.html","excerpt":"shell介绍和shell环境","text":"shell介绍和shell环境 Shellshell是用户使用Linux系统的一座桥梁，是一种命令语言。Shell 脚本（shell script）是一种为 shell 编写的脚本程序。业界所说的 shell 通常都是指 shell 脚本，但是shell 和 shell script是两个不同的概念。 Linux Shell分类： Bourne Shell（/usr/bin/sh或/bin/sh） Bourne Again Shell（/bin/bash） C Shell（/usr/bin/csh） K Shell（/usr/bin/ksh） … Bash 也是大多数Linux系统默认的Shell。 命令： 1234# 查看shell外壳echo $SHELL# 切换bashchsh -s /bin/bash shell环境信息bashrc与profile都用于保存用户的环境信息，bashrc用于交互式non-loginshell，而profile用于交互式login shell。 1231. 当bash以login shell启动时，它会执行/etc/profile中的命令，然后/etc/profile调用/etc/profile.d目录下的所有脚本；然后执行~/.bash_profile，~/.bash_profile调用~/.bashrc，最后~/.bashrc又调用/etc/bashrc.2. 非login方式启动时，它会调用~/.bashrc，随后~/.bashrc中调用/etc/bashrc，最后/etc/bashrc调用所有/etc/profile.d目录下的脚本 /etc/profile：为系统的每个用户设置环境信息，当用户第一次登录时，该文件被执行，并从/etc/profile.d目录的配置文件中搜集shell的设置 /etc/bashrc：为每一个运行bash shell的用户执行此文件，当bash shell被打开时，该文件被读取。进行个性化设置 ~/.bash_profile：每个用户都可使用该文件输入专用于自己使用的shell信息，当用户登录时该文件执行一次。默认情况下，他设置一些环境变量，执行用户的.bashrc文件 ~/.bashrc：该文件包含专用于某个用户的bash shell的bash信息，当登录时以及每次打开新的shell时，该文件被读取 ~/.bash_logout：当每次退出bash shell时，执行该文件 ~/.profile可以设定本用户专有的路径，环境变量，等，它只能登入的时候执行一次 ~/.bashrc也是某用户专有设定文档，可以设定路径，命令别名，每次shell script的执行都会使用它一次 命令： 12# 立马生效source ~/.bash_profile","categories":[{"name":"技术","slug":"技术","permalink":"http://www.lights8080.com/categories/%E6%8A%80%E6%9C%AF/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://www.lights8080.com/tags/Linux/"},{"name":"shell","slug":"shell","permalink":"http://www.lights8080.com/tags/shell/"}]},{"title":"pyenv for Mac","slug":"技术/其他/pyenv for Mac","date":"2021-06-17T16:00:00.000Z","updated":"2021-08-03T01:47:54.000Z","comments":true,"path":"p/7afafb85.html","link":"","permalink":"http://www.lights8080.com/p/7afafb85.html","excerpt":"pyenv for mac安装说明","text":"pyenv for mac安装说明 安装123brew install pyenv# pyenv管理的Python命令目录添加到$PATHecho &#x27;eval &quot;$(pyenv init --path)&quot;&#x27; &gt;&gt; ~/.bash_profile 命令123456789101112131415161718# 安装指定python版本pyenv install 3.6.9# 查看安装的python版本pyenv versions# 设置全局python版本pyenv global 3.5.2# 为本地环境设置python版本，覆盖globalpyenv local 3.6.9# 为当前shell会话设置python版本，覆盖localpyenv shell 3.6.9# 取消设置pyenv local --unset 参考https://github.com/pyenv/pyenv#homebrew-on-macoshttps://github.com/pyenv/pyenv/blob/master/COMMANDS.md#command-reference","categories":[{"name":"技术","slug":"技术","permalink":"http://www.lights8080.com/categories/%E6%8A%80%E6%9C%AF/"}],"tags":[{"name":"for Mac","slug":"for-Mac","permalink":"http://www.lights8080.com/tags/for-Mac/"},{"name":"Python","slug":"Python","permalink":"http://www.lights8080.com/tags/Python/"}]},{"title":"ElastAlert-实践","slug":"技术/ELK/ElastAlert-实践","date":"2021-06-16T16:00:00.000Z","updated":"2021-08-03T01:47:54.000Z","comments":true,"path":"p/147ee709.html","link":"","permalink":"http://www.lights8080.com/p/147ee709.html","excerpt":"","text":"基于Elastalert的二次开发https://github.com/lights8080/elastalert forked from Yelp/elastalert 修改内容大致如下： 修改报警及日志的日期格式为%Y-%m-%d %H:%M:%S %Z 集成钉钉报警（支持At、secret认证），参考example_rules/example_frequency_lights8080.yaml PercentageMatchRule，报警内容增加match_bucket_count字段 FrequencyRule，报警内容增加doc_count字段 requirements.txt改为elasticsearch==7.0.0 优化日志 规则配置建议 buffer_time与run_every参数设置相同 支持钉钉报警 新增文件：elastalert_modules/dingtalk_alert.py 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485#! /usr/bin/env python# -*- coding: utf-8 -*-&quot;&quot;&quot;@author: xuyaoqiang,lights8080@contact: xuyaoqiang@gmail.com@date: 2017-09-14 17:35,2021-06-23@version: 0.0.0@license:@copyright:&quot;&quot;&quot;import jsonimport requestsfrom elastalert.alerts import Alerter, DateTimeEncoderfrom requests.exceptions import RequestExceptionfrom elastalert.util import EAExceptionimport sysimport ioimport timeimport datetimeimport hmacimport hashlibimport base64import urllibsys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding=&#x27;utf-8&#x27;)class DingTalkAlerter(Alerter): required_options = frozenset([&#x27;dingtalk_webhook&#x27;, &#x27;dingtalk_msgtype&#x27;]) def __init__(self, rule): super(DingTalkAlerter, self).__init__(rule) self.dingtalk_webhook_url = self.rule[&#x27;dingtalk_webhook&#x27;] self.dingtalk_msgtype = self.rule.get(&#x27;dingtalk_msgtype&#x27;, &#x27;text&#x27;) self.dingtalk_isAtAll = self.rule.get(&#x27;dingtalk_isAtAll&#x27;, False) self.dingtalk_title = self.rule.get(&#x27;dingtalk_title&#x27;, &#x27;&#x27;) self.dingtalk_atMobiles = self.rule.get(&#x27;dingtalk_atMobiles&#x27;, []) self.dingtalk_secret = self.rule.get(&#x27;dingtalk_secret&#x27;, &#x27;&#x27;) def format_body(self, body): return body.encode(&#x27;utf8&#x27;) def alert(self, matches): headers = &#123; &quot;Content-Type&quot;: &quot;application/json&quot;, &quot;Accept&quot;: &quot;application/json;charset=utf-8&quot; &#125; body = self.create_alert_body(matches) payload = &#123; &quot;msgtype&quot;: self.dingtalk_msgtype, &quot;text&quot;: &#123; &quot;content&quot;: body &#125;, &quot;at&quot;: &#123; &quot;isAtAll&quot;:False &#125; &#125; if len(self.dingtalk_atMobiles) &gt; 0: payload[&quot;at&quot;][&quot;atMobiles&quot;] = self.dingtalk_atMobiles url = self.dingtalk_webhook_url if len(self.dingtalk_secret) &gt; 0: timestamp = round(time.time() * 1000) secret_enc = bytes(self.dingtalk_secret, encoding=&#x27;utf8&#x27;) string_to_sign = &#x27;&#123;&#125;\\n&#123;&#125;&#x27;.format(timestamp, self.dingtalk_secret) string_to_sign_enc = bytes(string_to_sign, encoding=&#x27;utf8&#x27;) hmac_code = hmac.new(secret_enc, string_to_sign_enc, digestmod=hashlib.sha256).digest() sign = urllib.parse.quote(base64.b64encode(hmac_code)) url = &#x27;&#123;&#125;&amp;timestamp=&#123;&#125;&amp;sign=&#123;&#125;&#x27;.format(self.dingtalk_webhook_url, timestamp, sign) try: response = requests.post(url, data=json.dumps(payload, cls=DateTimeEncoder), headers=headers) response.raise_for_status() print(response) except RequestException as e: raise EAException(&quot;Error request to Dingtalk: &#123;0&#125;&quot;.format(str(e))) def get_info(self): return &#123; &quot;type&quot;: &quot;dingtalk&quot;, &quot;dingtalk_webhook&quot;: self.dingtalk_webhook_url &#125; pass 修改规则 12345678alert:- &quot;elastalert_modules.dingtalk_alert.DingTalkAlerter&quot;dingtalk_webhook: &quot;https://oapi.dingtalk.com/robot/send?access_token=token&quot;dingtalk_msgtype: &quot;text&quot;dingtalk_secret: &quot;secret&quot;dingtalk_atMobiles:- &quot;18610241024&quot; 报警实践转换为本地时区规则配置增强模块： 12match_enhancements:- &quot;elastalert.enhancements.TimeEnhancement&quot; 修改前： 1234Example ruleAt least 50 events occurred between 2021-06-18 19:55:24 CST and 2021-06-18 20:00:24 CST@timestamp: 2021-06-18T12:00:24.768631Z 修改后： 12345Example ruleAt least 50 events occurred between 2021-06-18 19:55:24 CST and 2021-06-18 20:00:24 CST@timestamp: 2021-06-18 20:00:24 CST","categories":[{"name":"技术","slug":"技术","permalink":"http://www.lights8080.com/categories/%E6%8A%80%E6%9C%AF/"}],"tags":[{"name":"ELK","slug":"ELK","permalink":"http://www.lights8080.com/tags/ELK/"},{"name":"ElastAlert","slug":"ElastAlert","permalink":"http://www.lights8080.com/tags/ElastAlert/"}]},{"title":"ElastAlert-核心逻辑流程及源码解析","slug":"技术/ELK/ElastAlert-核心逻辑流程及源码解析","date":"2021-06-09T16:00:00.000Z","updated":"2021-08-03T01:47:54.000Z","comments":true,"path":"p/8fb14f6e.html","link":"","permalink":"http://www.lights8080.com/p/8fb14f6e.html","excerpt":"ElastAlert核心逻辑流程及源码解析 run_every、buffer_time、timeframe之间的关系 num_hits、num_matches说明","text":"ElastAlert核心逻辑流程及源码解析 run_every、buffer_time、timeframe之间的关系 num_hits、num_matches说明 1、核心主流程及源码解析核心主流程： 初始化ElastAlerter对象，并调用start()：加载规则、并启动job job调用规则处理(handle_rule_execution)：计算结束时间，run_rule，设置job下次执行时间 run_rule：计算查询开始和结束时间，run_query，send_alert，回写索引 run_query：根据规则调用ES查询，向规则中添加计数数据 send_alert：根据计数数据报警，回写索引 流程图如下： ElastAlert核心逻辑流程及源码解析 2、run_every、buffer_time、timeframe之间的关系 run_every：任务执行时间间隔 buffer_time：查询窗口范围。未设置use_count_query和use_terms_query时，有效 timeframe：事件数的时间窗口 3、num_hits、num_matches说明 match[‘num_hits’]：查询文档计数或聚合桶数 match[‘num_matches’]：查询符合过滤规则的计数123456789101112### match[&#x27;num_hits&#x27;]不同的查询统计，计算方式不同# get_hits_count：thread_data.num_hits += res[&#x27;count&#x27;]# get_hits：thread_data.num_hits += len(res[&#x27;hits&#x27;][&#x27;hits&#x27;])# get_hits_terms：thread_data.num_hits += len(res[&#x27;aggregations&#x27;][&#x27;counts&#x27;][&#x27;buckets&#x27;])# get_hits_aggregation：thread_data.num_hits += res[&#x27;hits&#x27;][&#x27;total&#x27;][&#x27;value&#x27;]","categories":[{"name":"技术","slug":"技术","permalink":"http://www.lights8080.com/categories/%E6%8A%80%E6%9C%AF/"}],"tags":[{"name":"ELK","slug":"ELK","permalink":"http://www.lights8080.com/tags/ELK/"},{"name":"ElastAlert","slug":"ElastAlert","permalink":"http://www.lights8080.com/tags/ElastAlert/"}]},{"title":"Hexo搭建个人博客系统","slug":"工具/Hexo搭建个人博客系统","date":"2021-06-07T16:00:00.000Z","updated":"2021-08-03T01:53:27.000Z","comments":true,"path":"p/9af43afc.html","link":"","permalink":"http://www.lights8080.com/p/9af43afc.html","excerpt":"介绍基于Hexo搭建个人博客，包括评论、图床、站内搜索、字数统计、PV统计、百度统计等","text":"介绍基于Hexo搭建个人博客，包括评论、图床、站内搜索、字数统计、PV统计、百度统计等 特色 Hexo（https://hexo.io/zh-cn/） 模板：pure（https://github.com/cofess/hexo-theme-pure.git） 模板2：hexo-theme-matery（https://github.com/blinkfox/hexo-theme-matery） 图床：gitee 评论：valine（https://console.leancloud.cn/apps） 站内搜索：insight 字数统计：postCount PV统计（leancloud） 百度统计（https://tongji.baidu.com/web/10000362099/homepage/index） 命令123456789101112131415# 切换Node版本nvm use v14.17.0# 生成静态文件（-d：部署；-w：监视文件变动）hexo generate/hexo g# 启动服务器hexo server/hexo s# 部署网站hexo deploy/hexo d# 更换主题后，清除缓存文件和已生成的静态文件hexo cleanhexo clean &amp;&amp; hexo deployhexo s -w 安装步骤1234567891011121314151617181920212223# 切换Node版本nvm use v14.17.0# Hexo初始化项目hexo init blogcd blog# 安装依赖模块npm install hexo-wordcount --savenpm install hexo-generator-json-content --savenpm install hexo-generator-feed --savenpm install hexo-generator-sitemap --savenpm install hexo-generator-baidu-sitemap --savenpm install hexo-deployer-git --savenpm install highlight.js --save# 下载pure模板cd themesgit clone git@github.com:cofess/hexo-theme-pure.git# 启动npm installhexo server 环境配置$ vim _config.yml 123456# Hexo发布deploy:- type: git repo: https://github.com/lights8080/lights8080.github.io branch: master token: $ vim themes/pure/_config.yml 12345678# 评论comment: valine: appid: appkey: # 百度统计plugins: baidu_analytics: 文章链接唯一化npm install hexo-abbrlink –save 1234permalink: post/:abbrlink.htmlabbrlink: alg: crc32 # 算法：crc16(default) and crc32 rep: hex # 进制：dec(default) and hex 报错问题处理执行hexo server命令，报错 123456INFO Validating configINFO Start processingFATAL &#123; err: TypeError: line.matchAll is not a function at res.value.res.value.split.map.line (/home/seek/Data/hexosite/node_modules/hexo-util/lib/highlight.js:128:26) at Array.map (&lt;anonymous&gt;) 升级node到12以上https://stackoverflow.com/questions/67516168/i-just-installed-hexo-static-site-generator-on-debian-and-ran-hexo-server-to-see","categories":[{"name":"工具","slug":"工具","permalink":"http://www.lights8080.com/categories/%E5%B7%A5%E5%85%B7/"}],"tags":[{"name":"Hexo","slug":"Hexo","permalink":"http://www.lights8080.com/tags/Hexo/"},{"name":"开源产品","slug":"开源产品","permalink":"http://www.lights8080.com/tags/%E5%BC%80%E6%BA%90%E4%BA%A7%E5%93%81/"}]},{"title":"NodeJs VS Nginx","slug":"技术/其他/NodeJs VS Nginx","date":"2021-06-07T16:00:00.000Z","updated":"2021-08-03T01:47:54.000Z","comments":true,"path":"p/da8d67f4.html","link":"","permalink":"http://www.lights8080.com/p/da8d67f4.html","excerpt":"前端项目动态应用和静态应用的区别，有了 Vue + Nginx，为什么还要 Node？","text":"前端项目动态应用和静态应用的区别，有了 Vue + Nginx，为什么还要 Node？ Vue：只是一个 UI 层的框架，因此他打包出来的就是一套 UI 的静态文件：html + js-bundle Nginx：反向服务器，并不提供逻辑处理，只做譬如负载均衡、流量控制、静态服务器等功能 Node：运行在服务端的 JavaScript vue 对 node 的服务端渲染支持最好（反过来说就不准确了） NodeJS的核心优势 服务端渲染-SSR（Server Side Rendering） 鉴权 接口聚合 Node要避免一些高CPU开销的功能","categories":[{"name":"技术","slug":"技术","permalink":"http://www.lights8080.com/categories/%E6%8A%80%E6%9C%AF/"}],"tags":[{"name":"前端","slug":"前端","permalink":"http://www.lights8080.com/tags/%E5%89%8D%E7%AB%AF/"},{"name":"NodeJs","slug":"NodeJs","permalink":"http://www.lights8080.com/tags/NodeJs/"},{"name":"Nginx","slug":"Nginx","permalink":"http://www.lights8080.com/tags/Nginx/"}]},{"title":"ELK-合集","slug":"技术/ELK/ELK-合集","date":"2021-06-06T16:00:00.000Z","updated":"2021-08-03T01:47:54.000Z","comments":true,"path":"p/f61d7f78.html","link":"","permalink":"http://www.lights8080.com/p/f61d7f78.html","excerpt":"ELK文章合集，《Elastic Stack 实战手册》","text":"ELK文章合集，《Elastic Stack 实战手册》 通过最近对ELK全家桶的学习和应用实践，总结成系列文章。如有疑问，欢迎私信，共同学习，一起成长。 安利一下《Elastic Stack 实战手册》早鸟版首发，这是我最近才关注到的，写的很好https://developer.aliyun.com/topic/elasticstack/playbook 文章合集ELK-实践（架构选择&amp;部署说明） ELK-实践（业务框架&amp;业务配置） ELK-加密通信的说明和配置教程 Elasticsearch-介绍 Elasticsearch-索引（Index） Elasticsearch-映射（Mapping） Elasticsearch-搜索（Search DSL） Elasticsearch-聚合（Aggregations） Elasticsearch-安全特性（Security） Logstash介绍 Logstash配置及说明 Beats-Filebeat命令&amp;配置说明 Beats-Filebeat介绍 Kibana-介绍 ElastAlert-介绍 ElastAlert-配置 ElastAlert-核心逻辑流程及源码解析 ElastAlert-实践","categories":[{"name":"技术","slug":"技术","permalink":"http://www.lights8080.com/categories/%E6%8A%80%E6%9C%AF/"}],"tags":[{"name":"ELK","slug":"ELK","permalink":"http://www.lights8080.com/tags/ELK/"}]},{"title":"ELK-实践（业务框架&业务配置）","slug":"技术/ELK/ELK-实践（业务框架&业务配置）","date":"2021-06-03T16:00:00.000Z","updated":"2021-08-03T01:47:54.000Z","comments":true,"path":"p/bd7a66cd.html","link":"","permalink":"http://www.lights8080.com/p/bd7a66cd.html","excerpt":"介绍基于业务的数据模型框架和业务配置（Elasticsearch的映射和索引、Logstash配置管理、Filebeat规则管理、同步脚本等）。基于7.11版本。","text":"介绍基于业务的数据模型框架和业务配置（Elasticsearch的映射和索引、Logstash配置管理、Filebeat规则管理、同步脚本等）。基于7.11版本。 业务中Elasticsearch数据分为两类： 日志类：数据线性增长，无修改操作，无主键，数据价值随时间递减。如：用户操作、异常信息等 业务类：有主键，允许修改删除，长期保留。如：订单信息、基础信息 数据提取方式和特点： 日志类数据：Filebeat-&gt;Logstash-&gt;Elasticsearch。数据流、通用模型、容错性强 业务类数据：通过脚本获取数据信息（数据库、API等）直接更新到Elasticsearch。自定义灵活可控 1. Elasticsearch 映射和索引 lights-mappings：组件模板mappings lights-settings：组件模板settings lights-log：索引生命周期-日志类 lights-data：索引生命周期-数据类 lights-service-exception：服务异常信息（数据流） lights-nginx-web：Nginx日志（数据流） lights-order：订单信息（索引别名） 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237### 创建索引模板组件-mappingPUT /_component_template/lights-mappings&#123; &quot;template&quot;: &#123; &quot;mappings&quot;: &#123; &quot;dynamic_date_formats&quot;: [ &quot;yyyy-MM-dd HH:mm:ss&quot;, &quot;yyyy-MM-dd HH:mm:ss Z&quot;, &quot;yyyy-MM-dd HH:mm:ss.SSS&quot;, &quot;yyyy-MM-dd HH:mm:ss.SSS Z&quot; ], &quot;dynamic_templates&quot;: [ &#123; &quot;string_as_keyword&quot;: &#123; &quot;match_mapping_type&quot;: &quot;string&quot;, &quot;mapping&quot;: &#123; &quot;type&quot;: &quot;keyword&quot;, &quot;ignore_above&quot;: 1024 &#125; &#125; &#125; ], &quot;properties&quot;: &#123; &quot;@timestamp&quot;: &#123; &quot;type&quot;: &quot;date&quot; &#125;, &quot;message&quot;: &#123; &quot;type&quot;: &quot;text&quot;, &quot;index&quot;: false &#125;, &quot;log&quot;: &#123; &quot;properties&quot;: &#123; &quot;file&quot;: &#123; &quot;properties&quot;: &#123; &quot;path&quot;: &#123; &quot;type&quot;: &quot;text&quot;, &quot;index&quot;: false &#125; &#125; &#125; &#125; &#125; &#125; &#125; &#125;&#125;### 创建索引模板组件-settingPUT /_component_template/lights-settings&#123; &quot;template&quot;: &#123; &quot;settings&quot;: &#123; &quot;index&quot;: &#123; &quot;number_of_shards&quot;: 1, &quot;number_of_replicas&quot;: 0, &quot;refresh_interval&quot;: &quot;15s&quot;, &quot;codec&quot;: &quot;best_compression&quot; &#125;, &quot;query&quot;: &#123; &quot;default_field&quot;: [ &quot;@timestamp&quot;, &quot;message&quot; ] &#125; &#125; &#125;&#125;### 创建索引生命周期-日志类PUT _ilm/policy/lights-log&#123; &quot;policy&quot;: &#123; &quot;phases&quot;: &#123; &quot;hot&quot;: &#123; &quot;actions&quot;: &#123; &quot;rollover&quot;: &#123; &quot;max_size&quot;: &quot;50GB&quot;, &quot;max_age&quot;: &quot;1d&quot; &#125; &#125; &#125;, &quot;delete&quot;: &#123; &quot;min_age&quot;: &quot;365d&quot;, &quot;actions&quot;: &#123; &quot;delete&quot;: &#123;&#125; &#125; &#125; &#125; &#125;&#125;### 创建索引生命周期-数据类PUT /_ilm/policy/lights-data&#123; &quot;policy&quot;: &#123; &quot;phases&quot;: &#123; &quot;hot&quot;: &#123; &quot;actions&quot;: &#123; &quot;rollover&quot;: &#123; &quot;max_size&quot;: &quot;50GB&quot;, &quot;max_age&quot;: &quot;30d&quot; &#125; &#125; &#125; &#125; &#125;&#125;### 创建索引模板（数据流）-服务异常信息POST /_index_template/lights-service-exception&#123; &quot;index_patterns&quot;: [ &quot;lights-service-exception&quot; ], &quot;data_stream&quot;: &#123;&#125;, &quot;template&quot;: &#123; &quot;settings&quot;: &#123; &quot;number_of_shards&quot;: 1, &quot;number_of_replicas&quot;: 0, &quot;refresh_interval&quot;: &quot;15s&quot;, &quot;index.lifecycle.name&quot;: &quot;lights-log-30&quot; &#125; &#125;, &quot;priority&quot;: 100, &quot;composed_of&quot;: [ &quot;lights-mappings&quot;, &quot;lights-settings&quot; ], &quot;_meta&quot;: &#123; &quot;description&quot;: &quot;索引模板-服务异常信息&quot; &#125;&#125;### 创建索引模板（数据流）-Nginx日志POST /_index_template/lights-nginx-web&#123; &quot;index_patterns&quot;: [ &quot;lights-nginx-web&quot; ], &quot;data_stream&quot;: &#123;&#125;, &quot;template&quot;: &#123; &quot;settings&quot;: &#123; &quot;number_of_shards&quot;: 1, &quot;number_of_replicas&quot;: 0, &quot;refresh_interval&quot;: &quot;15s&quot;, &quot;codec&quot;: &quot;best_compression&quot;, &quot;index.lifecycle.name&quot;: &quot;lights-log&quot; &#125;, &quot;mappings&quot;: &#123; &quot;properties&quot;: &#123; &quot;bytes&quot;: &#123; &quot;type&quot;: &quot;long&quot; &#125;, &quot;status&quot;: &#123; &quot;type&quot;: &quot;integer&quot; &#125;, &quot;request_time&quot;: &#123; &quot;type&quot;: &quot;double&quot; &#125;, &quot;upstream_status&quot;: &#123; &quot;type&quot;: &quot;integer&quot; &#125;, &quot;upstream_response_time&quot;: &#123; &quot;type&quot;: &quot;double&quot; &#125; &#125; &#125; &#125;, &quot;priority&quot;: 100, &quot;composed_of&quot;: [ &quot;lights-mappings&quot;, &quot;lights-settings&quot; ]&#125;### 创建索引模板（索引别名）-订单数据POST /_index_template/lights-order&#123; &quot;index_patterns&quot;: [ &quot;lights-order-*&quot; ], &quot;template&quot;: &#123; &quot;settings&quot;: &#123; &quot;number_of_shards&quot;: 1, &quot;number_of_replicas&quot;: 0, &quot;refresh_interval&quot;: &quot;15s&quot; &#125;, &quot;mappings&quot;: &#123; &quot;dynamic_date_formats&quot;: [ &quot;yyyy-MM-dd HH:mm:ss&quot;, &quot;yyyy-MM-dd HH:mm:ss Z&quot;, &quot;yyyy-MM-dd HH:mm:ss.SSS&quot;, &quot;yyyy-MM-dd HH:mm:ss.SSS Z&quot; ], &quot;dynamic_templates&quot;: [ &#123; &quot;string_as_keyword&quot;: &#123; &quot;match_mapping_type&quot;: &quot;string&quot;, &quot;mapping&quot;: &#123; &quot;type&quot;: &quot;keyword&quot;, &quot;ignore_above&quot;: 1024 &#125; &#125; &#125;, &#123; &quot;total_as_double&quot;: &#123; &quot;match_mapping_type&quot;: &quot;long&quot;, &quot;match&quot;: &quot;*Amount&quot;, &quot;mapping&quot;: &#123; &quot;type&quot;: &quot;double&quot; &#125; &#125; &#125; ], &quot;properties&quot;: &#123; &quot;orderNum&quot;: &#123; &quot;type&quot;: &quot;integer&quot; &#125;, &quot;logs&quot;: &#123; &quot;properties&quot;: &#123; &quot;remark&quot;: &#123; &quot;type&quot;: &quot;text&quot; &#125; &#125; &#125; &#125; &#125;, &quot;aliases&quot;: &#123; &quot;lights-order&quot;: &#123;&#125; &#125; &#125;, &quot;priority&quot;: 100, &quot;_meta&quot;: &#123; &quot;description&quot;: &quot;索引模板-订单数据&quot; &#125;&#125; 2. Logstash Config 通过Filebeat标签识别日志类型 根据具体的服务名称打上业务标签，过滤、处理数据 根据不同的业务标签，输出到不同的Elasticsearch索引 config/lights.conf 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122# Sample Logstash configuration for creating a simple# Beats -&gt; Logstash -&gt; Elasticsearch pipeline.input &#123; beats &#123; port =&gt; 5044 host =&gt; &quot;0.0.0.0&quot; &#125;&#125;filter &#123; if &quot;lights-service&quot; in [tags] &#123; if !([service_name]) &#123; mutate &#123; copy =&gt; &#123; &quot;[log][file][path]&quot; =&gt; &quot;log_file_path&quot; &#125; &#125; mutate &#123; split =&gt; [ &quot;log_file_path&quot; , &quot;/&quot; ] add_field =&gt; &#123; &quot;service_name&quot; =&gt; &quot;%&#123;[log_file_path][2]&#125;&quot; &#125; remove_field =&gt; [&quot;log_file_path&quot;] &#125; &#125; if [message] =~ &quot; ERROR &quot; and [message] =~ &quot;Exception&quot; &#123; truncate &#123; fields =&gt; [&quot;message&quot;] length_bytes =&gt; 10000 &#125; grok &#123; match =&gt; &#123; &quot;message&quot; =&gt; [ &quot;%&#123;TIMESTAMP_ISO8601:timestamp&#125; \\[%&#123;DATA:thread&#125;\\] %&#123;DATA:level&#125; %&#123;DATA:class&#125; - %&#123;DATA:error_message&#125;\\n%&#123;GREEDYDATA:throwable&#125;&quot;, &quot;%&#123;TIMESTAMP_ISO8601:timestamp&#125; \\[%&#123;DATA:thread&#125;\\] %&#123;DATA:level&#125; %&#123;DATA:class&#125; - %&#123;DATA:error_message&#125;&quot;, &quot;%&#123;TIMESTAMP_ISO8601:timestamp&#125; \\[%&#123;DATA:thread&#125;\\] %&#123;DATA:level&#125; - %&#123;DATA:error_message&#125;\\n%&#123;GREEDYDATA:throwable&#125;&quot;, &quot;%&#123;TIMESTAMP_ISO8601:timestamp&#125; \\[%&#123;DATA:thread&#125;\\] %&#123;DATA:level&#125; - %&#123;DATA:error_message&#125;&quot; ] &#125; id =&gt; &quot;service-exception&quot; &#125; mutate &#123; add_tag =&gt; [ &quot;service-exception&quot; ] &#125; if [throwable] &#123; mutate &#123; remove_field =&gt; [&quot;throwable&quot;] &#125; &#125; &#125; else &#123; if [service_name] == &quot;lights-search-service&quot; and [message] =~ &quot;LightSearchServiceImpl&quot; and [message] =~ &quot; INFO &quot; &#123; grok &#123; match =&gt; &#123; &quot;message&quot; =&gt; [ &quot;%&#123;TIMESTAMP_ISO8601:timestamp&#125; \\[%&#123;DATA:thread&#125;\\] %&#123;DATA:level&#125; %&#123;DATA:class&#125; - %&#123;GREEDYDATA:response_message&#125;&quot; ] &#125; id =&gt; &quot;lights-search-service&quot; &#125; mutate &#123; add_tag =&gt; [ &quot;lights-search-service&quot; ] remove_field =&gt; [&quot;message&quot;] &#125; &#125; &#125; &#125; if &quot;lights-nginx-web&quot; in [tags] &#123; grok &#123; match =&gt; &#123; &quot;message&quot; =&gt; [ &quot;%&#123;IPORHOST:clientip&#125; %&#123;IPORHOST:server_name&#125; %&#123;HTTPDUSER:ident&#125; %&#123;HTTPDUSER:auth&#125; \\[%&#123;HTTPDATE:timestamp&#125;\\] \\&quot;(?:%&#123;WORD:verb&#125; %&#123;DATA:request&#125;(?: HTTP/%&#123;NUMBER:httpversion&#125;)?|%&#123;DATA:rawrequest&#125;)\\&quot; (?:-|%&#123;NUMBER:status&#125;) (?:-|%&#123;NUMBER:request_time&#125;) (?:-|%&#123;NUMBER:bytes&#125;) \\&quot;%&#123;DATA:http_referer&#125;\\&quot; \\&quot;%&#123;DATA:http_user_agent&#125;\\&quot; \\&quot;%&#123;DATA:http_x_forwarded_for&#125;\\&quot; \\&quot;%&#123;DATA:upstream_addr&#125;\\&quot; %&#123;NUMBER:upstream_status&#125; %&#123;NUMBER:upstream_response_time&#125;&quot; ] &#125; &#125; mutate &#123; remove_field =&gt; [&quot;message&quot;] &#125; &#125; date &#123; match =&gt; [ &quot;timestamp&quot;, &quot;yyyy-MM-dd HH:mm:ss.SSS&quot;, &quot;dd/MMM/yyyy:HH:mm:ss Z&quot; ] &#125; mutate &#123; remove_field =&gt; [&quot;@version&quot;, &quot;timestamp&quot;, &quot;agent&quot;, &quot;input&quot;] &#125;&#125;output &#123; #stdout &#123; codec =&gt; rubydebug &#123; metadata =&gt; true &#125; &#125; if &quot;_grokparsefailure&quot; in [tags] &#123; file &#123; path =&gt; &quot;/opt/elk/logstash-7.11.2/_grokparsefailure-%&#123;+yyyy.MM.dd&#125;&quot; &#125; &#125; else &#123; if &quot;service-exception&quot; in [tags] &#123; elasticsearch &#123; hosts =&gt; [&quot;http://10.88.2.1:9200&quot;] index =&gt; &quot;lights-service-exception&quot; action =&gt; &quot;create&quot; user =&gt; &quot;elastic&quot; password =&gt; &quot;xxxxx&quot; &#125; &#125; if &quot;lights-search-service&quot; in [tags] &#123; elasticsearch &#123; hosts =&gt; [&quot;http://10.88.2.1:9200&quot;] index =&gt; &quot;lights-search&quot; #document_id =&gt; &quot;%&#123;[@metadata][_id]&#125;&quot; action =&gt; &quot;create&quot; user =&gt; &quot;elastic&quot; password =&gt; &quot;xxxxx&quot; &#125; &#125; if &quot;lights-nginx-web&quot; in [tags] &#123; elasticsearch &#123; hosts =&gt; [&quot;http://10.88.2.1:9200&quot;] index =&gt; &quot;lights-nginx-web&quot; action =&gt; &quot;create&quot; user =&gt; &quot;elastic&quot; password =&gt; &quot;xxxxx&quot; &#125; &#125; &#125;&#125; 3.1 Filebeat123456789101112131415161718192021# Nginx日志- type: log enabled: true paths: - /log/nginx/lights.log tags: [&quot;lights-nginx&quot;] tail_files: true# Java日志(异常多行合并)- type: log enabled: true paths: - /log/&lt;service-name&gt;/*.log exclude_lines: [&#x27;/actuator/&#x27;] exclude_files: [&#x27;.gz$&#x27;] tags: [&quot;lights-service&quot;] multiline.type: pattern multiline.pattern: &#x27;^\\d&#123;4&#125;-\\d&#123;2&#125;-\\d&#123;2&#125;&#x27; multiline.negate: true multiline.match: after ignore_older: 6h 3.2 脚本（同步到Elasticsearch）1234567891011121314151617181920212223242526272829303132333435363738#!/bin/bash# 判断脚本正在执行LOCK=$(cat $es_script_path/.lock_$file_suffix)if [ $LOCK -eq 1 ];then echo &quot;`date &quot;+%Y-%m-%d %H:%M:%S&quot;` locked&quot; &amp;&amp; exit 1fiecho &quot;1&quot; &gt; $es_script_path/.lock_$file_suffixDATE_BEGIN=$(cat $es_script_path/.lasttime_$file_suffix)DATE_END=`date +&quot;%Y-%m-%d %H:%M:%S&quot;`DATE_BEGIN=&quot;2021-05-01 09:00:10&quot;DATE_END=&quot;2021-05-01 09:00:10&quot;ORDERS_SQL=&quot;&quot;&quot;select order_no from lights.order where createtime BETWEEN &#x27;$DATE_BEGIN&#x27; and &#x27;$DATE_END&#x27;&quot;&quot;&quot;# 查询同步订单列表ORDERS_RESULT=$(mysql -u &lt;user&gt; &lt;db&gt; -e &quot;$ORDERS_SQL&quot;);for LINE in $ORDERS_RESULTdo if [ &#x27;order_no&#x27; != &quot;$LINE&quot; ];then es_index=`echo &quot;lights-order-20$LINE&quot;|cut -c 1-20` echo &quot;sync order time:$DATE_BEGIN ~ $DATE_END order no: $LINE, es index: $es_index&quot; # 调用查询接口 orderdetail_result=$(curl -XPOST -H &quot;Content-Type: application/json&quot; -d &#x27;&#123;&quot;orderNo&quot;:&quot;&#x27;$LINE&#x27;&quot;&#125;&#x27; http://localhost:8080/flights-order-service/order/detail) echo &quot;$orderdetail_result&quot; &gt; $es_script_path/.data_$file_suffix # 时区设置 cat $es_script_path/.data_$file_suffix |jq &#x27;.orderInfo&#x27; -c &gt;$es_script_path/.data_1_$file_suffix sed -i &#x27;s/[0-9]\\&#123;4\\&#125;-[0-9]\\&#123;2\\&#125;-[0-9]\\&#123;2\\&#125; [0-9]\\&#123;2\\&#125;:[0-9]\\&#123;2\\&#125;:[0-9]\\&#123;2\\&#125;/&amp; +0800/g&#x27; $es_script_path/.data_1_$file_suffix # 保持到Elasticsearch curl -XPOST --user &#x27;elastic:&lt;password&gt;&#x27; -H &quot;Content-Type: application/json&quot; http://localhost:9200/$es_index/_doc/$LINE -d@$es_script_path/.data_1_$file_suffix fidone# 更新同步时间 及 解除正在执行标记echo $DATE_END &gt; $es_script_path/.lasttime_$file_suffixecho &quot;0&quot; &gt; $es_script_path/.lock_$file_suffix 4. 其他1. Logstash Req-Resp合并成一条事件正常情况下Controller层会拦截请求参数和响应结果并输出到日志，如何基于线程ID将前后两条日志记录合并到一个事件中。 12345678910111213141516171819202122232425262728293031if [service_name] == &quot;lights-order-service&quot; and [message] =~ &quot;LIGHTS-RE&quot; and [message] =~ &quot; INFO &quot; &#123; grok &#123; match =&gt; &#123; &quot;message&quot; =&gt; [ &quot;%&#123;TIMESTAMP_ISO8601:timestamp&#125; \\[%&#123;DATA:thread&#125;\\] %&#123;DATA:level&#125; - LIGHTS-%&#123;DATA:type&#125;-\\[%&#123;DATA:class_method&#125;\\] \\[%&#123;DATA:username&#125;\\] \\[%&#123;GREEDYDATA:request&#125;\\]&quot;, &quot;%&#123;TIMESTAMP_ISO8601:timestamp&#125; \\[%&#123;DATA:thread&#125;\\] %&#123;DATA:level&#125; - LIGHTS-%&#123;DATA:type&#125;-\\[%&#123;DATA:class_method&#125;\\] %&#123;GREEDYDATA:response&#125; size:%&#123;DATA:size&#125; spend:%&#123;DATA:spend&#125;ms&quot; ] &#125; id =&gt; &quot;lights-order-service&quot; &#125; if [type] == &quot;REQ&quot; &#123; aggregate &#123; task_id =&gt; &quot;%&#123;thread&#125;&quot; code =&gt; &quot;map[&#x27;request_timestamp&#x27;] = event.get(&#x27;timestamp&#x27;); map[&#x27;username&#x27;] = event.get(&#x27;username&#x27;); map[&#x27;request&#x27;] = event.get(&#x27;request&#x27;)&quot; map_action =&gt; &quot;create&quot; &#125; &#125; if [type] == &quot;RESP&quot; &#123; aggregate &#123; task_id =&gt; &quot;%&#123;thread&#125;&quot; code =&gt; &quot;event.set(&#x27;request_timestamp&#x27;, map[&#x27;request_timestamp&#x27;]); event.set(&#x27;username&#x27;, map[&#x27;username&#x27;]); event.set(&#x27;request&#x27;, map[&#x27;request&#x27;])&quot; map_action =&gt; &quot;update&quot; end_of_task =&gt; true timeout =&gt; 120 &#125; mutate &#123; add_tag =&gt; [ &quot;lights-order-service&quot; ] remove_field =&gt; [&quot;message&quot;] &#125; &#125;&#125; 2. Kibana小技巧和注意事项 Kibana可视化中Lens(条形图、饼图等)，如何使用索引的另一个时间x字段统计？新建一个Kibana索引，选择x字段作为时间字段。 Kibana可视化中Lens(条形图、饼图等)，如何像TSVB那样使用公式计算值？使用Kibana索引的脚本字段，在Lens中使用 Kibana中Discover时间框搜索是大于等于(&gt;=)开始时间，小于(&lt;)结束时间，对时间敏感的搜索需要注意 3. Elasticsearch时区问题Elasticsearch存储和读取时都要带上时区 Logstash存储修改UTC时间为东八区时间 123ruby &#123; code =&gt; &quot;event.set(&#x27;temp&#x27;, event.get(&#x27;@timestamp&#x27;).time.localtime + 8*60*60); event.set(&#x27;@timestamp&#x27;, event.get(&#x27;temp&#x27;))&quot; &#125; 直接存储ES的时间字段要带上时区信息2021-05-15 00:00:00 +0800 1curl -XPOST --user &#x27;elastic:xxx&#x27; -H &quot;Content-Type: application/json&quot; http://127.0.0.1:9200/es_index/_doc/$LINE -d@/json_data Kibana搜索时带上时区 12345678910111213GET lights-order/_count&#123; &quot;query&quot;: &#123; &quot;range&quot;: &#123; &quot;@timestamp&quot;: &#123; &quot;gte&quot;: &quot;2021-04-15 00:00:00&quot;, &quot;lte&quot;: &quot;2021-04-15 23:59:59&quot;, &quot;format&quot;: &quot;yyyy-MM-dd HH:mm:ss&quot;, &quot;time_zone&quot;:&quot;+08:00&quot; &#125; &#125; &#125;&#125; 4. 设计上的原则数据清洗一定发生在写入ES之前，而不是请求数据后处理，那势必会降低请求速度和效率。让ES做他擅长的事，检索+不复杂的聚合，否则数据量+复杂的业务逻辑会有性能问题。","categories":[{"name":"技术","slug":"技术","permalink":"http://www.lights8080.com/categories/%E6%8A%80%E6%9C%AF/"}],"tags":[{"name":"ELK","slug":"ELK","permalink":"http://www.lights8080.com/tags/ELK/"}]},{"title":"Elasticsearch-数据迁移","slug":"技术/ELK/Elasticsearch-数据迁移","date":"2021-06-02T16:00:00.000Z","updated":"2021-08-03T01:47:54.000Z","comments":true,"path":"p/eaaae947.html","link":"","permalink":"http://www.lights8080.com/p/eaaae947.html","excerpt":"","text":"Reindex数据迁移重建索引（_reindex），即：一旦索引被创建，则无法直接修改索引字段的mapping属性，必需要重建索引然后将旧的索引数据迁移到新的索引中才行（迁移过程底层使用了scroll API ）。 示例： 123456789101112131415161718POST _reindex &#123; &quot;conflicts&quot;: &quot;proceed&quot;, # 发生冲突继续执行 &quot;source&quot;: &#123; &quot;index&quot;: &quot;old_index&quot;, &quot;type&quot;: &quot;_doc&quot;, &quot;size&quot;: 5000, # 设置每批迁移的文档记录数 &quot;_source&quot;: [&quot;user&quot;, &quot;_doc&quot;], # 可设置要迁移的索引字段，不设置则默认所有字段 &quot;query&quot;: &#123; # 可设置要迁移的文档记录过滤条件 &quot;match_all&quot;: &#123; &#125; &#125; &#125;, &quot;dest&quot;: &#123; &quot;index&quot;: &quot;new_index&quot;, &quot;type&quot;: &quot;_doc&quot;, &quot;version_type&quot;: &quot;internal&quot; # &quot;internal&quot;或者不设置，则Elasticsearch强制性的将文档转储到目标中，覆盖具有相同类型和ID的任何内容 &#125; &#125;","categories":[{"name":"技术","slug":"技术","permalink":"http://www.lights8080.com/categories/%E6%8A%80%E6%9C%AF/"}],"tags":[{"name":"ELK","slug":"ELK","permalink":"http://www.lights8080.com/tags/ELK/"},{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"http://www.lights8080.com/tags/Elasticsearch/"}]},{"title":"Elasticsearch-配置及系统配置说明","slug":"技术/ELK/Elasticsearch-配置及系统配置说明","date":"2021-06-02T16:00:00.000Z","updated":"2021-08-03T01:47:54.000Z","comments":true,"path":"p/1130cce3.html","link":"","permalink":"http://www.lights8080.com/p/1130cce3.html","excerpt":"系统配置、Elasticsearch配置说明","text":"系统配置、Elasticsearch配置说明 系统配置理想情况下，Elasticsearch应该单独运行在服务器上，并使用所有可用的资源。为了做到这一点，您需要配置您的操作系统，以允许运行Elasticsearch的用户访问超过默认允许的资源。 Important System Configuration 系统配置-参考$ vim /etc/security/limits.conf 12345678root soft nofile 65535root hard nofile 65535* soft nofile 65535* hard nofile 65535elasticsearch soft memlock unlimitedelasticsearch hard memlock unlimitedelasticsearch soft nproc 4096elasticsearch hard nproc 4096 $ vim /etc/sysctl.conf 12vm.max_map_count = 262144net.ipv4.tcp_retries2 = 5 系统配置-说明Disable swapping（禁用内存交互）大多数操作系统尝试为文件系统缓存使用尽可能多的内存，并急切地交换未使用的应用程序内存。这可能导致部分JVM堆甚至其可执行页被交换到磁盘。交换对于性能和节点稳定性非常不利，应该不惜一切代价避免。 On Linux systems，临时禁用 1sudo swapoff -a On Linux systems，永久禁用编辑/etc/fstab，注释掉包含swap的任意行 Elasticsearch配置set bootstrap.memory_lock to true in elasticsearch.yml $ vim /etc/security/limits.conf 123# allow user &#x27;elasticsearch&#x27; mlockallelasticsearch soft memlock unlimitedelasticsearch hard memlock unlimited 检查：GET _nodes?filter_path=**.mlockall如果为false，最可能的原因是，运行Elasticsearch的用户没有锁定内存的权限，通过以下方式授权 File Descriptors（文件描述符）Elasticsearch使用了很多文件描述符或文件句柄。耗尽文件描述符可能是灾难性的，很可能会导致数据丢失。确保将运行Elasticsearch的用户打开文件描述符数量的限制增加到65536或更高。 $ vim /etc/security/limits.conf 12345# elasticsearch - nofile 65535root soft nofile 65535root hard nofile 65535* soft nofile 65535* hard nofile 65535 检查：GET _nodes/stats/process?filter_path=**.max_file_descriptors Virtual memory（虚拟内存）Elasticsearch默认使用一个mappfs目录来存储索引。默认操作系统对mmap计数的限制可能太低，这可能会导致内存不足异常。 暂时设置sysctl -w vm.max_map_count=262144 永久设置 $ vim /etc/sysctl.conf 123# 设置操作系统mmap数限制，Elasticsearch与Lucene使用mmap来映射部分索引到Elasticsearch的地址空间# 为了让mmap生效，Elasticsearch还需要有创建需要内存映射区的能力。最大map数检查是确保内核允许创建至少262144个内存映射区vm.max_map_count = 262144 Number of threads（线程数）Elasticsearch为不同类型的操作使用了许多线程池。它能够在需要时创建新线程，这一点很重要。确保Elasticsearch用户可以创建的线程数量至少是4096个。 $ vim /etc/security/limits.conf 12elasticsearch soft nproc 4096elasticsearch hard nproc 4096 TCP retransmission timeout（TCP重传超时）集群中的每一对节点通过许多TCP连接进行通信，这些TCP连接一直保持打开状态，直到其中一个节点关闭或由于底层基础设施中的故障而中断节点之间的通信。 TCP通过对通信应用程序隐藏临时的网络中断，在偶尔不可靠的网络上提供可靠的通信。在通知发送者任何问题之前，您的操作系统将多次重新传输任何丢失的消息。大多数Linux发行版默认重传任何丢失的数据包15次。重传速度呈指数级下降，所以这15次重传需要900秒才能完成。这意味着Linux使用这种方法需要花费许多分钟来检测网络分区或故障节点。Windows默认只有5次重传，相当于6秒左右的超时。 Linux默认允许在可能经历很长时间包丢失的网络上进行通信，但是对于单个数据中心内的生产网络来说，这个默认值太大了，就像大多数Elasticsearch集群一样。高可用集群必须能够快速检测节点故障，以便它们能够通过重新分配丢失的碎片、重新路由搜索以及可能选择一个新的主节点来迅速作出反应。因此，Linux用户应该减少TCP重传的最大数量。 $ vim /etc/sysctl.conf 1net.ipv4.tcp_retries2 = 5 Elasticsearch配置默认情况Elasticsearch假设处于开发模式中，任何的配置不正确都会在日志文件中写入警告，能够正常启动和运行节点；一旦配置了像network.host这样的网络设置，Elasticsearch就会假设处于生产环境中，并将上面的警告升级为异常，这些异常将阻止节点启动。 Important Elasticsearch configuration config/elasticsearch.yml123456789101112131415161718192021222324252627282930313233343536373839404142434445464748# ES数据目录和日志目录，path.data可以设置多个路径path: logs: /var/log/elasticsearch data: /var/data/elasticsearch# 集群名称，默认elasticsearch。相同的集群名称的节点，才能加入集群cluster.name: elasticsearch# 设置全新群集中符合主机资格的节点的初始集合（首次启动集群时需要）；默认为空表示该节点希望加入已经被引导的集群cluster.initial_master_nodes: [&quot;10.188.80.14&quot;]# 节点名称，默认随机生成node.name: prod-data-2node.master: truenode.data: true# 启用预处理，默认启用node.ingest: true# 集群绑定的主机名或IP地址，用于形成一个可以相互通讯的集群；0.0.0.0表示绑定到所有网络接口network.host: 192.168.1.10# 节点间通讯绑定端口，默认9300-9400transport.port: 9300# 提供可访问的集群列表，没有给出端口的通过`transport.port`确定端口，以前使用`discovery.zen.ping.unicast.hosts`discovery.seed_hosts: [&quot;10.188.80.14&quot;]# 启用单节点发现（节点将选举自己为主节点，并且不会与任何其他节点一起加入集群），推迟了TLS的配置；默认形成多节点集群（发现其他节点，并允许他们加入集群）discovery.type: single-node# 集群的最小master节点数，避免集群脑裂discovery.zen.minimum_master_nodes: 2# HTTP服务绑定到的主机地址http.bind_host: # 发布以供HTTP客户端连接的主机地址http.publish_host: # http.bind_host and the http.publish_hosthttp.host: &quot;10.188.80.14&quot;# HTTP请求绑定端口，默认9200-9300http.port: 9200# 允许跨域http.cors.enabled: truehttp.cors.allow-origin: &quot;*&quot;# 禁用swapping，避免影响集群的性能和稳定性，默认开启。（大多数操作系统尝试使用尽可能多的内存文件系统缓存和热切换出未使用的应用程序内存，避免JVM堆交互到磁盘上）bootstrap.memory_lock: true# 开启通过系统调用过滤器检查，默认为true，如果自己承担禁用系统调用过滤器的风险，设置为falsebootstrap.system_call_filter: true# 自动创建索引，默认为trueaction.auto_create_index: true# 删除索引时必须显示的指定名称，默认为trueaction.destructive_requires_name: true config/ jvm.options Elasticsearch有足够的可用堆是非常重要的。 堆的最小值（Xms）与堆的最大值（Xmx）设置成相同的。 Elasticsearch的可用堆越大，它能在内存中缓存的数据越多。但是需要注意堆越大在垃圾回收时造成的暂停会越长。 设置Xmx不要大于物理内存的50%。用来确保有足够多的物理内存预留给操作系统缓存。 禁止用串行收集器来运行Elasticsearch（-XX:+UseSerialGC），默认JVM配置通过Elasticsearch的配置将使用CMS回收器。 12-Xms8g-Xmx8g","categories":[{"name":"技术","slug":"技术","permalink":"http://www.lights8080.com/categories/%E6%8A%80%E6%9C%AF/"}],"tags":[{"name":"ELK","slug":"ELK","permalink":"http://www.lights8080.com/tags/ELK/"},{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"http://www.lights8080.com/tags/Elasticsearch/"}]},{"title":"人体系统-糖","slug":"科普/人体系统-糖","date":"2021-06-01T16:00:00.000Z","updated":"2021-08-03T01:47:54.000Z","comments":true,"path":"p/6479f113.html","link":"","permalink":"http://www.lights8080.com/p/6479f113.html","excerpt":"糖对人体的重要性。什么是糖、糖的作用、糖的分类、葡萄糖的人体旅程。","text":"糖对人体的重要性。什么是糖、糖的作用、糖的分类、葡萄糖的人体旅程。 什么是糖糖是一大类化学物质，糖类都是由碳氢氧三种元素构成的，最初观察发现其中氢和氧的比例是2:1，好像就是碳原子和几个水分子构成的，所以把糖类叫碳水化合物。后来发现一些糖中的氢和氧的比例不是2:1，所以把糖类叫碳水化合物不严谨。但是这个名字一直流传下来，平时所说的碳水化合物一般指的就是糖。 糖的作用糖是最主要的能源物质，人体进行各项生命活动所消耗的能量主要来自于糖类的氧化分解。分解速度快，提供能量非常迅速，我们的大脑和生理活动都需要糖。 人体获得能量：葡萄糖 + 氧气 =&gt; 水 + 二氧化碳 + 能量 葡萄糖在无氧的情况下葡萄糖 =&gt; 乳酸 + 能量(酸奶)葡萄糖 =&gt; 酒精 + 能量(酒) 糖的分类糖不一定是甜的，甜的也不一定是糖。 单糖单糖是构成各种糖分子的基本单位，不能再水解，可以直接被人体吸收。饮食中主要单糖有葡萄糖、果糖、半乳糖，其中葡萄糖是最重要的单糖。所有含碳水化合物的食物进入血液之前，都要分解转化为葡萄糖，到达肠道后，在转运蛋白的帮助下穿过肠壁，最快地为人体提供能量。 葡萄糖：经过肠道直接被人体吸收，进入到血液，称为血糖 果糖（水果、蜂蜜）：不直接被人体吸收，进入肝脏转换为葡萄糖或脂肪，因为其运转机制迟滞，没有葡萄糖进入血液得快，果糖更容易变成脂肪 半乳糖（牛奶）：不直接被人体吸收，进入肝脏转换为葡萄糖 二糖/双糖两个单糖结合在一起组成了双糖，可以水解成单糖，被人体吸收 麦芽糖（麦芽）：葡萄糖+葡萄糖 蔗糖（甘蔗）：葡萄糖+果糖 乳糖（牛奶）：葡萄糖+半乳糖 多糖多糖相对来讲是复合的碳水化合物，由很多单糖通过糖苷链接在一起形成的聚合物，因此多糖结构非常大，经常有分支和很多分子。多糖不溶于水而且没有甜味。 淀粉（大米、白面）：由葡萄糖连接而成 糖原（肝脏、肌肉）：肝脏可以把多余的单糖合成糖原储存在肝脏和肌肉里，当身体需要能量时糖原会迅速分解成葡萄糖。 脂肪（皮下、内脏）：糖原储存不下，更多的单糖会合成脂肪储存在皮下和内脏 纤维（木头、棉花、蔬菜）：不能消化的碳水化合物被划归为“膳食纤维”或者“不可用碳水化合物”。作用：有饱腹感、促进肠道蠕动、促进粪便形成 几丁质（虾壳、蟹壳）：人体不能消化的东西 代糖/甜味剂通过人工化学改造或者合成的具有甜味的化学物质。特点是甜度非常高，几乎没有热量，也不具有任何营养价值。 阿斯巴甜：甜度是等量蔗糖的200倍 三氯蔗糖：甜度是等量蔗糖的600倍 葡萄糖的人体旅程食物中的碳水化合物通过分解转换为单糖，其中葡萄糖经过肠壁吸收直接进入血液（此时叫血糖），果糖和半乳糖经过吸收进入到肝脏，被转换为葡萄糖、糖原或脂肪。 葡萄糖 ==&gt; 糖原/脂肪：胰岛素葡萄糖 &lt;== 糖原/脂肪：胰高血糖素 参考碳水化合物和糖有什么区别？","categories":[{"name":"科普","slug":"科普","permalink":"http://www.lights8080.com/categories/%E7%A7%91%E6%99%AE/"}],"tags":[{"name":"李永乐老师","slug":"李永乐老师","permalink":"http://www.lights8080.com/tags/%E6%9D%8E%E6%B0%B8%E4%B9%90%E8%80%81%E5%B8%88/"},{"name":"人体系统","slug":"人体系统","permalink":"http://www.lights8080.com/tags/%E4%BA%BA%E4%BD%93%E7%B3%BB%E7%BB%9F/"}]},{"title":"ElastAlert-配置","slug":"技术/ELK/ElastAlert-配置","date":"2021-05-31T16:00:00.000Z","updated":"2021-08-03T01:47:54.000Z","comments":true,"path":"p/ecd7a74d.html","link":"","permalink":"http://www.lights8080.com/p/ecd7a74d.html","excerpt":"全局配置和规则配置说明","text":"全局配置和规则配置说明 全局配置（Configuration）123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051# Elasticsearch集群配置es_host:es_port:use_ssl:verify_certs:es_username:es_password:es_url_prefix:es_conn_timeout:# 设置检索rules和hashes的加载类rules_loader: &#x27;FileRulesLoader&#x27;# 规则配置文件的文件夹的名称，仅rules_loader=FileRulesLoader时有效rules_folder: rules# 是否递归rules目录的子目录配置scan_subdirectories: true# 查询Elasticsearch的时间间隔run_every: minutes: 1# 查询窗口的大小buffer_time: minutes: 15# ElastAlert将存储数据的索引名称writeback_index: elastalert# 单次查询Elasticsearch最大文档数，默认10000max_query_size: 10000# 滚动浏览的最大页面数，默认0（表示不限制）max_scrolling_count: 0# 在滚动浏览上下文中应保持活动状态的最长时间scroll_keepalive: # 汇总在一起的最大警报数max_aggregation:# 两次查询之间的最长时间old_query_limit:# 禁用未捕获异常的规则，默认Truedisable_rules_on_error: # ElastAlert完成执行后会显示“禁用规则”列表show_disabled_rules: true# 通知邮件列表，当前只有未捕获的异常会发送通知电子邮件notify_email:# 警报是否包括规则中描述的元数据，默认Falseadd_metadata_alert: false# 跳过无效的文件而不是退出skip_invalid:# 失败警报的重试窗口alert_time_limit:# 增强模块，与规则一起使用，将其传递给报警器之前对其进行修改或删除match_enhancements:- &quot;elastalert_modules.my_enhancements.MyEnhancement&quot;# 匹配立刻运行增强run_enhancements_first: true 规则配置（Rule Configuration）123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119# Elasticsearch配置es_host: 10.188.10.1es_port: 9200es_username: elastices_password: xxxindex: logstash-*# Rule Typetype: &#x27;any&#x27;# 导入公共配置import:# 用于标识警报的利益相关者owner: &#x27;xxx&#x27;# 用于标识警报的相对优先级priority: 2# 用于标识警报的类别catagory: &#x27;&#x27;# 规则描述description: &#x27;&#x27;# 设置请求里查询窗口的范围。当use_count_query或use_terms_query为true时，将忽略此值buffer_time: minutes: 5# 延迟查询query_delay: minutes: 5# 开启timeframe（查询开始时间=now()-timeframe）scan_entire_timeframe: true# 1. 查询开始时间，scan_entire_timeframe开启，use_count_query和use_terms_query未设置时有效# 2. 规则的事件发生窗口期，如：FrequencyRule-EventWindowtimeframe: minutes: 1# Elasticsearch查询过滤器filter:- query: query_string: query: &quot;level: ERROR&quot;# 触发报警的事件数num_events: 5# 传递给规则类型和警报的查询结果字段列表，默认所有字段include: - &quot;username&quot;# 针对每个字段的前X（top_count_number）个最常用的值执行Terms查询top_count_keys: - &quot;username&quot;# 术语的前X个最常用的值，与top_count_keys一同使用top_count_number: 5# 如果为true，top_count_keys中所有字段都会附加.rawraw_count_keys: true# 单次查询获取的最大文档数max_query_size: 10000# 计数查询（count api），而不下载所匹配的文档use_count_query: false# 聚合查询（aggregation），和query_key、doc_type、terms_size一起使用use_terms_query: false# use_terms_query=true时，为每个值单独计数query_key: &#x27;username&#x27;# top_count_keys存在，发送警报时，多个逗号分隔，必须配合compound_query_key使用query_key: &#x27;service_name,username&#x27;# 复合的查询key，必须与query_key一一对应，get_hits_terms时使用compound_query_key: - service_name - username doc_type: _doc# 桶的最大数terms_size: 50# 相关事件一同报警。一个桶触发报警，其他的桶一同触发报警attach_related: false# 将多次匹配汇总到一个警报中，将聚合时间期内发生的所有匹配项一起发送aggregation: # 需要大量匹配并只需要定期报告 hours: 2 # 汇总所有警报并定期发送报警 schedule: &#x27;2 4 * * mon,fri&#x27;# 为不同的字段值创建一个独立的聚合窗口，默认在聚合窗口期中所有事件被分组在一起aggregation_key: &#x27;my_data.username&#x27;# 基于第一个事件的时间创建聚合，默认当前时间aggregate_by_match_time: true# 对于聚合报警，指定摘要表字段summary_table_fields: - my_data.username# 忽略一段时间的重复警报，支持query_keyrealert: minutes: 10# 使realert的值呈指数增加exponential_realert: hours: 1# 是否将时间戳转换为警报中的本地时区use_local_time: true# 时间戳类型（iso, unix, unix_ms, custom）timestamp_type: &#x27;iso&#x27;# 自定义时间戳格式timestamp_format: &#x27;%Y-%m-%dT%H:%M:%SZ&#x27;# 指定时间字段，默认@timestamptimestamp_field: &#x27;@timestamp&#x27;### Metric Aggregation Type or Percentage Match Type# 使用run_every计算度量计算窗口大小，默认使用buffer_timeuse_run_every_query_size: true# 度量计算窗口大小，必须是buffer_time的倍数bucket_interval: # Alertsalert: - command - debugcommand: [&quot;python3&quot;, &quot;/opt/elastalert/weixin.py&quot;, &quot;生产环境报警，报警:&quot;, &quot;接口&#123;orgPathName&#125; 出现状态码&#123;statusCode&#125;频率高！&quot;,&quot;服务 IP: &#123;directBackServer&#125;; 服务端口：&#123;port&#125;&quot;] 不同的规则类型参数不同，详细请看源码文件elastalert/schema.yaml 参考ElastAlert","categories":[{"name":"技术","slug":"技术","permalink":"http://www.lights8080.com/categories/%E6%8A%80%E6%9C%AF/"}],"tags":[{"name":"ELK","slug":"ELK","permalink":"http://www.lights8080.com/tags/ELK/"},{"name":"ElastAlert","slug":"ElastAlert","permalink":"http://www.lights8080.com/tags/ElastAlert/"}]},{"title":"苦李朋友圈笔记-共勉","slug":"杂谈/苦李朋友圈笔记-共勉","date":"2021-05-20T16:00:00.000Z","updated":"2021-08-03T01:47:54.000Z","comments":true,"path":"p/fc42fa15.html","link":"","permalink":"http://www.lights8080.com/p/fc42fa15.html","excerpt":"","text":"有目标的人，会根据实现目标的需求不断调整自己的方法。没有目标的人，会根据自己的固有套路来调整目标。几年过去，前者在一个一个的实现小目标，后者是一事无成的老顽固。 一个人向积极的方向改变，常常是从运动开始的。而一个人抵御困境的能力，也往往要从运动中产生。 认知水平和知识水平是不一样的。知识是看书能得到的，而认知是你所处的环境、所认识的人以及亲身经历的事情教给你的。 当我们厌恶身边的人时，表达厌恶最好的方式不是和他们争吵，而是自己勤快点，加把劲离开他们。那样，他们就会永远从我们的生活中消失。和死了差不多。 不是所有的鱼都适合生活在同一片海里。 不能一边抱怨自己的起点低，一边不为自己的孩子创造更高的起点，这是双标。 人才的成长有两个阶段，先被人排挤，如果排挤不掉，他们就会来逢迎，你就站住脚了。 一个人倘若经常跳出来看一看人生的全景，真正看清事物的大小和价值的主次，就不太会被那些渺小的事物和次要的价值绊倒了。 除了生病以外，我们所感受到的所有痛苦都是我们的价值观带给我们的，并非真实存在。 努力很重要，有目标的努力更有价值。","categories":[{"name":"杂谈","slug":"杂谈","permalink":"http://www.lights8080.com/categories/%E6%9D%82%E8%B0%88/"}],"tags":[{"name":"语录","slug":"语录","permalink":"http://www.lights8080.com/tags/%E8%AF%AD%E5%BD%95/"}]},{"title":"ElastAlert-介绍","slug":"技术/ELK/ElastAlert-介绍","date":"2021-05-18T16:00:00.000Z","updated":"2021-08-03T01:47:54.000Z","comments":true,"path":"p/9f0e3b5c.html","link":"","permalink":"http://www.lights8080.com/p/9f0e3b5c.html","excerpt":"","text":"一、Alerting With Elasticsearch ElastAlert是一个简单的框架，用于从Elasticsearch中的数据中发出异常，尖峰或其他感兴趣的模式的警报。 它通过将Elasticsearch与两种类型的组件（规则类型和警报）结合使用。定期查询Elasticsearch，并将数据传递到规则类型，该规则类型确定找到任何匹配项。发生匹配时，它会发出一个或多个警报，这些警报根据不同的类型采取相应的措施。 ElastAlert由一组规则配置，每个规则定义一个查询，一个规则类型和一组警报。 特性 架构简单，定制灵活 支持多种匹配规则（频率、阈值、数据变化、黑白名单、变化率等） 支持多种警报类型（邮件、HTTP POST、自定义脚本等） 匹配项汇总报警，重复警报抑制，报警失败重试和过期 可用性强，状态信息保存到Elasticsearch的索引中 过程的调试和审计等 可用性（Reliability） ElastAlert 将其状态保存到 Elasticsearch，启动后，将恢复之前停止的状态 如果 Elasticsearch 没有响应，ElastAlert 将等到恢复后才继续 抛出错误的警报可能会在一段时间内自动重试 模块性（Modularity）ElastAlert具有三个主要组件（规则类型、警报、增强），可以作为模块导入和定制。 规则类型（Rule Types）规则类型负责处理从Elasticsearch返回的数据。它会使用规则配置进行初始化，传递通过规则过滤器查询Elasticsearch返回的数据，并根据此数据输出匹配项。 警报（Alerts）警报负责根据匹配采取行动。匹配项通常是一个字典，其中包含Elasticsearch中文档中的值，但可以包含由规则类型添加的任意数据。 增强（Enhancements）增强功能是一种拦截警报并以某种方式对其进行修改或增强的方法。在将其提供给警报器之前，将它们传递给匹配字典。 二、Running ElastAlert 安装、命令、测试规则、运行 安装123456789101112131415161718192021222324# Python3.6安装$ wget https://www.python.org/ftp/python/3.6.9/Python-3.6.9.tgz$ tar -zxvf Python-3.6.9.tgz$ cd Python-3.6.9$ ./configure$ make &amp;&amp; make install# 检查Python版本$ python3 -V# 安装$ git clone https://github.com/Yelp/elastalert.git$ wget https://github.com/daichi703n/elastalert/archive/refs/heads/fix/initialize_alerts_sent.zip$ cd elastalert/$ pip3 install &quot;setuptools&gt;=11.3&quot;$ python3 ./setup.py install# 报错（pip:No module named setuptools_rust）解决办法$ pip3 install setuptools-rust#$ pip3 install &quot;elasticsearch&gt;=5.0.0&quot;$ pip3 install elasticsearch==7.0.0 命令12345678910# 创建索引$ elastalert-create-index# 测试Rule，24小时内以调试模式运行。--config：指定配置文件$ elastalert-test-rule example_rules/example_frequency.yaml# 或$ python3 -m elastalert.elastalert --config ./config.yaml --rule rules/service_exception.yaml --start 2021-05-16T00:00:00+08:00 --debug --es_debug --es_debug_trace trace-20210617.log# 后台运行nohup python3 -m elastalert.elastalert --config ./config.yaml --rule ./your_rule.yaml --verbose &gt;&gt; ./elastalert.log 2&gt;&amp;1 &amp; 测试规则（Testing Rule）可以在调试模式下运行ElastAlert，也可以使用elastalert-test-rule（该脚本可以简化测试的各个方面） 功能： 检查配置文件是否加载成功 检查Elasticsearch过滤器是否解析 与最后的X天(s)运行，显示匹配您的过滤器的点击数 在一个结果中显示可用的术语 保存返回到JSON文件的文档 使用JSON文件或Elasticsearch的实际结果运行ElastAlert 打印调试警报或触发真实警报 如果存在，则检查结果中是否包含primary_key、compare_key和include术语 显示将要写入elastalert_status的元数据文档 参数： –schema-only：只对文件执行验证 –count-only：仅查找匹配文档的数量并列出可用字段 –days N：指定针对最近的N天运行，默认1天 –save-json FILE：将所有下载的文档保存为JSON文件 –data FILE：使用JSON文件代替Elasticsearch作为数据源 –alert：触发实际警报，而不是调试（日志文本）警报 –formatted-output：以格式化的JSON输出结果。 运行（Running ElastAlert）有两种运行ElastAlert的方法。作为守护程序或直接与Python一起使用（$ python3 elastalert/elastalert.py）。 参数： –config：指定要使用的配置文件，默认值为config.yaml –debug：debug模式；1.增加日志记录的详细程度，2.将所有的报警更改为DebugAlerter，抑制它们正常的报警行为，3.跳过写入查询和警报的元数据到Elasticsearch –verbose：将增加日志记录的详细程度，与–debug不兼容 –start ：强制从指定的时间查询，而不是当前时间。如：2021-05-16T00:00:00+08:00 –end ：将强制在指定时间之后停止查询，默认到当前时间 –rule &lt;rule.yaml&gt;：只运行给定的规则 –silence =：将使给定规则的警报静音一段时间。该规则必须使用–rule指定 –es_debug：启用对Elasticsearch进行的所有查询的日志记录。 –es_debug_trace &lt;trace.log&gt;：将启用将对Elasticsearch进行的所有查询的curl命令记录到指定的日志文件 –pin_rules：禁用动态加载规则 三、Rule Types and Configuration Options 规则类型、配置项、报警配合https://elastalert.readthedocs.io/en/stable/ruletypes.html 详细请看源码文件elastalert/schema.yaml Rule Typesfrequency：频率；在给定时间范围内至少有一定数量的事件时，此规则匹配 num_events：触发警报的事件数 timeframe：事件数必须发生在的此时间段内，触发报警 use_count_query：使用count API轮询Elasticsearch any：任意；过滤器每次匹配都会警报 blacklist：黑名单；将对照黑名单检查某个字段，如果该字段在黑名单中，则进行匹配 compare_key： 用于与黑名单进行比较的字段的名称，如果字段为null，那么这些事件将被忽略 blacklist：黑名单值的列表 whitelist：白名单；与黑名单类似 change：变动；将监视特定字段，并在该字段发生更改时进行匹配 compare_key：要监视更改的字段的名称 ignore_null：如果为true，则没有compare_key字段的事件将不会计为已更改 query_key：对每个query_key的唯一值分别计数。 timeframe: 可选；时间范围，两次更改之间的最长时间。在这段时间之后，ElastAlert将忘记compare_key字段的旧值 spike：突刺；当给定时间段内的事件量是前一时间段内的spike_height倍数时，此规则匹配 spike_height：上一个时间范围内的事件数量与前一个时间范围内的事件数量之比，匹配时触发警报 spike_type：‘up’, ‘down’ or ‘both’ timeframe：当前窗口和参考窗口的时间范围 field_value：可选；使用文档中字段的值而不是匹配文档的数量 threshold_ref：参考窗口中必须存在的最少数量的事件才能触发警报 threshold_cur：当前窗口中必须存在的最小数量的事件才能触发警报 flatline：阈值；当事件总数在一个时间段内低于给定阈值时，此规则匹配 threshold：不触发警报的最小事件数 timeframe：时间范围 new_term：新值；当新值出现在以前从未见过的字段中时，此规则匹配 fields：监视字段列表 cardinality：基数阈值；当时间范围内某个字段的唯一值的总数大于或小于阈值时，此规则匹配 cardinality_field：要计算基数的字段 timeframe：时间范围 metric_aggregation：当计算窗口中的度量值高于或低于阈值时，此规则匹配 metric_agg_key：指标字段 metric_agg_type：指标类型，‘min’, ‘max’, ‘avg’, ‘sum’, ‘cardinality’, ‘value_count’ max_threshold：最大阈值 min_threshold：最小阈值 spike_aggregation：当计算窗口中某个指标的值是spike_height乘以比前一个时间段大或小时，该规则匹配 metric_agg_key：指标字段 metric_agg_type：指标类型，‘min’, ‘max’, ‘avg’, ‘sum’, ‘cardinality’, ‘value_count’ spike_height：上一个时间范围内的事件数量与前一个时间范围内的事件数量之比，匹配时触发警报 spike_type：‘up’, ‘down’ or ‘both’ buffer_time：当前窗口和参考窗口的时间范围 query_key：按字段分组 metric_agg_script：计算指标脚本 threshold_ref：参考窗口中用于触发警报的指标的最小值 threshold_cur：当前窗口中用于触发警报的指标的最小值 min_doc_count：当前窗口中触发警报所需的最小事件数 percentage_match：当计算窗口内匹配桶中文档的百分比高于或低于阈值时，此规则匹配 match_bucket_filter：匹配桶定义了一个过滤器 min_percentage：匹配文档的百分比小于此数字，则会触发警报 max_percentage：匹配文档的百分比大于此数字，则会触发警报 Alerts每个规则都可以附加任何数量的警报。每个警报器的选项既可以定义在yaml文件，也可以嵌套在警报名称中，允许同一警报器的多个不同设置。 Command：命令报警，并从匹配项中传递参数 Email：邮件报警 Http Post：URL报警 … 示例： 12345678910111213141516name: API not 200index: sg-access-*type: frequencynum_events: 20timeframe: minutes: 1filter:- query: query_string: query: &quot;NOT statusCode: 200&quot;alert: - command - debugcommand: [&quot;python3&quot;, &quot;/opt/elastalert/weixin.py&quot;, &quot;生产环境报警，报警:&quot;, &quot;接口&#123;orgPathName&#125; 出现状态码&#123;statusCode&#125;频率高！&quot;,&quot;服务 IP: &#123;directBackServer&#125;; 服务端口：&#123;port&#125;&quot;] 四、ElastAlert Metadata IndexElastAlert使用Elasticsearch来存储有关其状态的各种信息。这不仅可以对ElastAlert的操作进行某种程度的审核和调试，还可以避免在ElastAlert关闭，重新启动或崩溃时丢失数据或重复警报。Elasticsearch群集和索引信息在全局配置文件中使用es_host，es_port和writeback_index进行定义。ElastAlert必须能够写入此索引。脚本elastalert-create-index将为您创建具有正确映射的索引，并可以选择从现有ElastAlert回写索引中复制文档。 ElastAlert将在回写索引（writeback index）中创建三种不同类型的文档。 elastalert_status 记录规则的查询执行日志 elastalert_status是ElastAlert在确定其首次开始时要使用的时间范围，以避免重复查询。对于每个规则，它将从最近的结束时间开始查询。如果ElastAlert在调试模式下运行，它仍将通过查找最近执行的搜索来尝试基于其开始时间，但不会将任何查询的结果写回到Elasticsearch。 elastalert 记录触发的每个警报信息日志 elastalert_error 当ElastAlert中发生错误时，它将同时写入Elasticsearch和stderr silence 记录何时将抑制给定规则的警报的日志 参考ElastAlertelastalert搭建ElastAlert安装与使用Rule Filters说明","categories":[{"name":"技术","slug":"技术","permalink":"http://www.lights8080.com/categories/%E6%8A%80%E6%9C%AF/"}],"tags":[{"name":"ELK","slug":"ELK","permalink":"http://www.lights8080.com/tags/ELK/"},{"name":"ElastAlert","slug":"ElastAlert","permalink":"http://www.lights8080.com/tags/ElastAlert/"}]},{"title":"Linux [buff/cache]内存缓存占用过高分析和优化","slug":"技术/Linux/Linux [buffcache]内存缓存占用过高分析和优化","date":"2021-05-18T16:00:00.000Z","updated":"2021-08-03T01:47:54.000Z","comments":true,"path":"p/f7c987f0.html","link":"","permalink":"http://www.lights8080.com/p/f7c987f0.html","excerpt":"buff/cache过高问题解决过程。问题现场、问题分析、如何解决、扩展知识","text":"buff/cache过高问题解决过程。问题现场、问题分析、如何解决、扩展知识 问题现场查看系统内存的使用状态 监控报警可用内存空间不足，常规的解决方案如下： 增加内存（增加成本） 增加虚拟内存（影响性能） 定期清理缓存（echo 1 &gt; /proc/sys/vm/drop_caches） 本文将介绍定期清除页面缓存，但是过会儿内存又被占满问题的分析。 问题分析 通过监控系统负载情况（vmstat 1），确定是页面缓存（cache项）占用量大，并且释放页面缓存后从块设备读入数据量（bi项）会马上增加。 通过监控io情况（iostat -x -k 1）也可以看出 基于此可以猜测是有进程在频繁的读取文件导致，监视磁盘I/O使用状况（iotop -oP），释放页面缓存后有几个sed命令读取文件进程占用IO很高。 至此结合业务分析是因为每分钟读取日志统计指标导致 扩展知识/proc/meminfo查看更详细的内存信息：$ cat /proc/meminfo |grep -E &quot;Buffer|Cache|Swap|Mem|Shmem|Slab|SReclaimable|SUnreclaim&quot; MemFree：空闲的物理内存 MemAvailable：可用的物理内存，MemFree+Buffers+Cached Buffers：（Buffer Cache）对磁盘块设备数据的缓存 Cached：（Page Cache）对文件系统上文件数据的缓存，MemFree+SReclaimable SwapTotal：虚拟内存，利用磁盘空间虚拟出的一块逻辑内存 Slab：Linux内存管理机制 SReclaimable：Slab可回收部分 SUnreclaim：Slab不可回收部分 Shmem：进程间共同使用的共享内存 /proc/sys/vm/drop_caches清除缓存策略：1：清除page cache2：清除slab分配器中的对象（包括目录项和inode）3：清除page cache和slab分配器中的对象 参考OOM killer及OvercommitLinux buffer/cache 内存占用过高的原因以及解决办法Linux查看Buffer&amp;Cache被哪些进程占用","categories":[{"name":"技术","slug":"技术","permalink":"http://www.lights8080.com/categories/%E6%8A%80%E6%9C%AF/"}],"tags":[{"name":"线上问题","slug":"线上问题","permalink":"http://www.lights8080.com/tags/%E7%BA%BF%E4%B8%8A%E9%97%AE%E9%A2%98/"},{"name":"Linux","slug":"Linux","permalink":"http://www.lights8080.com/tags/Linux/"}]},{"title":"OOM killer及Overcommit","slug":"技术/Linux/OOM killer及Overcommit","date":"2021-05-18T16:00:00.000Z","updated":"2021-08-03T01:47:54.000Z","comments":true,"path":"p/49aea59e.html","link":"","permalink":"http://www.lights8080.com/p/49aea59e.html","excerpt":"OOM killer(Out Of Memory killer)是Linux内核的一种内存管理机制，该机制在系统物理内存不足时，选择性杀死一些进程以释放内存，以使系统继续运行。","text":"OOM killer(Out Of Memory killer)是Linux内核的一种内存管理机制，该机制在系统物理内存不足时，选择性杀死一些进程以释放内存，以使系统继续运行。 OOM killerOOM killer(Out Of Memory killer)是Linux内核的一种内存管理机制，该机制在系统物理内存不足时，选择性（oom_killer遍历当前所有进程，根据进程的内存使用情况进行打分，然后从中选择一个分数最高的进程）杀死一些进程以释放内存，以使系统继续运行。 Overcommit（过量使用）这个特性出于优化系统考虑，因为进程实际使用到的内存往往比申请的内存少。 按照Linux的算法，物理内存页的分配发生在使用瞬间，而不是在申请瞬间。Overcommit针对的也是内存申请，而不是内存分配。 Linux下允许程序申请比系统可用内存更多的内存。因为不是所有的程序申请了内存就立刻使用的，当实际使用时超过可分配物理内存时，利用OOM机制选择性杀死一些进程以释放内存。 参数调优vm.overcommit_memory 0：OVERCOMMIT_GUESS（默认），内核将检查是否有足够的可用内存供应用进程使用 1：OVERCOMMIT_ALWAYS，允许超过CommitLimit的分配，即允许分配所有的物理内存，而不管当前的内存状态如何 2：OVERCOMMIT_NEVER，拒绝超过CommitLimit的分配，即拒绝等于或者大于CommitLimit指定的物理 RAM 比例的内存请求 CommitLimit 和 Commited_AS CommitLimit：最大能分配的内存 计算公式：(Physical RAM * vm.overcommit_ratio / 100) + Swap Committed_AS：当前已经分配的内存 OVERCOMMIT策略的可用内存判定 OVERCOMMIT_GUESS：判定可用内存 = free + buff/cache - share + Swap + SLAB已标记可回收的内存 - 系统运行预留的内存 - 管理员操作预留内存 OVERCOMMIT_ALWAYS：直接返回成功 OVERCOMMIT_NEVER：判断Committed_AS &lt; CommitLimit 相关命令123456# 查看overcommit策略$ cat /proc/sys/vm/overcommit_memory# 查看进程OOM得分，oom_killer将首先杀死得分最高的进程$ cat /proc/&lt;pid&gt;/oom_score# 查看CommitLimit和Committed_AS$ cat /proc/meminfo |grep -i commit 参考 Linux OOM killer机制介绍","categories":[{"name":"技术","slug":"技术","permalink":"http://www.lights8080.com/categories/%E6%8A%80%E6%9C%AF/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://www.lights8080.com/tags/Linux/"}]},{"title":"Spring Cloud Gateway与后端服务问题处理总结","slug":"技术/SpringCloud/Spring Cloud Gateway与后端服务问题处理总结","date":"2021-05-18T16:00:00.000Z","updated":"2021-08-03T01:47:54.000Z","comments":true,"path":"p/3336a7cf.html","link":"","permalink":"http://www.lights8080.com/p/3336a7cf.html","excerpt":"Spring Cloud Gateway相关问题分析、解决思路的过程。 Connection prematurely closed BEFORE response 浪涌导致网关报错分析","text":"Spring Cloud Gateway相关问题分析、解决思路的过程。 Connection prematurely closed BEFORE response 浪涌导致网关报错分析 问题1（Connection prematurely closed BEFORE response）后端服务偶尔报错Connection prematurely closed BEFORE response。 这个问题的产生原因和解决办法网上很容易找到。我这里只贴出问题原理图和解决办法。详细说明请参考https://javazhiyin.blog.csdn.net/article/details/112914264 原理图 解决办法 spring cloud gateway增加jvm启动参数后进先出策略，确保获取的连接最大概率是最近刚被用过的 1-Dreactor.netty.pool.leasingStrategy=lifo 后端服务配置后端服务连接超时时长改为10秒（默认20s），超时没有数据交互则关闭连接。 123server: tomcat: connection-timeout: 10000 spring cloud gateway增加配置设置连接的最大空闲时长为5秒（默认NULL：响应完成即可关闭），超时则关闭连接释放资源。这个时长的设置要小于后端服务的连接超时时长，确保网关回收请求在后端服务回收请求之前完成。 123456spring: cloud: gateway: httpclient: pool: max-idle-time: 5000 问题2（浪涌导致网关报错分析） 每天不定时出现响应失败，Nginx响应状态码出现大量的500和504，网关同样出现大量的500和504，后端服务正常。 Nginx、Gateway、Service每小时统计数如下，其中Nginx，0点的数比较少是因为日志文件截取导致缺失 经过分析得到，2点是正常情况，Nginx-&gt;Gateway-&gt;Service数都对的上。 1点的数据显示Service收到的请求数减少，响应时间也正常，Gateway报错分为504：Gateway响应时间超过导致（配置的60s），500：Gateway连接超过导致（配置的3s），说明Gateway请求并未到达Service端。 查看Nginx和Gateway的连接数出现了激增，因为外部流量瞬间涌入导致服务器连接数资源被占用。 优化方案 开启Gateway限流策略 1234567891011spring: cloud: gateway: default-filters: - name: RequestRateLimiter args: redis-rate-limiter.replenishRate: 200 redis-rate-limiter.burstCapacity: 50 redis-rate-limiter.requestedTokens: 1 key-resolver: &quot;#&#123;@userKeyResolver&#125;&quot; deny-empty-key: false Gateway请求Service超时配置的60s，根据业务需要超过10s响应都视作无效，所以配置响应超时时间为10秒 123456spring: cloud: gateway: httpclient: pool: response-timeout: 10s Gateway的连接池使用弹性方式，导致服务器连接数资源被占满，改为固定方式。 12345678910111213spring: cloud: gateway: httpclient: pool: # 线程池类型，ELASTIC：弹性，FIXED：固定 type: FIXED # 超过此时间连接不使用就关闭 max-idle-time: 5000 # 线程池最大连接数，type=FIXED时有效 max-connections: 200 # 从线程池获取线程的最大等待时间，type=FIXED时有效 acquire-timeout: 45000 由于Gateway到不同的Service，响应时间不一样，可以在Service端的元数据信息中修改连接超时时间和响应超时时间 1234567spring: cloud: nacos: discovery: metadata: response-timeout: 10000 connect-timeout: 3000","categories":[{"name":"技术","slug":"技术","permalink":"http://www.lights8080.com/categories/%E6%8A%80%E6%9C%AF/"}],"tags":[{"name":"线上问题","slug":"线上问题","permalink":"http://www.lights8080.com/tags/%E7%BA%BF%E4%B8%8A%E9%97%AE%E9%A2%98/"},{"name":"SpringCloud","slug":"SpringCloud","permalink":"http://www.lights8080.com/tags/SpringCloud/"}]},{"title":"思维乱撞","slug":"杂谈/思维乱撞","date":"2021-05-12T16:00:00.000Z","updated":"2021-08-03T01:47:54.000Z","comments":true,"path":"p/525bfb06.html","link":"","permalink":"http://www.lights8080.com/p/525bfb06.html","excerpt":"思维局限性、美国个人支付、奋斗一生和及时行乐","text":"思维局限性、美国个人支付、奋斗一生和及时行乐 如果发现一个软件很活跃，但你觉得用另一个产品不是更好吗甚至这个软件还有某些缺陷。那一定是你没有遇到适用场景，当遇到时，你会说这正是我想要的产品。 产品经理，应该从产品角度去思考用怎样的功能满足需求，而不是简单的把需求做成功能。 不懂产品，迎合需求，长期来看就是负债。 开发人员如果更懂产品，会写出更良好的结构化代码，而不是给代码打补丁。 移动支付对我们来说太平常了。随处都可以拿出手机扫码支付。但是在美国个人支票几乎天天都还在用。出门购物刷信用卡/现金，金额较大的大都还用个人支票，比如：缴学费、交房租、还贷、包红包等。 我们看来这太落伍了，但站在系统角度，考虑健壮性和可用性，个人支票支付方式是个很好的方案。支付环境不依赖网络，设备，电力系统。单靠信用体系维护运行，零成本。 有些人奋斗一生，生活水平提高了不少，但迫于社会价值观，内心所想被压抑一生。 有些人及时行乐，生活虽不富裕，但活的简简单单，随心所欲（在不伤害别人的前提下）。 艰苦奋斗还是随遇而安，卸掉社会属性，单从生命个体的角度看，怎样的一生更符合生命的本质呢？ 写完之后我也不知道是怎么把它们联系在一起了。。。","categories":[{"name":"杂谈","slug":"杂谈","permalink":"http://www.lights8080.com/categories/%E6%9D%82%E8%B0%88/"}],"tags":[{"name":"杂谈","slug":"杂谈","permalink":"http://www.lights8080.com/tags/%E6%9D%82%E8%B0%88/"},{"name":"美国个人支票","slug":"美国个人支票","permalink":"http://www.lights8080.com/tags/%E7%BE%8E%E5%9B%BD%E4%B8%AA%E4%BA%BA%E6%94%AF%E7%A5%A8/"}]},{"title":"ELK-实践（架构选择&部署说明）","slug":"技术/ELK/ELK-实践（架构选择&部署说明）","date":"2021-05-09T16:00:00.000Z","updated":"2021-08-03T01:47:54.000Z","comments":true,"path":"p/39512cbe.html","link":"","permalink":"http://www.lights8080.com/p/39512cbe.html","excerpt":"介绍业务规模和架构选择，以及部署说明。基于7.11版本。","text":"介绍业务规模和架构选择，以及部署说明。基于7.11版本。 业务规模业务每天查询量在千万级，采集数据的规模上亿（后续会更大）。单台Logstash，数据延迟并不大，肉眼可见的Logstash的数据处理能力 架构选择ELK架构有很多种，这里简单列出常用的几个： 架构1（最为简单）Logstash -&gt; Elasticsearch -&gt; Kibana 架构2（使用Beats作为日志收集器）Beats -&gt; Logstash -&gt; Elasticsearch -&gt; Kibana 架构3（引入消息队列）Beats -&gt; Logstash -&gt; Kafka -&gt; Logstash -&gt; Elasticsearch -&gt; Kibana 其中架构3可能是被大家积极推荐和最为认可的理想架构。优点是消息队列可以把数据缓存起来避免数据丢失，可以抵挡浪涌削峰填谷，保护下游Logstash。适用于日志规模比较庞大的场景。 但我的实践中采用的架构2，理由如下： 消除不必要的复杂性，较低成本 关于数据丢失，Filebeat至少投递一次和Logstash持久队列可以解决这个问题 日志规模比较大的情况，可以水平扩展Logstash节点，一组Logstash之间实现负载 实践Logstash处理能力很强 部署说明&amp;实践配置介绍服务器环境配置，以及Elasticsearch、Kibana、Logstash、Filebeat的部署和配置参考。 logstash-7.11.2/config/lights.conf filebeat-7.11.2/inputs.d/lights.yml关于业务的这两个配置，不理解的请私信或留言吧 新建用户和修改文件夹权限123456# 新建用户$ groupadd elk$ useradd -m -d /home/elk -s /bin/bash -g elk elk# 修改文件夹所属组$ chown -R elk:elk /opt/elk$ chown -R elk:elk /data/elk 修改系统配置 vim /etc/sysctl.conf 1vm.max_map_count = 262144 vim /etc/security/limits.conf 12elk soft memlock unlimitedelk hard memlock unlimited Elasticsearch vim bin/elasticsearch 12export JAVA_HOME=/opt/elk/elasticsearch-7.11.2/jdkexport PATH=$JAVA_HOME/bin:$PATH vim config/jvm.options 12-Xms8g-Xmx8g vim config/elasticsearch.yml 1234567891011121314151617181920cluster.name: elasticsearchnode.name: node-1node.master: truenode.data: truenode.ingest: truepath.data: /data/elk/elasticsearch/datapath.logs: /data/elk/elasticsearch/logsnetwork.host: &quot;10.88.2.1&quot;http.port: 9200transport.port: 9300discovery.seed_hosts: [&quot;10.88.2.1&quot;]#discovery.type: single-nodecluster.initial_master_nodes: [&quot;10.88.2.1&quot;]http.cors.enabled: truehttp.cors.allow-origin: &quot;*&quot;bootstrap.memory_lock: truebootstrap.system_call_filter: truexpack.security.enabled: true 命令 1234# 启动$ sh ./bin/elasticsearch -d -p es.pid# 初始化内置用户$ bin/elasticsearch-setup-passwords auto Kibana vim config/kibana.yml 12345678910server.port: 5601server.host: &quot;0.0.0.0&quot;server.name: &quot;elk-1&quot;elasticsearch.hosts: [&quot;http://10.88.2.1:9200&quot;]elasticsearch.username: &quot;kibana_system&quot;elasticsearch.password: &quot;xxxxxxx&quot;i18n.locale: &quot;zh-CN&quot;xpack.reporting.encryptionKey: &quot;something_at_least_32_characters&quot;xpack.security.encryptionKey: &quot;something_at_least_32_characters&quot;xpack.encryptedSavedObjects.encryptionKey: &quot;something_at_least_32_characters&quot; 命令 123456# 启动$ nohup sh ./bin/kibana &gt;kibana.log 2&gt;&amp;1 &amp;# 查看端口netstat -napl|grep 5601# 停止kill &lt;port&gt; Logstash vim config/jvm.options 12-Xms4g-Xmx4g vim config/logstash.yml 12pipeline.workers: 4queue.type: persisted vim config/lights.conf 1# 略，请参考实践配置 命令1234# 测试配置$ ./bin/logstash -f config/lights.conf --config.test_and_exit# 启动$ nohup ./bin/logstash -f config/lights.conf --config.reload.automatic &gt; logstash.log &amp; Filebeat vim filebeat.yml 12345678filebeat.config.inputs: enabled: true path: $&#123;path.config&#125;/inputs.d/*.yml reload.enabled: true reload.period: 10soutput.logstash: hosts: [&quot;10.88.2.1:5044&quot;] vim inputs.d/lights.yml 1# 略，请参考实践配置 命令 12# 启动$ nohup ./filebeat -e &gt;filebeat.log 2&gt;&amp;1 &amp; Metricbeat vim metricbeat.yml 123456789101112131415161718192021metricbeat.config.modules: path: $&#123;path.config&#125;/modules.d/*.yml reload.enabled: truesetup.template.settings: index.number_of_shards: 1 index.codec: best_compressionsetup.kibana: host: &quot;10.88.2.1:5601&quot; username: &quot;elastic&quot; password: &quot;xxx&quot;output.elasticsearch: hosts: [&quot;10.88.2.1:9200&quot;] username: &quot;elastic&quot; password: &quot;xxx&quot;processors: - add_host_metadata: ~ vim modules.d/elasticsearch-xpack.yml 123456- module: elasticsearch xpack.enabled: true period: 10s hosts: [&quot;http://10.88.2.1:9200&quot;] username: &quot;elastic&quot; password: &quot;password&quot; 命令 1234# 开启elasticsearch模块$ ./metricbeat modules enable elasticsearch-xpack# 启动$ nohup ./metricbeat -e &gt;metricbeat.log 2&gt;&amp;1 &amp;","categories":[{"name":"技术","slug":"技术","permalink":"http://www.lights8080.com/categories/%E6%8A%80%E6%9C%AF/"}],"tags":[{"name":"ELK","slug":"ELK","permalink":"http://www.lights8080.com/tags/ELK/"}]},{"title":"Beats-Filebeat命令&配置说明","slug":"技术/ELK/Beats-Filebeat命令&配置说明","date":"2021-04-28T16:00:00.000Z","updated":"2021-08-03T01:47:54.000Z","comments":true,"path":"p/85a42e72.html","link":"","permalink":"http://www.lights8080.com/p/85a42e72.html","excerpt":"介绍Filebeat命令、配置以及最佳实战。 基于7.11版本。","text":"介绍Filebeat命令、配置以及最佳实战。 基于7.11版本。 命令 export：导出配置到标准输出（configuration, index template, ILM policy, dashboard） keystore：管理秘钥仓库 modules：管理模块配置 run：运行Filebeat。不知道命令的情况下，默认使用此命令 –modules MODULE_LIST：指定运行的模块 setup：一次性初始化环境。包括索引模板，ILM政策，写别名，Kibana仪表盘等 –dashboards：设置Kibana仪表盘，需配置连接Kibana信息 –pipelines：设置Elasticsearch的ingest pipelines -e：发送输出到标准错误而不是syslog –index-management：设置与Elasticsearch索引管理相关的组件（template, ILM policy, and write alias） test：测试配置文件 全局标记 -E, –E “SETTING_NAME=VALUE”：覆盖指定的配置 -M, –M “VAR_NAME=VALUE”：覆盖默认的模块配置 -c, –c FILE：指定Filebeat的配置文件 -f：指定管道配置文件 -d, –d SELECTORS：调试选择器，”*”：开启所有组件的调试，”publish”：开启调试”publish“相关信息 -e, –e：日志发送到stderr并禁用syslog文件输出 –path.config：设置配置文件路径 示例： 123456789# 一次性设置Elasticsearch索引和Kibana仪表板，-e：发送输出到标准错误而不是syslog./filebeat setup -e# 启用要运行的模块./filebeat modules enable system# 启动sudo chown root filebeat.yml sudo ./filebeat -c filebeat.yml -e 配置说明 filebeat.yml project paths：项目路径 general settings：配置包括Global、General config file loading：允许外部加载inputs和modules配置 modules：一种开始处理常见日志格式的快速方法 inputs：指定Filebeat查找和处理的数据 output：指定输出。如：Logstash、Elasticsearch、Kafka Processors：过滤和增强导出的数据 internal queue：存储事件的内部缓冲队列（内存和磁盘），负责缓冲输入事件并按批次发送到输出。 load balancing：配置输出的负载均衡 logging：Filebeat日志输出选项 http endpoint：Filebeat通过端点查看内部指标 autodiscover：容器运行时，自动发现配置，移动目标的监视系统 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188### project Paths# Filebeat主目录，默认安装路径path.home: path.config: $&#123;path.home&#125;path.data: $&#123;path.home&#125;/datapath.logs: $&#123;path.home&#125;/logs### Global Filebeat configuration options# 注册表的根路径，默认$&#123;path.data&#125;/registryfilebeat.registry.path: registryfilebeat.registry.file_permissions: 0600# 控制何时将注册表项写入磁盘(刷新)的超时值filebeat.registry.flush: 0s# Filebeat 在关闭之前等待发布者完成发送事件的关闭时间filebeat.shutdown_timeout: 5s### General configuration options# Beat的名字，默认使用主机名name: my-service# 标记列表，方便Kibana或Logstash过滤，如服务层，集群名等tags: [&quot;service-X&quot;, &quot;web-tier&quot;]# 属性中添加附加信息的可选字段，如环境信息fields: env: staging service_name: service-X# 将自定义字段作为顶级字段存储到到输出文档中，默认falsefields_under_root: false### Processors configuration# 定义模块的处理器，删除所有DEBUG消息processors: - drop_event: when: regexp: message: &quot;^DBG:&quot;### Logginglogging.level: infologging.to_files: truelogging.files: path: /var/log/filebeat name: filebeat # 日志文件最大大小，默认10M rotateeverybytes: 10485760 # 日志滚动删除旧文件，默认7 keepfiles: 7 # 日志文件滚动周期，默认禁用 interval: 24h # 标准错误记录到日志文件中 redirect_stderr: false# 定期记录内部发生变化的指标，默认开启logging.metrics.enabled: true# 记录内部指标的时间logging.metrics.period: 30s### 外部配置filebeat.config.inputs: enabled: true # 要检查的更改文件路径 path: configs/*.yml # 开启配置动态加载 reload.enabled: true # 指定检查文件更改的频率 reload.period: 10sfilebeat.config.modules: path: $&#123;path.config&#125;/modules.d/*.yml reload.enabled: false### 日志输入filebeat.inputs:- type: log enabled: true paths: - &quot;/var/log/wifi.log&quot; - &quot;/var/log/apache2/*&quot; - &quot;/var/log/*/*.log&quot; # 递归模式，默认false。For example: /foo/** expands to /foo, /foo/*, /foo/*/*, and so on recursive_glob.enabled: false # 文件编码 encoding: plain # 设置标记文件的位置 file_identity.inode_marker.path: /logs/.filebeat-marker # 标记列表，方便Kibana或Logstash过滤 tags: [&quot;service-X&quot;, &quot;web-tier&quot;] # 属性中添加附加信息的可选字段 fields: env: staging # 将自定义字段作为顶级字段存储到到输出文档中 fields_under_root: false # 包含的正则表达式列表，先于exclude_lines执行 include_lines: [&#x27;^ERR&#x27;, &#x27;^WARN&#x27;] # 排除的正则表达式列表 exclude_lines: [&#x27;^DBG&#x27;] # 多行消息匹配,Java 堆栈跟踪的例子（https://www.elastic.co/guide/en/beats/filebeat/7.x/multiline-examples.html） multiline.type: pattern multiline.pattern: &#x27;^[[:space:]]+(at|\\.&#123;3&#125;)[[:space:]]+\\b|^Caused by:&#x27; # 否定模式，true：没有匹配的行作为事件行的连贯行；false：匹配的行作为事件行的连贯行。默认false。 multiline.negate: false # 连贯行组合事件行之前（before）还是之后（after） multiline.match: after # 每个收割机获取文件时使用的缓冲区大小 harvester_buffer_size: 16384 # 单个日志消息的最大字节数，超出部分丢弃（10M） max_bytes: 10485760 # 排除文件 exclude_files: [&#x27;\\.gz$&#x27;] ##### Harvester closing options # 指定的时间段后关闭文件句柄，基于文件的修改，被扫描到后继续进行。建议设置一个大于最少更新频率的值，默认5分钟 close_inactive: 5m # 重命名或移动文件时关闭收割机，默认关闭 close_renamed: false # 删除文件时立马关闭收割机，当文件再次出现时被扫描到后继续进行，默认启用 close_removed: true # 收割机到达文件末尾时立刻关闭，默认禁用 close_eof: false # 指定时间后关闭，按照扫描频路再次开启新的收割机，默认禁用 close_timeout: 0 ##### State options # 指定时间段后文件无更新，则清除注册表中的状态，默认0，表示禁用清除注册表。clean_inactive设置必须大于ignore_older + scan_frequency，否则可能导致不断的重新发送全部内容 clean_inactive: 0 # 重命名或移动的文件，注册表中的状态将被清除。默认开启 clean_removed: true # 扫描频率，默认10秒 scan_frequency: 10s # 扫描顺序，默认禁用，可选值：modtime|filename。如果为此设置指定值，则可以使用scan.order配置文件是按升序还是降序进行扫描 scan.sort: scan.order: asc|desc # Filebeat 将开始在每个文件的结尾而不是开始读取新文件，适用于Filebeat尚未处理的文件。如果已经运行过Filebeat并且文件的状态已经保留，则tail_files配置无效。 tail_files: false # 忽略在指定时间跨度之前修改的所有文件，依赖于文件的修改时间。默认0，不忽略任何文件。必须大于close_inactive ignore_older: 0 # 限制并行启动的收割机数量 harvester_limit: 0 # 根据文件的inode和设备id来区分文件 file_identity.native: ~ # 根据路径来区分文件 file_identity.path: ~### 日志输出output.kafka: # kafka服务器 hosts: [&quot;kafka1:9092&quot;, &quot;kafka2:9092&quot;, &quot;kafka3:9092&quot;] # 动态设置topic topic: &#x27;%&#123;[fields.log_topic]&#125;&#x27; # 事件将仅发布到可用分区 partition.round_robin: reachable_only: false # ACK可靠性级别，默认1。0 = no response，1 = wait for local commit，-1 = wait for all replica to commit required_acks: 1 # gzip压缩级别，0禁用压缩 compression: gzip compression_level: 4 # 消息的最大字节数 max_message_bytes: 1000000# 负载均衡的Logstashoutput.logstash: hosts: [&quot;localhost:5044&quot;, &quot;localhost:5045&quot;] loadbalance: true worker: 2 ### Internal queue# 用于缓冲要发布的事件的内部队列配置。默认mem（内存队列）queue.mem: # 内存队列的最大缓冲事件数 events: 4096 # 发布所需的最小事件数，设置为0则发布事件直接输出使用，无需等待 flush.min_events: 2048 # 达到flush.min_events的最大等待事件，设置为0则无需等待 flush.timeout: 1squeue.disk: # 启用磁盘队列，指定最大大小既使用空间 max_size: 10GB path: $&#123;path.data&#125;/diskqueue # 队列文件以段的形式保存，每个段包含一些待发送到输出的事件，所有事件发送后删除 segment_size: max_size / 10 # 当事件等待输出时，从磁盘读取到内存中的事件数。调高此值可以提高输出速度，但是会占用更多内存 read_ahead: 512 # 队列等待事件写入磁盘时可以存储到内存中的事件数 write_ahead: 2048 # 磁盘错误导致的队列操作失败，重试间隔时间 retry_interval: 1s # 多个连续的写入磁盘错误，队列将重试间隔增加2倍，最大间隔时间 max_retry_interval: 30squeue.spool: 最佳实践调优 文件内容变更延迟发送事件调优默认基于内存缓冲事件，最晚需要11s才会发布事件到输出。filebeat.inputs.type:log.scan_frequency: 10s：文件的扫描频率queue.mem.flush.timeout: 1s：缓冲事件的超时时间queue.mem.flush.min_events: 2048：超时时间内发布事件所需的最小缓冲数 测试时编辑文件导致整个文件内容重新发送不要用vim修改，使用echo &quot;xxx&quot; &gt;&gt; log_file","categories":[{"name":"技术","slug":"技术","permalink":"http://www.lights8080.com/categories/%E6%8A%80%E6%9C%AF/"}],"tags":[{"name":"ELK","slug":"ELK","permalink":"http://www.lights8080.com/tags/ELK/"},{"name":"Filebeat","slug":"Filebeat","permalink":"http://www.lights8080.com/tags/Filebeat/"}]},{"title":"Kibana-介绍","slug":"技术/ELK/Kibana-介绍","date":"2021-04-28T16:00:00.000Z","updated":"2021-08-03T01:47:54.000Z","comments":true,"path":"p/18d3c38b.html","link":"","permalink":"http://www.lights8080.com/p/18d3c38b.html","excerpt":"介绍Kibana的侧边栏、面板类型、配置说明等。基于7.11版本。","text":"介绍Kibana的侧边栏、面板类型、配置说明等。基于7.11版本。 Kibana Kibana是一个开源分析和可视化平台，旨在与Elasticsearch协同工作。您使用Kibana搜索，查看和与存储在Elasticsearch索引中的数据进行交互。您可以轻松执行高级数据分析，并在各种图表，表格和地图中可视化您的数据。https://www.elastic.co/guide/en/kibana/7.11/index.html 侧边栏 Discover（数据探索）：搜索、过滤和展示所选索引模型（Index Pattern）文档数据 Visualize（可视化）：为数据创建可视化控件 Dashboard（仪表盘）：展示保存的可视化结果集合 Canvas（画布）：非常自由灵活对数据进行可视化布局与展现 Maps（地图）：已地图的方式展示聚合信息 Machine Learning（机器学习） Infrastructure（基础设施监控）：通过metricbeat监控基础服务。如：redis、rocketmq Metrics（度量应用）：探索整个生态系统中有关系统和服务的指标 Logs（日志）：实时跟踪相关的日志数据；提供了一个紧凑的，类似控制台的显示器。可以实时日志拖尾 APM（Application Performance Monitoring-应用程序性能监视）：业务跟踪及监控。 Uptime（正常运行时间）：监控应用程序和服务的可用性问题；通过HTTP/S，TCP和ICMP监控网络端点的状态 SIEM（Security Information &amp; Event Management-安全信息与事件管理）：安全分析师的高度交互式工作区 Dev Tools（开发工具）：包括控制台、查询分析和聚合 Stack Monitoring（ELK监控）：可视化监控数据 Management（Kibana管理）：包括索引模式的初始设置和持续配置等 Dashboard（仪表板）仪表板是用于分析数据的面板的集合。在仪表板上，您可以添加各种面板，可以重新排列并讲述关于数据的故事。 编辑仪表板： Add controls（添加控制器） Add markdown（添加说明文档） Arrange panels（面板排版） Clone panels（克隆面板） Customize time ranges（自定义时间范围） 探索仪表板数据： Inspect elements（检查元素）：查看可视化和保存的搜索背后的数据和请求 Explore underlying data（探索面板底层数据）：可以在其中查看和过滤可视化面板中的数据，为了探索仪表板上面板的底层数据，Kibana打开了Discover，可视化的索引模式、筛选器、查询和时间范围将继续应用。仅适用于单个索引模式的面板 自定义仪表板操作： Dashboard drilldowns（仪表盘深度探讨）：能够从另一个仪表板打开仪表板，带有时间范围、过滤器和其他参数，因此上下文保持不变。仪表板钻取可以帮助您从一个新的角度继续分析。 URL drilldowns（URL深度探讨）：能够从仪表板导航到内部或外部URL。目标URL可以是动态的，这取决于仪表板上下文或用户与面板的交互。 共享仪表板： 将代码嵌入网页中，必须具有Kibana访问权限才能查看嵌入式仪表板 直接链接到 Kibana 的控制面板 生成PDF/PNG报告 面板类型 Area（面积图）：使用面积图比较两个或多个类别随时间变化的趋势，并显示趋势的幅度。 Stacked Area（堆积面积图）：使用堆积面积图可视化部分-整体关系，并显示每个类别对累积总数的贡献。 Bar（条形图）：使用条形图对大量类别的数据进行比较，也支持水平条形图。 Stacked bar（堆积条形图）：使用堆叠的条形图可以比较分类值级别之间的数值。 Line（折线图）：使用折线图可以直观地显示一系列值，发现一段时间内的趋势并预测未来值。 Pie（饼图）：使用饼图显示多个类别之间的比较，说明一个类别相对于其他类别的优势，并显示百分比或比例数据。 Donut（甜甜圈图）：与饼形图相似，但删除了中心圆。当您想一次显示多个统计信息时，请使用甜甜圈图。 Tree map（树图）：将数据的不同部分关联到整体，使用树图可以有效利用空间来显示每个类别的总计百分比。 Heat map（热图）：显示数据的图形表示形式，其中各个值由颜色表示。当数据集包含范畴数据时，使用热图。 Goal（进度图）：显示指标如何朝固定目标发展，使用目标显示目标进度状态的易于阅读的视觉效果。 Gauge（计量图）：沿比例尺显示数据，使用计量图来显示度量值与参考阈值的关系。 Metric（度量值）：显示聚合的单个数值。 Data table（表格数据）：以表格格式显示原始数据或聚合结果。 Tag cloud（标签云）：显示单词在出现的频率，使用标签云可以轻松生成大型文档的摘要。 Maps（地图） Lens（透镜）：创建强大的数据可视化效果的最简单、最快捷的方法。可以将任意多的数据字段拖放到可视化构建窗格中。 TSVB（时间序列数据）：TSVB是时间序列数据可视化工具，充分利用Elasticsearch聚合框架的功能。可以组合无数个聚合来显示数据。支持：选择不同的数据展示方式、叠加注释事件等。 Timelion（时间序列数据）：时间序列数据可视化工具，可以在单个可视化文件中组合独立的数据源。在7.0及更高版本中，不建议使用Timelion应用。 Vega（自定义可视化）：使用Vega和Vega-Lite构建自定义可视化，并由一个或多个数据源支持。支持：使用嵌套或父/子映射的聚合、没有索引模式的聚合、自定义时间过滤器查询、复杂的计算、从_source而不是聚合中提取数据等。 Controls（控制器）：可以实时过滤面板上的数据，支持选择列表和范围滑杆 Markdown（文本编辑器）：当您要将上下文（例如重要信息，说明和图像）添加到仪表板上的其他面板时，请使用Markdown。 Alerts and Actions（监控警报） General alert details（警报详细信息）： Name：警报名称，显示在警报列表，帮助识别和查询警报 Tags：警报标签列表，显示在警报列表，有助于查询和组织警报 Check every：检查警报条件的频率 Notify every：限制重复报警的频率 Alert type and conditions（警报类型和条件）：选择不同的警报类型不同的条件表达形式。 Index threshold：索引阈值警报类型 Action type and action details（动作类型和详细信息）：每个操作都必须指定一个连接器实例。 Index：Index data into Elasticsearch Email：Send email from your server Server log：Add a message to a Kibana log Webhook：Send a request to a web service Graph（图形分析）图形分析功能使您能够发现 Elasticsearch 索引中的项目是如何相关的。您可以研究索引词汇之间的连接，并查看哪些连接最有意义。这在各种应用程序中都很有用，从欺诈检测到推荐引擎。 例如，图形浏览可以帮助您发现黑客所针对的网站漏洞，从而可以加固您的网站。或者，您可以向电子商务客户提供基于图的个性化推荐。 图形分析特性为 Kibana 提供了一个简单但强大的图形探索 API 和一个交互式图形可视化工具。两者都可以在现有的 Elasticsearch 索引中使用ー你不需要存储任何额外的数据来使用这些特性。 kibana.yml1234567891011server.port: 5601server.host: &quot;0.0.0.0&quot;server.name: &quot;elk-1&quot;i18n.locale: &quot;zh-CN&quot;elasticsearch.hosts: [&quot;http://your_elasticsearch_host:9200&quot;]elasticsearch.username: &quot;kibana_system&quot;elasticsearch.password: &quot;xxxxxxxxxxxxxxx&quot;# 防止会话在重启时失效xpack.security.encryptionKey: &quot;something_at_least_32_characters&quot;# 防止挂起的报告在重新启动时失败xpack.reporting.encryptionKey: &quot;something_at_least_32_characters&quot; 问题1. 创建索引失败错误信息：POST 403 (forbidden) on create index pattern解决办法： 12345678PUT _settings&#123; &quot;index&quot;: &#123; &quot;blocks&quot;: &#123; &quot;read_only_allow_delete&quot;: &quot;false&quot; &#125; &#125;&#125;","categories":[{"name":"技术","slug":"技术","permalink":"http://www.lights8080.com/categories/%E6%8A%80%E6%9C%AF/"}],"tags":[{"name":"ELK","slug":"ELK","permalink":"http://www.lights8080.com/tags/ELK/"},{"name":"Kibana","slug":"Kibana","permalink":"http://www.lights8080.com/tags/Kibana/"}]},{"title":"人格分裂","slug":"杂谈/人格分裂","date":"2021-04-27T16:00:00.000Z","updated":"2021-08-03T01:47:54.000Z","comments":true,"path":"p/3e42ed8d.html","link":"","permalink":"http://www.lights8080.com/p/3e42ed8d.html","excerpt":"人格分裂和癌细胞一样，是人类无法驾驭的更强大的存在","text":"人格分裂和癌细胞一样，是人类无法驾驭的更强大的存在 癌细胞为什么是人类无法驾驭的更强大的存在呢？生命的存在是因为细胞分裂，正常的细胞分裂，都会磨损真核细胞染色体末端的端粒酶，端粒酶没有了就无法继续分裂。但是癌细胞却有修复端粒酶的能力，可以无限繁殖下去，这正是永生的破解密码。 人格分裂/多重人格被定义为一种心理疾病，一个生命个体上存在两种或以上不同身份和人格状态。直白点说就是一个身体里住着好几个灵魂。 把这灵魂分为两类：受我们意愿所控制的灵魂定义为主灵魂，那些不受我们意愿控制的灵魂为独立灵魂。 人的一生中应该都会遇到许多次的轻微的人格分裂时刻。比如当你失恋了，你满脑子都是失恋的情景，你明明不想让自己想那个情景，但是就是不受控制的在脑子里闪现。这个失恋的情景就是独立灵魂，不受控制的让你去想。 当大脑进入这样的状态时，有了不受控制的独立意识，我认为就是轻微的人格分裂了。这种状态下，大脑非常疲惫（就好像汽车一直开在160km/h，又无法刹车），无法集中精力做事，甚至会影响你的身体状态。 在这种状态下，喜欢抽烟喝酒，想用外界的干预来抑制或麻醉独立灵魂。 人格分裂对于生命体来说无疑是非常痛苦的，但是对于人脑组织或灵魂来说，是不是进化了呢？ 灵魂是不是更高维度的存在，降维寄宿到人的身体里。想象一下灵魂出窍，其实不是生命体有灵魂，而是灵魂寄宿在生命体当中，人格分裂的人，是多个灵魂共同寄宿在同一个生命体中，生命结束的时候灵魂就会飞走。","categories":[{"name":"杂谈","slug":"杂谈","permalink":"http://www.lights8080.com/categories/%E6%9D%82%E8%B0%88/"}],"tags":[{"name":"杂谈","slug":"杂谈","permalink":"http://www.lights8080.com/tags/%E6%9D%82%E8%B0%88/"}]},{"title":"Beats-Filebeat介绍","slug":"技术/ELK/Beats-Filebeat介绍","date":"2021-04-27T16:00:00.000Z","updated":"2021-08-03T01:47:54.000Z","comments":true,"path":"p/2becaa3c.html","link":"","permalink":"http://www.lights8080.com/p/2becaa3c.html","excerpt":"Filebeat介绍，包括工作方式、模块、如何避免数据重复、处理器的速查表。 基于7.11版本。","text":"Filebeat介绍，包括工作方式、模块、如何避免数据重复、处理器的速查表。 基于7.11版本。 Beats是一款轻量级数据采集器，你可以将它作为代理程序安装在你的服务器上，然后将操作数据发送到 Elasticsearch。可以直接发送数据到 Elasticsearch 或者通过 Logstash，在那里你可以进一步处理和增强数据。 Filebeat（日志文件） Metricbeat（指标） Heartbeat（可用性监控） Functionbeat（函数计算采集器） Filebeat Filebeat是用于转发集中日志数据的传输工具。作为服务器上的代理安装，收集日志事件，并将它们转发到Elasticsearch或Logstash。https://www.elastic.co/guide/en/beats/filebeat/7.11/index.html 工作方式当启动Filebeat，会根据显示指定的日志数据启动一个或多个输入，每个日志文件都会启动一个收割机（harvester）。每个收割机会读取日志的最新内容，并将日志数据发送到libbeat，libbeat汇总事件并发送到输出。 Filebeat由两个主要部分组成：inputs和harvesters。它们一起工作以尾部文件将事件数据发送到指定的输出。 收割机（harvesters）：一个收割机负责读取单个文件的内容。收割机逐行读取每个文件并将内容发送到输出。每个文件启动一个收割机负责打开和关闭文件，收割机运行时文件描述符保持打开状态。如果文件被删除或重命名，Filebeat将继续读取该文件，副作用是磁盘上的空间将保留到收割机关闭为止。 输入（inputs）：一个输入负责管理收割机并查找所有可读取的资源。日志类型输入检查每个文件，以查看收割机是否需要启动，是否已经运行，或是否需要忽略该文件。自收割机关闭之后，如果文件大小有更改才会获取新行。 Filebeat保持每个文件的状态，并经常将状态刷新到磁盘的注册表文件。该状态用于记录收割机正在读取的最后一个偏移量并确保发送所有的日志行。如果输出不可达，则Filebeat会保持跟踪发送的最后几行，并在输出可用时继续读取文件。Filebeat运行时状态信息也会保持在内存中，当Filebeat重启时，将使用注册文件中的数据重新构建状态，并且在最后一个已知位置继续每个收割机。 对于每个输入，Filebeat会保持每个文件的状态。由于文件可以重命名和移动，因此文件名和路径不能标识一个文件。Filebeat将存储每个文件的唯一标识符以检测文件是否以前被获取过。 Filebeat保证事件将至少一次传递到输出，并且不会丢失数据。因为他在注册文件中存储了每个事件的传递状态。如果输出被阻止或未确认所有事件的情况下，Filebeat将继续尝试发送事件，直到输出确认接收为止。如果Filebeat在发送事件的过程中关闭，则不会等待输出确认所有的事件。重启Filebeat时，将再次发送关闭之前输出未确认的所有事件。这样可以确保每个事件至少发送一次，但是有可能会重复发送。 如何避免Elasticsearch数据重复由于Beats框架确保至少一次交付，又由于Elasticsearch的文档ID通常是接受到数据后才设置的，因此重复事件被索引为新文档。通过在建立索引期间设置，则Elasticsearch会覆盖现有文档而不是新创建一个新文档。 在Beats中设置文档ID。 在Logstash管道设置文档ID。 填充地理位置信息基于IP地址填充地理位置信息。然后可以使用此信息来可视化IP地址在地图中的位置。 Filebeat与Elasticsearch中的GoeIp处理器一起使用 Logstash中使用GeoIP过滤器 模块（Modules） Filebeat模块简化了常见的日志格式的收集，解析和可视化。每个Filebeat模块由一个或多个文件集组成，这些文件集包含摄取节点管道（ingest node pipelines），Elasticsearch模板，Filebeat输入配置和Kibana仪表板。 use ingest pipelines for parsing use Logstash pipelines for parsing 如Nginx日志，由一个或多个文件集组成（access和error）： Filebeat输入配置要查找的日志文件路径，还负责在需要时将多行事件缝合在一起。 Elasticsearch Ingest Node管道定义，用于解析日志行。 定义字段，为每个字段正确的配置到Elasticsearch。 Kibana仪表盘可视化日志文件 处理器（Processors）过滤和增强数据的处理器如果只需要导出的数据的一部分或者需要增强导出数据。Filebeat提供了两个选项来过滤和增强导出的数据。 可以为每个输入指定包含和排除的行或文件，需要为每个输入配置选项。（include_lines, exclude_lines, and exclude_files options） 定义处理器（Processor）可以的导出的所有数据进行全局处理。 可以在配置中定义处理器，在发送到输出之前处理所有事件。libbeat提供的处理器分为： 减少导出字段 增加元数据增强事件 执行其他处理和解码 每个处理器都接收一个事件，对该事件应用已定义的操作，然后返回该事件。如果定义处理器列表，则将按照在Filebeat配置文件中定义的顺序执行它们。执行顺序：event -&gt; processor 1 -&gt; event1 -&gt; processor 2 -&gt; event2 … 123456789101112131415processors: - if: &lt;condition&gt; then: - &lt;processor_name&gt;: &lt;parameters&gt; - &lt;processor_name&gt;: &lt;parameters&gt; ... else: - &lt;processor_name&gt;: &lt;parameters&gt; - &lt;processor_name&gt;: &lt;parameters&gt; ... add_docker_metadata使用来自Docker容器的相关元数据注释每个事件。包括（Container ID、Name、Image、Labels） add_fields将其他字段添加到事件中 add_host_metadata为事件添加主机信息 add_id为事件生成唯一的ID add_labels将一组键值对添加到事件 add_locale通过将机器的时区偏离UTC或时区名称来丰富每个事件 add_tags将标签添加到标签列表中 convert将事件中的字段转换为其他类型，例如将字符串转换为整数。 copy_fields将一个字段复制到另一个字段。 decode_base64_field指定要对base64进行解码的字段 dissect解剖处理器使用定义的模式对传入的字符串进行标记 drop_event如果满足相关条件，则drop_event处理器将丢弃整个事件。条件是强制性的，因为没有一个条件，所有事件都将被丢弃 drop_fields指定在满足特定条件时要删除的字段，条件是可选的。@timestamp和type字段在列表中，也不能删除他们。 fingerprint根据事件字段的指定子集生成事件的指纹。 include_fields指定在满足特定条件时要导出的字段，条件是可选的。@timestamp和type字段，也始终将其导出。 rename指定要重命名的字段的列表。 script执行Javascript代码以处理事件。该处理器使用ECMAScript 5.1的纯Go实现，并且没有外部依赖性。 timestamp时间戳处理器从字段解析时间戳。默认情况下，时间戳处理器将已解析的结果写入@timestamp字段。 truncate_fields将字段截断为给定的大小。如果字段的大小小于限制，则该字段将保持不变。 urldecode指定要从URL编码格式解码的字段列表","categories":[{"name":"技术","slug":"技术","permalink":"http://www.lights8080.com/categories/%E6%8A%80%E6%9C%AF/"}],"tags":[{"name":"ELK","slug":"ELK","permalink":"http://www.lights8080.com/tags/ELK/"},{"name":"Filebeat","slug":"Filebeat","permalink":"http://www.lights8080.com/tags/Filebeat/"}]},{"title":"ELK-加密通信的说明和配置教程","slug":"技术/ELK/ELK-加密通信的说明和配置教程","date":"2021-04-27T16:00:00.000Z","updated":"2021-08-03T01:47:54.000Z","comments":true,"path":"p/12eaa363.html","link":"","permalink":"http://www.lights8080.com/p/12eaa363.html","excerpt":"介绍Elasticsearch节点之间的加密通信、浏览器与Kibana之间的加密通信、Kibana与Elasticsearch之间的加密通信、操作步骤和配置说明。基于7.11。","text":"介绍Elasticsearch节点之间的加密通信、浏览器与Kibana之间的加密通信、Kibana与Elasticsearch之间的加密通信、操作步骤和配置说明。基于7.11。 1 Elasticsearch加密通信Elastic Stack安全特性能够加密加密往返于Elasticsearch集群以及从其内部的通信。使用传输层安全性(TLS/SSL)保护连接的安全。未启用加密的群集将以纯文本格式（包括密码）发送所有数据。如果启用了Elasticsearch安全功能，除非您具有试用许可证，否则必须配置SSL/TLS进行节点间通信。 1.1 Elasticsearch节点之间的加密通信https://www.elastic.co/guide/en/elasticsearch/reference/7.11/configuring-tls.html#tls-transport Verify that the xpack.security.enabled setting is true.（启用安全功能） Generate a private key and X.509 certificate.（生成私钥和X.509证书） Configure each node to: Required: Enable TLS on the transport layer.（传输层启用TLS） Recommended: Enable TLS on the HTTP layer.（HTTP层启用TLS） 1.2 HTTP客户端的加密通信https://www.elastic.co/guide/en/elasticsearch/reference/7.11/configuring-tls.html#tls-http Generate node certificates.（生成节点证书） Enable TLS and specify the information required to access the node’s certificate.（启用TLS并指定节点证书） Restart Elasticsearch.（重启Elasticsearch） 2 Kibana通信加密传输层安全安全协议(SSL)和传输层安全协议(TLS)为数据传输提供加密。虽然这些术语通常可以互换使用，但 Kibana 只支持 TLS，它取代了旧的 SSL 协议。浏览器将流量发送到 Kibana，Kibana 将流量发送到 Elasticsearch，这些通信通道分别配置为使用 TLS。 2.1 浏览器与Kibana之间的加密通信https://www.elastic.co/guide/en/kibana/7.11/configuring-tls.html#configuring-tls-browser-kib 获得 Kibana 的服务器证书和私钥 配置 Kibana 以访问服务器证书和私钥 将 Kibana 配置为为入站连接启用 TLS 重启 Kibana 2.2 Kibana与Elasticsearch之间的加密通信https://www.elastic.co/guide/en/kibana/7.11/configuring-tls.html#configuring-tls-kib-es Enable TLS on the HTTP layer in Elasticsearch.（在Elasticsearch的HTTP层上启动TLS） Obtain the certificate authority (CA) certificate chain for Elasticsearch.（获取Elasticsearch的证书颁发机构(CA)证书链） used the elasticsearch-certutil http command,include the CA certificate chain in PEM format.（使用elasticsearch-certutil http命令生成CA证书链） extract the CA certificate.（通过PKCS#12文件提取CA证书链） Configure Kibana to trust the Elasticsearch CA certificate chain for the HTTP layer.（配置Kibana以信任HTTP层的Elasticsearch CA证书链） Configure Kibana to enable TLS for outbound connections to Elasticsearch.（配置Kibana与Elasticsearch的连接启用TLS） 3 操作步骤 操作命令 12345678910111213141516# 进入Elasticsearch目录cd /data/elk/elasticsearch-7.11.2# 创建证书颁发机构：获得文件：elastic-stack-ca.p12./bin/elasticsearch-certutil ca# 为每个节点生成证书和私钥，获得文件：elastic-certificates.p12./bin/elasticsearch-certutil cert --ca elastic-stack-ca.p12# 生成专门用于加密HTTP客户端通信的证书，获得文件：elasticsearch-ssl-http.zip./bin/elasticsearch-certutil http# 解压HTTP通信证书，获得文件：elasticsearch/http.p12和kibana/elasticsearch-ca.pemunzip elasticsearch-ssl-http.zip# 在每个Elasticsearch节点的配置目录中创建一个文件夹certs，放置安全证书mkdir /data/elk/elasticsearch-7.11.2/config/certscp elastic-certificates.p12 config/certscp elasticsearch/http.p12 config/certs# 复制HTTP通信证书到Kibana配置目录cp kibana/elasticsearch-ca.pem /data/elk/kibana-7.11.2/config 生成加密HTTP客户端通信证书说明（./bin/elasticsearch-certutil http）参考：https://lights8080.github.io/post/es-an-quan-security 4 参数配置 Elasticsearchelasticsearch-7.11.2/config/elasticsearch.yml 12345678910# 在节点上启用Elasticsearch安全功能xpack.security.enabled: true# 节点间加密通信配置xpack.security.transport.ssl.enabled: truexpack.security.transport.ssl.verification_mode: certificate xpack.security.transport.ssl.keystore.path: elastic-certificates.p12 xpack.security.transport.ssl.truststore.path: elastic-certificates.p12 # HTTP加密通信配置xpack.security.http.ssl.enabled: truexpack.security.http.ssl.keystore.path: &quot;certs/http.p12&quot; Kibanakibana-7.11.2/config/kibana.yml 1234# 配置Kibana与Elasticsearch的连接启用TLSelasticsearch.hosts: [&quot;https://127.0.0.1:9200&quot;]# 配置信任HTTP层的Elasticsearch CA证书链elasticsearch.ssl.certificateAuthorities: [&quot;/data/elk/kibana-7.11.2/config/elasticsearch-ca.pem&quot;] Logstashlogstash-7.11.2/config/logstash-sample.conf 12345678output &#123; elasticsearch &#123; hosts =&gt; [&quot;https://127.0.0.1:9200&quot;] user =&gt; &quot;elastic&quot; password =&gt; &quot;xxxxxx&quot; cacert =&gt; &quot;/data/elk/elasticsearch-7.11.2/config/certs/elasticsearch-ca.pem&quot; &#125;&#125; Metricbeatmetricbeat-7.11.2/modules.d/elasticsearch-xpack.yml 1234567- module: elasticsearch xpack.enabled: true period: 10s hosts: [&quot;https://127.0.0.1:9200&quot;] username: &quot;elastic&quot; password: &quot;xxxxxx&quot; ssl.certificate_authorities: [&quot;/data/elk/elasticsearch-7.11.2/config/certs/elasticsearch-ca.pem&quot;] 其他Elastic产品使用加密通信略","categories":[{"name":"技术","slug":"技术","permalink":"http://www.lights8080.com/categories/%E6%8A%80%E6%9C%AF/"}],"tags":[{"name":"ELK","slug":"ELK","permalink":"http://www.lights8080.com/tags/ELK/"}]},{"title":"Elasticsearch-安全特性（Security）","slug":"技术/ELK/Elasticsearch-安全特性（Security）","date":"2021-04-26T16:00:00.000Z","updated":"2021-08-03T01:47:54.000Z","comments":true,"path":"p/da8f862e.html","link":"","permalink":"http://www.lights8080.com/p/da8f862e.html","excerpt":"Elasticsearch安全特性，介绍加密通讯的基本原理、开启安全特性的操作步骤、如何生成节点证书、用户认证和相关概念等。 基于7.11版本。","text":"Elasticsearch安全特性，介绍加密通讯的基本原理、开启安全特性的操作步骤、如何生成节点证书、用户认证和相关概念等。 基于7.11版本。 一、安全特性Elastic Stack安全功能使您可以轻松保护集群。 Elasticsearch集群保护方式： 通过密码保护，基于角色的访问控制和IP过滤防止未经授权的访问。 使用SSL/TLS加密保留数据的完整性。 维护审计跟踪，知道谁在对集群进行操作 Elasticsearch配置安全的简易步骤： 集群内每个节点设置为xpack.security.enabled: true 为节点间通信配置TLS/SSL【#加密通讯】 启动Elasticsearch 设置内置用户和密码（命令：elasticsearch-setup-passwords auto） 设置角色和用户，控制对Elasticsearch的访问 (可选)启用审计功能xpack.security.audit.enabled: true，并重启集群 1 加密通讯 未启用加密的群集将以纯文本格式（包括密码）发送所有数据。如果启用了Elasticsearch安全功能，除非您具有试用许可证，否则必须配置SSL/TLS进行节点间通信。 Elasticsearch集群节点间通信使用SSL/TLS加密，保护节点安全有助于降低基于网络的攻击的风险 要求节点使用SSL证书添加到集群时进行身份验证，新节点的身份验证有助于防止流氓节点加入群集并通过复制接收数据 Elasticsearch集群配置STL 为每个Elasticsearch节点生成一个私钥和X.509证书【#1.3 生成节点证书】 在集群中配置每个节点，以使用其签名证书标识自己，并在传输层上启用TLS。还可以选择在HTTP层上启用TLS 配置Kibana以加密浏览器和Kibana服务器之间的通信，并通过HTTPS连接到Elasticsearch 配置其他Elastic产品使用加密通信 1.1 加密集群中节点之间的通信 生成节点证书【#1.3 生成节点证书】 启用TLS并制定访问节点证书所需要的信息1234xpack.security.transport.ssl.enabled: truexpack.security.transport.ssl.verification_mode: certificatexpack.security.transport.ssl.keystore.path: elastic-certificates.p12xpack.security.transport.ssl.truststore.path: elastic-certificates.p12 （可选）如果你用密码保护节点的证书，将密码添加到 Elasticsearch 密钥存储库 重启Elasticsearch 1.2 加密HTTP客户端通信 生成HTTP证书【#1.3 生成节点证书】 启用TLS并制定访问节点证书所需要的信息12xpack.security.http.ssl.enabled: truexpack.security.http.ssl.keystore.path: &quot;http.p12&quot; （可选）如果你用密码保护节点的证书，将密码添加到 Elasticsearch 密钥存储库 重启Elasticsearch 1.3 生成节点证书 （可选）为 Elasticsearch 集群创建一个证书颁发机构。 ./bin/elasticsearch-certutil ca输出文件是一个PKCS#12密钥存储库，其中包含证书颁发机构的公共证书和用于签署节点证书的私钥。 为集群中的每个节点生成证书和私钥交互形式：./bin/elasticsearch-certutil cert --ca elastic-stack-ca.p12命令形式：./bin/elasticsearch-certutil cert --ca elastic-stack-ca.p12 --dns localhost --ip 127.0.0.1,::1 --out config/certs/node-1.p12输出是一个包含节点证书、节点密钥和CA证书的PKCS#12密钥存储库。 （可选）生成专门用于加密 HTTP 客户端通信的附加证书。 ./bin/elasticsearch-certutil http命令步骤如下： 123456789101112131415161718192021222324252627282930## Do you wish to generate a Certificate Signing Request (CSR)? - 是否生成证书签名请求(CSR)?Generate a CSR? [y/N]N## Do you have an existing Certificate Authority (CA) key-pair that you wish to use to sign your certificate? -是否有一个现有的证书颁发机构(CA)密钥对，您希望使用它来签署证书?Use an existing CA? [y/N]y## What is the path to your CA? - 你的CA路径在哪里?CA Path: /data/elk/elasticsearch-7.11.2/elastic-stack-ca.p12## How long should your certificates be valid? - 您的证书应该多长时间有效?For how long should your certificate be valid? [5y] 3Y## Do you wish to generate one certificate per node? - 是否希望每个节点生成一个证书?Generate a certificate per node? [y/N]N## Which hostnames will be used to connect to your nodes? - 哪些主机名将用于连接到您的节点?### Enter all the hostnames that you need, one per line. - 输入需要的所有主机名，每行一个。### When you are done, press &lt;ENTER&gt; once more to move on to the next step. - 完成后，再次按&lt;ENTER&gt;继续下一步。Is this correct [Y/n]Y## Which IP addresses will be used to connect to your nodes? - 哪些IP地址将用于连接到您的节点?### Enter all the IP addresses that you need, one per line.### When you are done, press &lt;ENTER&gt; once more to move on to the next step.Is this correct [Y/n]Y## Other certificate options - 其他证书选项Do you wish to change any of these options? [y/N]N输出是一个.zip 文件，包含 Elasticsearch 和 Kibana 各自的一个目录 输出文件elasticsearch-ssl-http.zip 123456789/elasticsearch|_ README.txt|_ http.p12|_ sample-elasticsearch.yml/kibana|_ README.txt|_ elasticsearch-ca.pem|_ sample-kibana.yml 将节点证书复制到适当的位置 在每个Elasticsearch节点上的配置目录中创建文件夹certs。如：/home/es/config/certs。 在每个节点上，将创建的证书复制到certs目录（通常是一个.p12文件） 如果生成了HTTP证书，复制http.p12到certs目录 配置其他Elastic产品，将证书复制到相关目录 2 用户认证安全功能提供了基于角色的访问控制（RBAC）机制，该机制使您可以通过为角色分配特权并将角色分配给用户或组来授权用户。安全功能还提供了基于属性的访问控制（ABAC）机制，使您可以使用属性来限制对搜索查询和聚合中文档的访问。 role-based access control (RBAC) Secured Resource（访问受限的资源）：索引，别名，文档，字段，用户和Elasticsearch群集本身都是受保护对象。 Privilege（特权）：对受保护的资源执行的一个或多个动作的命名组，如read是索引特权，代表所有启用读取已索引/存储的数据的操作。 Permissions（权限）：针对受保护资源的一组一个或多个特权，权限可以很容易地用语言来描述。 Role（角色）：一组命名的权限 User（用户）：经过身份验证的用户 Group（用户组）：用户所属的一个或多个组 内置用户 这些内置用户存储在指定的.security索引中，由Elasticsearch管理。 elasticsearch-setup-passwords工具是首次设置内置用户密码的最简单方法 基于令牌的身份验证 token-service：访问令牌（根据OAuth2规范生成访问令牌和刷新令牌）是短期令牌，默认情况下20分钟后过期。（Authorization: Bearer xxx） api-key-service：API秘钥，默认情况下API密钥不会过期，创建时，可以指定到期时间和权限。（Authorization: ApiKey xxx） 二、相关概念 SSL（Secure Socket Layer)/TLS(Transport Layer Security） 数字证书：互联网通讯中标志通讯各方身份信息的一系列数据 X.509：是一种数字证书（Public Key Certificates）的格式标准，主要定义了证书中应该包含哪些内容。 HTTPS依赖的TLS/SSL证书使用的就是使用的X.509格式。一个X.509 Certificate包含一个Public Key和一个身份信息（a hostname, or an organization, or an individual），它要么是被CA签发的要么是自签发的。 CA（Certificate Authority）：颁发数字证书的权威机构，承担公钥体系中公钥的合法性检验的责任。 编码格式：用来存储和发送公钥/私钥、证书和其他数据的文件格式；分为DER和PEM DER（Distinguished Encoding Rules）：二进制不可读，常用于Windows系统 PEM（Privacy-Enhanced Mail）：内容是BASE64编码，常用于*NIX系统 PKCS（Public Key Cryptography Standards）：公钥密码学标准 PKCS#12：描述个人信息交换语法标准，通常用来存储Private Keys和Public Key Certificates（例如前面提到的X.509）的文件格式，使用基于密码的对称密钥进行保护。 三、参考文档安全集群加密通讯","categories":[{"name":"技术","slug":"技术","permalink":"http://www.lights8080.com/categories/%E6%8A%80%E6%9C%AF/"}],"tags":[{"name":"ELK","slug":"ELK","permalink":"http://www.lights8080.com/tags/ELK/"},{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"http://www.lights8080.com/tags/Elasticsearch/"}]},{"title":"Elasticsearch-聚合（Aggregations）","slug":"技术/ELK/Elasticsearch-聚合（Aggregations）","date":"2021-04-24T16:00:00.000Z","updated":"2021-08-03T01:47:54.000Z","comments":true,"path":"p/9df50bf9.html","link":"","permalink":"http://www.lights8080.com/p/9df50bf9.html","excerpt":"Elasticsearch聚合速查表，介绍指标聚合、桶分聚合、管道聚合的分类和聚合示例。 基于7.11版本。","text":"Elasticsearch聚合速查表，介绍指标聚合、桶分聚合、管道聚合的分类和聚合示例。 基于7.11版本。 聚合将数据汇总为指标, 统计, 或其他分析。 聚合分类 Metric：指标聚合，从文档字段值中计算指标，如总和、平均值等 Bucket：桶分聚合，根据字段值、范围或其他条件将文档分组为桶 Pipeline：管道聚合，从其他的聚合结果作为输入 Bucket Adjacency matrix：邻接矩阵，获取矩阵每个组的计数 Auto-interval date histogram：时间柱状图，根据桶的数量自动的选择桶的间隔 Children：子聚合，如：join field Composite：多存储桶聚合，类似于多字段分组 Date histogram：日期柱状图，可以按日历感知时间间隔（如：day，week，houth）和固定时间间隔。 Date range：时间范围聚合，from：从大于等于某个时间，to：到小于某个时间 Filter：过滤器聚合，将当前的聚合的上下文缩小到一组特定文档。在当前聚合上应用过滤，不影响其他聚合器。 Filters：多桶过滤器聚合，每个桶都与一个过滤器相关联 Geo-distance：地理距离聚合，工作在geo_point字段上，定义一个原点或一组距离范围的桶，评估落在每个桶的文档。 Geo hash grid：网格聚合，每个单元格使用自定义精度的geohash进行标记，geohash可以在1~12之间选择精度 Geotile grid：网格聚合，每个单元格对应许多在线地图的图块，使用{zoom}/{x}/{y}标记 Global：在搜索的上下文中定义一个，不受搜索影响的上下文进行聚合。与Filter对应 Histogram：柱状图聚合，指定间隔，返回落在间隔内的文档数 IP range：IP类型字段的范围聚合 Missing：NULL字段聚合 Nested：嵌套文档聚合 Parent：父文档聚合 Range：范围聚合，定义一组范围，每组范围代表一个桶 Rare terms：稀少（长期分布但不频繁的项）的术语聚合 Reverse nested：在嵌套聚合内定义聚合父文档 Sampler：采样器聚合，将聚合的文档限制在得分最高的文档上，降低繁重缓慢的聚合成本。shard_size：限制在每个分片上使用得分最高的文档数 Diversified sampler：多样化采集聚合，采用多样化的设置进行抽样可以提供一种方法来消除内容偏差 Terms：动态桶聚合。结果是近似值，可以通过size、shard_size来控制其精度。对标关系数据库中的group by。size：定义返回桶的数；shard_size：每个分片使用文档样本数 Significant terms：显著的关键词聚合，通过background sets（背景集合）对比聚合数据。通常使用整个索引库内容当做背景集合，可以通过background_filter设置。 Significant text：显著的文本聚合，像Significant terms一样，区别是作用在text字段 Variable width histogram：动态的宽度柱状图聚合，定义桶数，动态确定桶间隔。 Subtleties of bucketing range fields：范围字段导致桶数大于文档数 Metric Avg：计算平均值，单值的指标聚合，提取文档的数值型字段或提供的脚本。 Min：计算最小值，histogram fields时，返回values中的最小值 Max：计算最大值 Sum：计算总和 Boxplot：盒型图，返回最大值、最小值、25%、50%和75%的值。常用语响应时间的分析 Cardinality：去重求和，计算不同值的近似计数，可以从文档中的特定字段提取值，也可以通过脚本 stats：统计信息，多值的指标聚合，可以从文档中的特定字段提取值，也可以通过脚本 status: min, max, sum, count and avg string stats: count, min_length, max_length, avg_length, entropy extended stats: sum_of_squares, variance, std_deviation geo：地图 geo bounds: 地理边界聚合 geo centroid: 地理重心聚合 Median absolute deviation：中位数绝对偏差，更可靠的统计信息，可以减少异常值对于数据集的影响。 Percentile rank：百分比等级，显示低于特定值的百分比。如：显示web服务加载时间的占比 Percentiles：百分位的值，显示出现百分位观察值的点，percents指定返回的百分位。如：显示大于观察值95%的值 Scripted metric：使用脚本执行获取指标 Value count：去重计数 Weighted avg：带权重的平均值 Pipeline Avg bucket：【sibling pipeline aggs】，计算平均值 max bucket：【sibling pipeline aggs】，计算最大值 min bucket：【sibling pipeline aggs】，计算最小值 sum bucket：【sibling pipeline aggs】，计算总和 bucket script：【parent pipeline aggs】，脚本计算 bucket selector：【parent pipeline aggs】，桶过滤 bucket sort：【parent pipeline aggs】，桶排序 cumulative cardinality：累积基数，此值显示自查询时间段开始以来总的计数，也可显示增量的计数。如：每天网站的新访问者新增数量。 cumulative sum：累积总和，此值显示自查询时间段开始以来累积总和。如：月销售额的累积总和。 derivative：柱状图导数计算 stats bucket：【sibling pipeline aggs】，统计信息，包括min, max, sum, count and avg extended stats bucket：【sibling pipeline aggs】扩展的统计信息，包括平方和、标准差等 inference bucket：【parent pipeline aggs】，训练模型推断 moving average：滑动窗口平均值 moving function：滑动窗口上自定义函数 moving percentiles：基于百分位的滑动窗口 normalize：计算标准的数学值 percentiles bucket：计算桶的百分位的值 serial differencing：时间序列差值 示例1：聚合、分组1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465GET /bank/_search&#123; # 仅返回聚合结果，不需要搜索结果的内容 &quot;size&quot;: 0, &quot;aggs&quot;: &#123; # 单列分组统计，sql: select sum(balance) as sum_balance,avg(balance) as avg_balance from bank group by state.keyword limit 10; &quot;group_by_state&quot;: &#123; # 定义桶的类型 &quot;terms&quot;: &#123; &quot;field&quot;: &quot;state.keyword&quot; &#125;, # 添加自定义Mata信息 &quot;meta&quot;: &#123; &quot;my-metadata-field&quot;: &quot;foo&quot; &#125; # 子聚合 &quot;aggs&quot;: &#123; &quot;avg_balance&quot;: &#123; &quot;avg&quot;: &#123; &quot;field&quot;: &quot;balance&quot; &#125; &#125;, &quot;sum_balance&quot;: &#123; &quot;sum&quot;: &#123; &quot;field&quot;: &quot;balance&quot; &#125; &#125; &#125; &#125;, # 多列分组统计 sql: select sum(balance) as sum_balance from bank group by state.keyword, gender.keyword limit 50; &quot;group_by_fields&quot;: &#123; &quot;composite&quot;:&#123; &quot;sources&quot;: [ &#123; &quot;state&quot;: &#123; &quot;terms&quot;: &#123; &quot;field&quot;: &quot;state.keyword&quot; &#125; &#125; &#125;,&#123; &quot;gender&quot;: &#123; &quot;terms&quot;: &#123; &quot;field&quot;: &quot;gender.keyword&quot; &#125; &#125; &#125; ], # 查询记录数，默认10条 &quot;size&quot;: 50 &#125;, # 子聚合 &quot;aggs&quot;: &#123; &quot;sum_balance&quot;: &#123; &quot;sum&quot;: &#123; &quot;field&quot;: &quot;balance&quot; &#125; &#125; &#125; &#125; &#125; # 聚合后置过滤器，对聚合结果无影响 &quot;post_filter&quot;: &#123; &quot;term&quot;: &#123; &quot;color&quot;: &quot;red&quot; &#125; &#125;&#125; 示例2：条件过滤，多列分组、排序、分页123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566GET /lights-order/_search&#123; &quot;size&quot;: 0, &quot;query&quot;: &#123; &quot;range&quot;: &#123; &quot;gmtCreate&quot;: &#123; &quot;gte&quot;: &quot;2021-05-06 00:00:00&quot;, &quot;lte&quot;: &quot;2021-05-06 23:59:59&quot;, &quot;format&quot;: &quot;yyyy-MM-dd HH:mm:ss&quot;, &quot;time_zone&quot;:&quot;+08:00&quot; &#125; &#125; &#125;, &quot;aggs&quot;: &#123; &quot;group_by_fields&quot;: &#123; &quot;composite&quot;:&#123; &quot;sources&quot;: [ &#123; &quot;cid&quot;: &#123; &quot;terms&quot;: &#123; &quot;field&quot;: &quot;cid&quot; &#125; &#125; &#125;,&#123; &quot;ipcc&quot;: &#123; &quot;terms&quot;: &#123; &quot;field&quot;: &quot;ipcc&quot; &#125; &#125; &#125; ], &quot;size&quot;: 50 &#125;, &quot;aggs&quot;: &#123; &quot;sum_totalAmount&quot;: &#123; &quot;sum&quot;: &#123; &quot;field&quot;: &quot;totalAmount&quot; &#125; &#125;, &quot;order_count&quot;: &#123; &quot;value_count&quot;: &#123; &quot;field&quot;: &quot;id&quot; &#125; &#125;, # 按order_count排序 &quot;sales_bucket_sort&quot;: &#123; &quot;bucket_sort&quot;: &#123; &quot;sort&quot;: [ &#123; &quot;order_count&quot;: &#123; &quot;order&quot;: &quot;asc&quot; &#125; &#125; ], &quot;from&quot;: 0, &quot;size&quot;: 10 &#125; &#125;, # 按doc_count排序 &quot;top_bucket_sort&quot;:&#123; &quot;bucket_sort&quot;: &#123; &quot;sort&quot;: [ &#123; &quot;_count&quot;: &#123; &quot;order&quot;: &quot;desc&quot; &#125; &#125; ] &#125; &#125; &#125; &#125; &#125;&#125;","categories":[{"name":"技术","slug":"技术","permalink":"http://www.lights8080.com/categories/%E6%8A%80%E6%9C%AF/"}],"tags":[{"name":"ELK","slug":"ELK","permalink":"http://www.lights8080.com/tags/ELK/"},{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"http://www.lights8080.com/tags/Elasticsearch/"}]},{"title":"语录-3","slug":"杂谈/语录-3","date":"2021-04-15T16:00:00.000Z","updated":"2021-08-03T01:47:54.000Z","comments":true,"path":"p/4a78df00.html","link":"","permalink":"http://www.lights8080.com/p/4a78df00.html","excerpt":"五只猴子的故事 任正非在一次座谈会上谈加强项目财务的有效管理 眼镜蛇效应 能源效率的诅咒","text":"五只猴子的故事 任正非在一次座谈会上谈加强项目财务的有效管理 眼镜蛇效应 能源效率的诅咒 “ 分享几个小故事，都是从“阮一峰的网络日志”中读到。” 1. 五只猴子的故事科学家在笼子里放了五只猴子。笼子中间有一架梯子，梯子上面放着香蕉。每当一只猴子爬上梯子，科学家就用冷水泼洒其余的猴子。过了一阵子，只要一只猴子爬上梯子，其他猴子就会殴打它。一段时间后，所有猴子都不敢爬上梯子。然后，科学家用一只新猴子，替换了原来的一只猴子，并且停止用冷水泼洒猴子。这只新猴子立即爬楼梯去拿香蕉，但随即遭到其他猴子的殴打。经过几次殴打，新猴子学会了不爬梯子，即使它从来不知道为什么。接着，替换了第二只猴子，也发生了同样的事情。刚才放进笼子的那只猴子，同样殴打了新来的猴子。替换了第三只猴子，也是如此。就这样，第四只、第五只猴子也接连被替换了。最终，笼子里面的五只猴子，尽管从未被泼冷水，仍然继续殴打任何试图爬上梯子的猴子。​ 2. 任正非在一次座谈会上谈加强项目财务的有效管理任正非：我在越南提出一个问题，百年一遇的台风，把爱立信的铁塔吹倒了，诺基亚的铁塔也吹倒了，就我们的铁塔没有倒，我请问你这个财务人员，如何评价？ 杜仲夏：这说明对成本管理并没有做好，这说明项目存在过度交付的问题。就像飞利浦的灯泡，只有两年寿命，用了两年刚好坏掉，这就是最好的产品。如果客户只付了两年灯泡的钱，但是我们保证10年的寿命，只能说明我们不懂经营。 任正非：但是我们当年考市场人员和财务人员的时候，每个人都充满了自豪感，你看，诺基亚和爱立信的铁塔都倒了，就我们没倒，华为的水平多高啊！华为公司的铁塔只有一个标准，在永远不会有台风的沙漠里，装的也是这种铁塔。我们僵化地制定了太高的标准，为此我们每年多浪费了10万到20万吨钢铁。所以，我们今天必须加强项目财务的有效管理，我想三五年后我们一定会看到有结果。 3. 眼镜蛇效应眼镜蛇效应一词来自殖民时期的印度：英国政府计划要减少眼镜蛇的数量，因而颁布法令说每打死一条眼镜蛇都可以领取赏金。然而印度人为了赏金反而开始养殖眼镜蛇。当英国政府意识到这种情况而取消赏金后，养殖蛇的人把蛇都放了；放出去的蛇继而大量繁殖，结果眼镜蛇族群数量不減反增。 4. 能源效率的诅咒我们为了降低能源消耗，发明了节省能源的 LED 照明。结果，更高效的照明导致了更多的照明，从而使得社会整体能源消耗增加。很多事情都是这样，为了省电，我们提高了能源效率，结果人们因此买更多的电器，消耗更多的电。","categories":[{"name":"杂谈","slug":"杂谈","permalink":"http://www.lights8080.com/categories/%E6%9D%82%E8%B0%88/"}],"tags":[{"name":"语录","slug":"语录","permalink":"http://www.lights8080.com/tags/%E8%AF%AD%E5%BD%95/"}]},{"title":"Elasticsearch-搜索（Search-DSL）","slug":"技术/ELK/Elasticsearch-搜索（Search-DSL）","date":"2021-04-07T16:00:00.000Z","updated":"2021-08-03T01:47:54.000Z","comments":true,"path":"p/12e2da86.html","link":"","permalink":"http://www.lights8080.com/p/12e2da86.html","excerpt":"Elasticsearch介绍查询搜索请求包含哪些选项，并介绍其中的Query DSL。包括语法说明、查询和过滤上下文、复合查询等和查询示例。基于7.11版本。","text":"Elasticsearch介绍查询搜索请求包含哪些选项，并介绍其中的Query DSL。包括语法说明、查询和过滤上下文、复合查询等和查询示例。基于7.11版本。 搜索请求是对Elasticsearch数据流或索引中的数据信息的请求，包括以下自定义选项： Query DSL（查询语法） Aggregations（分组聚合） Search multiple data streams and indices（多数据流和索引搜索） Paginate search results（分页查询） Retrieve selected fields（查询指定字段） Sort search results（排序） Run an async search（异步搜索） 本文介绍其中的Query DSL。 查询特定语言（Query DSL - Domain Specific Language）Elasticsearch提供了基于JSON的丰富的查询特定语言来定义查询，包含两种类型的子句组成： Leaf query clauses：页查询。在特定的字段中查找特定值，如match、term和range查询 Compound query clauses：复合查询。包装其他的Leaf和Compound子查询，逻辑组合多个查询（bool、dis_max），或更改其行为（constant_score）。 查询和过滤上下文（Query and filter context）相关性得分（relevance scores）：Elasticsearch按相关性得分对匹配的搜索结果进行排序，该得分衡量每个文档与查询的匹配程度。相关性得分是一个正浮点数，在查询API的_score元数据字段中返回，分值越高，文档越相关。不同的查询类型可以计算不同的相关性得分，计算分数还取决于查询子句是运行在查询上下文还是过滤器上下文中。 查询上下文（Query context）：回答的是文档与该查询子句的匹配程度如何，主要用于计算文档相关性得分。 过滤器上下文（Filter context）：回答的是文档与该查询子句是否匹配，主要用于过滤结构化数据，不计算相关性得分。频繁的使用filter context将会被ES自动缓存，以提升性能。 Compound queries：复合查询 boolean：匹配和过滤，满足条件可以获得更高的得分 boosting：降低文档的得分，而不是排除 constant_score：固定值得分 dis_max：提升多个文档具有相同固定值得分 function_score：根据算法修改查询文档得分 1. boolean query用于匹配和筛选文档。bool查询是采用more-matches-is-better的机制，因此满足must和should子句的文档将获得更高的分值。 must：返回的文档必须满足此查询子句，参与分值计算 filter：返回的文档必须满足此查询子句，不参与分值计算，缓存结果 should：返回的文档可能满足此查询子句，参与分值计算 must_not：该查询子句必须不能出现在匹配的文档中，不参与分值计算，缓存结果 minimum_should_match：指定至少匹配几个should子句，若一个bool查询包含至少一个should子句且无must或filter子句，则默认值为1。 boost：提升权重 2. boosting目的是降低某些文档分值，而不是从结果中排除。 boosting计算相关性得分规则： 从符合positive子句的查询中获得原始的相关性得分 得分 乘上 negative_boost系数，获得最终得分 positive：必填，返回的文档必须匹配此查询。 negative：必填，降低匹配文档的分值。 negative_boost：必填，介于0~1.0之间的数 3. constant_score包装filter query并返回分值，分值等于boost参数。 filter：必填，返回索引文档都必须匹配的查询条件 boost：可选，指定分值，默认为1 4. dis_max用于提升多个字段中包含相同术语的文档分配更高的得分。dis_max计算相关性得分规则： 获得匹配子句中最高得分 其他匹配的子句得分 乘上 tie_treaker系数 将最高得分与其他匹配得分相加，获得最终得分 queries：必填，返回的文档必须匹配一个或多个查询条件，匹配的条件越多则分值越高 tie_breaker：可选，介于0~1.0之间的数，用于增加匹配文档的分值。默认为0 5. function_score允许修改查询文档的相关性得分，通过得分函数（function_score）在过滤后的文档集合上计算，获得最终得分。 query：指定查询条件，默认”match_all”: {} score_mode：计算分值的模式。multiply（默认）、sum、avg、first、max、min boost_mode：计算的分值与查询分值合并模式。multiply（默认）、replace（忽略查询分值）、sum、avg、max、min function_score：计算分值的函数。script_score（函数）、weight（权重）、random_score（0~1随机）、field_value_factor（字段因素） Full text queries：全文检索，查询已分析的文本字段 intervals：根据匹配项的顺序和接近程度返回文档 match：标准的全文查询，模糊匹配和短语接近查询 match_bool_prefix：分析其输入解析构造为bool query，最后一个词在前缀查询中使用 match_phrase：分析其输入解析为短语匹配查询 match_phrase_prefix：分析其输入解析为短语匹配查询，最后一个词在前缀查询中使用 multi_match：多个字段匹配查询 query_string：语法解析器查询 simple_query_string：更简单的语法解析器查询 Geo queries：坐标查询 geo_bounding_box：矩形查询 geo_distance：坐标点范围查询 geo_polygon：多边形查询 geo_shape：包括几何图形查询和指定地理形状相交点查询 Shape queries：像geo_shape一样，支持索引任意二维的几何图形功能Joining queries：连接查询 nested：嵌套类型查询 has_child：匹配子文档的字段，返回父文档。前提是同一索引中建立的父子关系 has_parent：匹配父文档的字段，返回所有子文档。前提是同一索引中建立的父子关系 parent_id：查询指定父文档的所有子文档。 Span queries：区间查询。精准控制多个输入词的先后顺序，已经多个关键词在文档中的前后距离 span_containing：区间列表查询 field_masking_span：允许跨越不同字段查询 span_first：跨度查询，匹配项必须出现在该字段的前N个位置 span_multi：Wraps a term, range, prefix, wildcard, regexp, or fuzzy query. span_near：接受多个跨度查询，顺序相同且指定距离之内 span_not：包装其他span query，排除与该文档匹配的所有文档 span_or：返回任意指定查询匹配的文档 span_term：等同于term query，可以和其他span query一起使用 span_within： Specialized queries：专业的查询 distance_feature：基于时间或坐标查询，越接近原点得分越高 more_like_this：按文本、文档和文档的集合查询 percolate：按存储的指定文档匹配查询 rank_feature：通过定义字段的rank_feature或rank_features属性值提高得分 script：脚本过滤文档 script_score：通过脚本自定义得分 wrapper：接受一个base64字符串查询 pinned：提升特定文档的查询 Term-level queries：术语级查询 exists：返回包含字段的任意文档 fuzzy：返回搜索词的相似词的文档 ids：返回指定ID的文档 prefix：返回字段中指定前缀的文档 range：返回范围内的文档 regexp：返回正则匹配的文档 term：返回字段中包含特定术语的文档 terms：返回字段中包含一个或多个术语的文档 terms_set：返回字段中包含最少数目术语的文档 type：返回指定类型的文档 wildcard：返回通配符匹配文档 查询示例1234567891011121314151617181920212223242526272829303132333435363738394041# 多索引同步搜索GET /my-index-000001,my-index-000002/_search&#123; # 指定查询超时时间 &quot;timeout&quot;: &quot;2s&quot;, # 字段匹配，相当于query context &quot;query&quot;: &#123; &quot;bool&quot;: &#123; &quot;must&quot;: [ &#123; &quot;match_all&quot;: &#123;&#125;&#125;, &#123; &quot;match&quot;: &#123; &quot;title&quot;: &quot;Search&quot; &#125;&#125; ], # 相当于filter context &quot;filter&quot;: [ &#123; &quot;term&quot;: &#123; &quot;status&quot;: &quot;published&quot; , &quot;_name&quot; : &quot;status_pub&quot;&#125;&#125;, &#123; &quot;range&quot;: &#123; &quot;publish_date&quot;: &#123; &quot;gte&quot;: &quot;2015-01-01&quot; &#125;&#125;&#125; ] &#125; &#125;, # 排序 &quot;sort&quot;: [ &#123; &quot;account_number&quot;: &quot;asc&quot; &#125;, &#123; &quot;post_date&quot; : &#123;&quot;order&quot; : &quot;asc&quot;&#125;&#125;, &quot;_score&quot; ], # 范围搜索 &quot;range&quot;: &#123; &quot;gmtCreate&quot;: &#123; &quot;gte&quot;: &quot;2020-10-01 00:00:00&quot;, &quot;lte&quot;: &quot;2020-10-31 23:59:59&quot;, &quot;format&quot;: &quot;yyyy-MM-dd HH:mm:ss&quot;, &quot;time_zone&quot;:&quot;+08:00&quot; &#125; &#125; # 查询指定字段 &quot;fields&quot;: [&quot;user.id&quot;, &quot;@timestamp&quot;], &quot;_source&quot;: false # 分页 &quot;from&quot;: 10, &quot;size&quot;: 10&#125;","categories":[{"name":"技术","slug":"技术","permalink":"http://www.lights8080.com/categories/%E6%8A%80%E6%9C%AF/"}],"tags":[{"name":"ELK","slug":"ELK","permalink":"http://www.lights8080.com/tags/ELK/"},{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"http://www.lights8080.com/tags/Elasticsearch/"}]},{"title":"Elasticsearch-索引（Index）","slug":"技术/ELK/Elasticsearch-索引（Index）","date":"2021-04-06T16:00:00.000Z","updated":"2021-08-03T01:47:54.000Z","comments":true,"path":"p/cae9608.html","link":"","permalink":"http://www.lights8080.com/p/cae9608.html","excerpt":"Elasticsearch索引介绍，包括索引设置、索引模板、索引生命周期管理、翻滚索引、索引别名、滚动索引。 基于7.11版本。","text":"Elasticsearch索引介绍，包括索引设置、索引模板、索引生命周期管理、翻滚索引、索引别名、滚动索引。 基于7.11版本。 索引设置（Index Settings）static只能在创建索引时或关闭的索引上设置 index.number_of_shards：主分片数量，默认1 index.number_of_routing_shards：拆分索引的路由分片数量，默认值位于2~1024之间，依赖索引主分片数量 index.shard.check_on_startup：打开前检查分片是否损坏，默认false index.codec：压缩存储算法，默认LZ4 index.routing_partition_size：自定义路由可以到达的分片数量，默认1 dynamic可以使用API实时对索引进行操作 index.number_of_replicas：主分片的副本数，默认1 index.auto_expand_replicas：根据集群中数据节点的数量自动扩展副本的数量，默认false index.search.idle.after：搜索空闲之前不能接收搜索和获取请求的时间，默认30s index.refresh_interval：刷新操作频率，最近对索引的更改既可见，默认1s。-1关闭刷新操作 index.max_result_window：查询索引结果的最大数量，默认10000 index.max_inner_result_window：内部或聚合命中最大数量，默认100 index.max_rescore_window：打分请求的最大索引数量，默认10000（同index.max_result_window） index.max_docvalue_fields_search：查询中允许的最大字段数，默认100 index.max_script_fields：查询中允许的最大脚本字段数，默认32 index.query.default_field：查询返回的默认字段，默认*（表示所有） 索引模板（Index Templates）索引模板是告诉Elasticsearch在创建索引时如何配置索引的一种方法。对于数据流（data stream），索引模板配置是创建他们的后备索引。在创建索引之前先配置模板，模板设置将用作创建索引的基础。 模板有两种类型，索引模板（index templates）和组件模板（component templates）。 组件模板是可重用的构建块，用于配置映射（mappings）、设置（settings）和别名（alias）。使用组件模板来构造索引模板，但它们不会直接应用于索引。索引模板可以包含组件模板的集合，也可以直接指定设置，映射和别名。如果匹配多个模板，优先使用优先级最高的模板。 可以使用模拟API创建索引，确定最终的索引设置。POST /_index_template/_simulate。 注意事项： 如果新数据流或索引与多个索引模板匹配，则使用优先级最高的索引模板。 Elasticsearch内置了许多索引模板（如：metrics--,logs-*-*），每个模板的优先级是100。如果不想使用内置模板，请为您的模板分配更高的优先级。 如果显式设置创建索引，并且该索引与索引模板匹配，则创建索引请求中的设置将优先于索引模板中指定的设置。 索引模板仅在创建索引期间应用。索引模板的更改不会影响现有索引。 当可组合模板匹配给定索引时，它始终优先于旧模板。如果没有可组合模板匹配，则旧版模板可能仍匹配并被应用。 示例12345678910111213141516171819202122232425262728293031323334353637PUT _template/datastream_template&#123; # 1.1 匹配所有&quot;datastream-&quot;开头的索引 &quot;index_patterns&quot;: [&quot;datastream-*&quot;], # 2 指定创建数据流索引模板 &quot;data_stream&quot;: &#123;&#125;, &quot;settings&quot;: &#123; &quot;number_of_shards&quot;: 1, &quot;number_of_replicas&quot;: 0, &quot;refresh_interval&quot;: &quot;15s&quot;, # 指定索引管理策略，索引关联策略 &quot;index.lifecycle.name&quot;: &quot;datastream_policy&quot;, # 指定滚动写别名 &quot;index.lifecycle.rollover_alias&quot;: &quot;datastream&quot;, # 满足策略的索引检查频率 &quot;indices.lifecycle.poll_interval&quot;: &quot;10m&quot;, # 跳过滚动 &quot;index.lifecycle.indexing_complete&quot;: true &#125;, &quot;mappings&quot;: &#123; &quot;dynamic_date_formats&quot;: [ &quot;yyyy-MM-dd HH:mm:ss&quot;, &quot;yyyy-MM-dd HH:mm:ss Z&quot;, &quot;yyyy-MM-dd HH:mm:ss.SSS&quot;, &quot;yyyy-MM-dd HH:mm:ss.SSS Z&quot; ], &quot;_default_&quot;: &#123; &quot;_all&quot;: &#123; &quot;enabled&quot;: false &#125; &#125; &#125;, # 1.2 非数据流索引是使用 &quot;aliases&quot;: &#123; &quot;last_3_months&quot;: &#123;&#125; &#125;&#125; 索引生命周期管理（Index Lifecycle Manager - ILM）配置索引生命周期管理策略，能够随着时间推移根据性能、弹性和保留要求自动的管理索引。 索引生命周期策略可以触发以下操作： 翻转（Rollover）：当现有索引达到一定分片大小，文档数或使用年限时，为翻转目标创建新索引。翻转目标可以是索引别名或数据流。 收缩（Shrink）：减少索引中主碎片的数量。 强制合并（Force merge）：手动触发合并以减少索引每个分片中的段数，并释放已删除文档所使用的空间。 冻结（Freeze）：将索引设为只读，并最大程度地减少其内存占用量。 删除（Delete）：永久删除索引，包括其所有数据和元数据。 使用ILM可以更轻松地管理热-温-冷体系结构中的索引，在使用时间序列数据时很常见（如日志和指标）。 索引生命周期（Index lifecycle）ILM定义了以下四个阶段（Phases） Hot：频繁的写入和查询 Warm：索引不在更新，仍然在查询 Cold：不再更新的索引，很少查询仍然可以搜索，查询较慢也没关系 Delete：不再需要的索引，可以安全的删除 索引的生命周期策略指定了应用于哪些阶段，每个阶段中执行什么操作，以及何时在两个阶段之间进行转换。 创建索引时可以手动应用生命周期策略。对于时间序列索引，需要将生命周期策略与用于在序列中创建新索引的索引模板相关联。当索引滚动时，不会自动将手动应用的策略应用于新索引。 阶段转换（phase transitions）ILM根据其年龄在整个生命周期中移动索引。要控制这些翻转的时间，请为每个阶段设置一个最小年龄。为了使索引移至下一阶段，当前阶段中的所有操作都必须完成，并且索引必须早于下一阶段的最小年龄。 最小年龄默认为0，这会导致ILM在当前阶段中的所有操作完成后立即将索引移至下一阶段。 如果索引具有未分配的分片并且集群运行状况为黄色，则索引仍可以根据其索引生命周期管理策略过渡到下一阶段。但是，由于Elasticsearch只能在绿色集群上执行某些清理任务，因此可能会有意外的副作用。 阶段执行（phase execution）ILM控制阶段中的动作的执行的顺序，以及哪些步骤是执行每个动作的必要索引操作。 当索引进入阶段后，ILM将阶段定义信息缓存在索引元数据中，这样可以确保索引政策更新不会将索引置于永远不退出阶段的状态。 ILM定期运行，检查索引是否符合策略标准，并执行所需的步骤。为了避免竞争情况，ILM可能需要运行多次执行，完成一项动作所需的所有步骤。这意味着即使indexs.lifecycle.poll_interval设置为10分钟并且索引满足翻转条件，也可能需要20分钟才能完成翻转。 阶段动作（phase actions）参考https://www.elastic.co/guide/en/elasticsearch/reference/7.11/ilm-index-lifecycle.html#ilm-phase-actions 索引生命周期动作（Index Lifecycle Actions） Allocate：将分片移动到具有不同性能特征的节点，并减少副本的数量。 Delete：永久删除索引。 Force merge：减少索引段的数量并清除已删除的文档。将索引设为只读。 Freeze：冻结索引以最大程度地减少其内存占用量。 Migrate：将索引分片移动到对应于当前 ILM 阶段的数据层。 Read only：阻止对索引的写操作。 Rollover：移动索引作为滚动别名的写索引，并开始索引到新索引。 Searchable snapshot：为配置库中的管理索引拍摄快照，并将其作为可搜索快照。 Set priority：降低索引在生命周期中的优先级，以确保首先恢复热索引。 Shrink：通过将索引缩小为新索引来减少主碎片的数量。 Unfollow：将关注者索引转换为常规索引。在Rollover、Shrink和Searchable snapshot操作之前自动执行。 Wait for snapshot：删除索引之前，请确保快照已存在。 ILM更新（Lifecycle policy updates）您可以通过修改当前策略或切换到其他策略的方式来更改管理索引或滚动索引集合的生命周期。 为确保策略更新不会将索引置于无法退出当前阶段的状态，进入这个阶段时，阶段定义会缓存在索引元数据中。如果策略更新可以安全的应用，ILM更新缓冲的阶段定义；如果不能，则使用缓冲阶段定义完成该阶段。 Rollover（翻转）在为日志或指标等时间序列数据编制索引时，不能无限期地写入单个索引。为了满足索引和搜索性能要求并管理资源使用，可以写入索引直到达到某个阈值，然后创建一个新索引并开始写入该索引。 使用滚动索引能够： 优化活跃的索引，以在高性能热节点上获得高接收速率。 针对热节点上的搜索性能进行优化。 将较旧的，访问频率较低的数据转移到价格较低的冷节点上。 根据您的保留政策，通过删除整个索引来删除数据。 我们建议使用数据流来管理时间序列数据。数据流自动跟踪写入索引，同时将配置保持在最低水平。数据流设计用于仅追加数据，其中数据流名称可用作操作（读取，写入，翻转，收缩等）目标。如果您的用例需要就地更新数据，则可以使用索引别名来管理时间序列数据。 自动翻转（automatic rollover）：ILM使您能够根据索引大小，文档数或使用年限自动翻转到新索引。触发翻转后，将创建一个新索引，将写入别名更新为指向新索引，并将所有后续更新写入新索引。与基于时间的过渡相比，基于大小，文档数或使用年限翻转至新索引更可取。在任意时间滚动通常会导致许多小的索引，这可能会对性能和资源使用产生负面影响。 示例123456789101112131415161718192021222324252627282930313233343536373839404142434445464748# 查看索引所处哪个阶段、应用策略等GET datastream-*/_ilm/explain# 创建索引管理策略PUT _ilm/policy/full_policy&#123; &quot;policy&quot;: &#123; &quot;phases&quot;: &#123; &quot;hot&quot;: &#123; &quot;actions&quot;: &#123; &quot;rollover&quot;: &#123; &quot;max_age&quot;: &quot;7d&quot;, &quot;max_size&quot;: &quot;50G&quot; &#125; &#125; &#125;, &quot;warm&quot;: &#123; &quot;min_age&quot;: &quot;30d&quot;, &quot;actions&quot;: &#123; &quot;forcemerge&quot;: &#123; &quot;max_num_segments&quot;: 1 &#125;, &quot;shrink&quot;: &#123; &quot;number_of_shards&quot;: 1 &#125;, &quot;allocate&quot;: &#123; &quot;number_of_replicas&quot;: 2 &#125; &#125; &#125;, &quot;cold&quot;: &#123; &quot;min_age&quot;: &quot;60d&quot;, &quot;actions&quot;: &#123; &quot;allocate&quot;: &#123; &quot;require&quot;: &#123; &quot;box_type&quot;: &quot;cold&quot; &#125; &#125; &#125; &#125;, &quot;delete&quot;: &#123; &quot;min_age&quot;: &quot;90d&quot;, &quot;actions&quot;: &#123; &quot;delete&quot;: &#123;&#125; &#125; &#125; &#125; &#125;&#125; 数据流（Data streams）数据流用于跨多个索引存储仅追加的时间序列数据，同时提供一个用于请求的数据流名称。可以将索引和搜索请求直接提交到数据流。流自动将请求路由到存储流数据的索引。同样可以使用索引生命周期管理（ILM）来自动管理这些后备索引。数据流非常适合日志，事件，指标和其他连续生成的数据。 索引别名（index alias）Request body actions：必填，要执行的一组动作 add：添加一个索引别名 remove：删除一个索引别名 remove_index：删除索引 actions on alias objects： index：指定索引名称，允许逗号分隔或通配符 alias：指定别名名称，允许逗号分隔或通配符 filter：使用别名查询时，限制条件 is_write_index：标记作为别名的写索引，一个别名同时只能有一个写索引 routing：指定路由到特定分片 search_routing：搜索路由 index_routing：索引路由 示例12345678910111213141516171819202122232425262728293031PUT /&lt;index&gt;/_alias/&lt;alias&gt;DELETE /&lt;index&gt;/_alias/&lt;alias&gt;GET /_aliasGET /_alias/&lt;alias&gt;GET /&lt;index&gt;/_alias/&lt;alias&gt;POST /_aliases&#123; &quot;actions&quot;: [ &#123; &quot;remove&quot;: &#123; &quot;index&quot;: &quot;test1&quot;, &quot;alias&quot;: &quot;alias1&quot; &#125; &#125;, &#123; &quot;add&quot;: &#123; &quot;index&quot;: &quot;test&quot;, &quot;alias&quot;: &quot;alias1&quot;, &quot;is_write_index&quot;: false &#125; &#125;, &#123; &quot;add&quot;: &#123; &quot;index&quot;: &quot;test2&quot;, &quot;alias&quot;: &quot;alias1&quot;, &quot;is_write_index&quot;: true &#125; &#125; ]&#125; 滚动索引（rollover index）当现有的索引满足您提供的条件（a list of conditions）时，滚动索引API会为滚动目标（rollover target）创建一个新的索引。当滚动目标是别名（alias）时，别名会指向新索引（当指向多个索引时，必须有一个索引设置is_write_index=true）当滚动目标是数据流（data stream）时，新索引会成为数据流的写索引，并生成一个增量 Rollover request12POST /&lt;rollover-target&gt;/_rollover/&lt;target-index&gt;POST /&lt;rollover-target&gt;/_rollover/ 滚动索引接受一个滚动目标（rollover target）和一个条件列表（a list of conditions）。可以使用API撤销太大或太旧的索引。 当满足滚动条件，滚动请求在不同的场景下，滚动操作有所不同： 如果滚动目标是别名指向单个索引时： 创建新索引 别名指向新索引 原始索引中移除别名 如果滚动目标是别名指向多个索引时，必须有一个索引设置is_write_index=true： 创建新索引 设置新索引is_write_index=true 设置原始索引is_write_index=false 如果滚动目标是数据流： 创建新索引 在数据流上添加新索引作为支持索引和写索引 增加数据流的generation属性 Path parameters &lt;rollover-target&gt;：必填，现有的分配给目标索引的索引别名或数据流名称。 &lt;target-index&gt;：可选，用于创建和分配索引别名的目标索引名称。如果&lt;rollover-target&gt;是数据流，则不允许使用此参数。如果&lt;rollover-target&gt;是索引别名，则分配给以”-“和数字结尾的索引名称，如logs-000001。 示例1234567891011121314151617PUT /logs-000001&#123; &quot;aliases&quot;: &#123; &quot;logs_write&quot;: &#123;&#125; &#125;&#125;# Add &gt; 1000 documents to logs-000001POST /logs_write/_rollover&#123; &quot;conditions&quot;: &#123; &quot;max_age&quot;: &quot;7d&quot;, &quot;max_docs&quot;: 1000, &quot;max_size&quot;: &quot;5gb&quot; &#125;&#125;","categories":[{"name":"技术","slug":"技术","permalink":"http://www.lights8080.com/categories/%E6%8A%80%E6%9C%AF/"}],"tags":[{"name":"ELK","slug":"ELK","permalink":"http://www.lights8080.com/tags/ELK/"},{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"http://www.lights8080.com/tags/Elasticsearch/"}]},{"title":"Elasticsearch-介绍","slug":"技术/ELK/Elasticsearch-介绍","date":"2021-03-31T16:00:00.000Z","updated":"2021-08-03T01:47:54.000Z","comments":true,"path":"p/9e5edb25.html","link":"","permalink":"http://www.lights8080.com/p/9e5edb25.html","excerpt":"Elasticsearch介绍，包括文档与索引、倒排索引、搜索和分析、可伸缩和弹性（节点、分片、跨集群复制）、常用场景。 内容大部分源自官方文档第一节“What is Elasticsearch?” 基于7.11版本。","text":"Elasticsearch介绍，包括文档与索引、倒排索引、搜索和分析、可伸缩和弹性（节点、分片、跨集群复制）、常用场景。 内容大部分源自官方文档第一节“What is Elasticsearch?” 基于7.11版本。 Elasticsearch是一个分布式搜索和分析引擎，为所有类型的数据提供了近实时的搜索和分析。不仅可以进行简单的数据探索，还可以汇总信息来发现数据中的趋势和模式。随着数据和查询量的增长，分布式特性可以使部署顺畅的无缝的增长。 1. 文档和索引文档（Document）Elasticsearch是分布式文档存储，它不会将信息存储为“行-列”数据结构，而是存储为JSON文档的数据结构。当一个集群中有多个节点时，存储的文档分布在集群中，并且可以从任何节点访问。 Elasticsearch使用倒排索引的数据结构，支持非常快速的全文本搜索。一个倒排索引由文档中所有不重复词的列表构成，对于其中每个词，有一个包含它的文档列表。 索引（Index）索引是一个逻辑命名空间，可以看作是文档的优化集合，每个文档都是字段的集合。默认Elasticsearch对每个字段中的所有数据建立索引，并且每个索引字段都具有专用的优化数据结构。如：文本字段存储在倒排索引中，数字、地理字段存储在BKD树中。 Elasticsearch还支持动态映射（dynamic mapping），自动检测并向索引添加新字段。可以定义规则来控制动态映射，也可以显式定义映射以完全控制字段的存储和索引方式。 显式定义映射的意义： 区分全文本字符串字段和精确值字符串字段 执行特定语言的文本分析 优化字段进行部分匹配 自定义日期格式 无法自动检测到的数据类型，如地理信息 为不同的目的以不同的方式对同一字段建立索引通常很有用。 倒排索引（inverted index）倒排索引的结构，适用于快速的全文（Text）搜索。一个倒排索引由文档中所有不重复词的列表构成，对于其中每个词，有一个包含它的文档列表。 倒排索引建立的是分词（Term）和文档（Document）之间的映射关系，在倒排索引中，数据是面向词而不是面向文档的。 Term（词）：精准值，foo、Foo是不相同的词 Text（文本）：非结构化文本，默认文本会被解析为词，这是索引中实际存储的内容 Elasticsearch索引存储结构图： 一个索引包含很多分片，一个分片是一个Lucene索引，它本身就是一个完整的搜索引擎，可以独立执行建立索引和搜索任务。 Lucene索引又由很多分段组成，每个分段都是一个倒排索引。 Elasticsearch每次refresh都会生成一个新的分段，其中包含若干文档的数据。 每个分段（Segment）内部，文档的不同字段被单独建立索引。每个字段的值由若干词（Term）组成。 词（Term）是原文本内容经过分词器处理和语言处理后的最终结果。 2. 搜索和分析搜索（Search）Elasticsearch提供了一个简单，一致的REST API，用于管理集群以及建立索引和搜索数据。REST APIs支持结构化查询、全文本查询和结合了两者的复杂查询。 结构化查询：类似于SQL构建的查询，按索引中搜索字段，然后按字段对匹配项进行排序。 全文本查询：查找到所有与查询字符串匹配的文档，按相关性对它们进行排序。 可以使用Elasticsearch的全面JSON风格的查询语言(Query DSL)访问所有这些搜索功能。还可以构造SQL风格的查询来在Elasticsearch内部本地搜索和聚合数据。 分析（Analyze）聚合使您能够构建数据的复杂摘要，并深入了解关键指标，模式和趋势。聚合利用了用于搜索的相同数据结构，速度很快，可以实时分析和可视化数据。聚合操作和搜索的请求在一起运行，可以在单个请求中同一时间相同的数据进行搜索文档，过滤结果并执行分析。 3. 可伸缩和弹性集群（Cluster）一个Elasticsearch集群由一个或多个节点（Node）组成，每个集群都有一个cluster name作为标识。 集群的三种状态： Green：所有主分片和副本分片都准备就绪。数据不会丢失 Yellow：所有主分片准备就绪，但至少一个主分片对应的副本分片没有就绪。意味着高可用和容灾能力下降 Red：至少有一个主分片没有就绪。此时查询的结果会出现数据丢失 Elasticsearch可以根据需要进行扩展，它天生就实现了分布式。你可以向集群添加节点以增加容量，它会自动将数据和查询负载分布到所有可用节点，不需要大改应用程序。 Elasticsearch索引只是一个或多个物理分片的逻辑分组，其中每个分片实际上是一个独立的的索引。通过将文档分布在索引中的多个分片上，并将这些分片分布在多个节点上，当集群增长(或缩小)时，Elasticsearch自动迁移分片以平衡集群。 节点（Node）节点是属于集群的运行实例，测试时可以一个服务器上启动多个节点，线上通常一个服务器只有一个实例。 启动时，节点将使用单播来发现具有相同集群名称的现有集群，并尝试加入。 候选主节点（Master-eligible Node） 主节点（Master Node） 数据节点（Data Node） 协调节点（Coordinating Node） 热节点（Hot Node） 冷节点（Warm Node） 预处理节点（Ingest Node） 分片（Shard）分片是单个Lucene实例。这是一个低级的工作单元，由ElasticSearch自动管理，在节点发生故障或新增时，可以自动的将分片从一个节点移动到另一个节点。分为主分片（primaries）和副本分片（replicas）两种类型。 主分片：建立索引时默认会有一个主分片，每个文档都属于一个主分片，创建索引后，主分片数量是固定的无法更改。 副本分片：每个主分片可以有零个或多个副本，可以随时更改，副本永远不会和主分片在同一节点上启动。副本提供数据的冗余副本，以防止硬件故障，并增加处理像搜索或检索文档这样的读请求的能力。 跨集群复制（Cross-cluster replication）出于性能原因，群集中的节点必须位于同一网络上，跨不同数据中心中的节点在群集中平衡分片的时间太长了。但是这不符合高可用架构的要求，跨群集复制(CCR)可以解决单个集群重大故障问题。 CCR提供了一种将主集群的索引自动同步到次要远程集群的方法，次要远程集群可以作为热备份。如果主集群失败，次要集群可以接管。您还可以使用CCR创建次要集群，以满足用户在地理位置上接近的读请求。主集群上的索引是Leader，负责所有写请求，复制到次要集群上的索引是只读的Follower。 4. 常用场景 根据关键字查询日志详情 监控系统的运行状况 统计分析，比如接口的调用次数、执行时间、成功率等 异常数据自动触发消息通知 基于日志的数据挖掘 参考看完这篇还不会Elasticsearch，我跪搓衣板！一篇文章带你搞定 ElasticSearch 术语","categories":[{"name":"技术","slug":"技术","permalink":"http://www.lights8080.com/categories/%E6%8A%80%E6%9C%AF/"}],"tags":[{"name":"ELK","slug":"ELK","permalink":"http://www.lights8080.com/tags/ELK/"},{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"http://www.lights8080.com/tags/Elasticsearch/"}]},{"title":"Elasticsearch-文本分析（Text Analysis）","slug":"技术/ELK/Elasticsearch-文本分析（Text Analysis）","date":"2021-03-31T16:00:00.000Z","updated":"2021-08-03T01:47:54.000Z","comments":true,"path":"p/ffcd0ce9.html","link":"","permalink":"http://www.lights8080.com/p/ffcd0ce9.html","excerpt":"","text":"文本分析使Elasticsearch能够执行全文搜索，其中搜索返回所有相关结果，而不仅仅是精确匹配。文本通过标记化（tokenization）使全文搜索成为可能，将文本分解为标记的更小块。在大多数情况下，这些标记是单个单词。 概念分析器（无论是内置的还是自定义的）只是一个包，其中包含三个较低级别的构建块：字符过滤器（character filters），标记生成器（tokenizers）和标记过滤器（token filters）。 索引和搜索分析器：文本分析发生在两次时间，索引时间（index time）和搜索时间（search time）。大多数情况，应在索引和搜索时使用同一台分析器，这样可以确保将字段的值和查询字符串更改为相同形式的标记。 词干化（Stemming）：词干化是将单词还原为词根形式的过程。这样可以确保在搜索过程中单词匹配的变体。如walking和walked的词根是walk。 标记图（Token graphs）：标记生成器将文本转换为标记流时，还会标记位置（position）和标记跨越的位置数（positionLength）。使用这些，可以为流创建有向无环图，称为标记图。 内置解析器 standard analyzer：标准分析器；按照Unicode编码算法，将文本按照单词边界划分为terms，转为小写，支持删除停用词。 simple analyzer：简易分析器；遇到非字母字时，将文本划分terms，转为小写，支持删除停用词。 whitespace analyzer：空格分析器；遇到任意空格是，将文本划分为terms，不会小写。 stop analyzer：同simple analyzer keyword analyzer：无操作解析器；会将文本作为单个term输出。 pattern analyzer：正则表达式解析器；按照正则表达式，将文本分成多个term，支持小写字母和停用词。 language analyzer：特定语言分析器；如：english custom analyzer：自定义解析器","categories":[{"name":"技术","slug":"技术","permalink":"http://www.lights8080.com/categories/%E6%8A%80%E6%9C%AF/"}],"tags":[{"name":"ELK","slug":"ELK","permalink":"http://www.lights8080.com/tags/ELK/"},{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"http://www.lights8080.com/tags/Elasticsearch/"}]},{"title":"Elasticsearch-映射（Mapping）","slug":"技术/ELK/Elasticsearch-映射（Mapping）","date":"2021-03-31T16:00:00.000Z","updated":"2021-08-03T01:47:54.000Z","comments":true,"path":"p/5217ff46.html","link":"","permalink":"http://www.lights8080.com/p/5217ff46.html","excerpt":"Elasticsearch映射介绍，包括动态映射、显式映射、字段数据类型、映射参数、映射限制设置。 内容大纲源自官方文档“Mapping”模块 基于7.11版本。","text":"Elasticsearch映射介绍，包括动态映射、显式映射、字段数据类型、映射参数、映射限制设置。 内容大纲源自官方文档“Mapping”模块 基于7.11版本。 映射（Mapping）是定义文档及其包含的字段如何存储和索引的过程。每个文档都是字段的集合，每个字段都有自己的数据类型。为数据创建一个映射定义，包含与文档相关的字段列表。 动态映射（Dynamic mapping）当Elasticsearch在文档中检测到新字段时，会动态将该字段添加到映射中称为动态映射。 动态字段映射：根据数据类型规则应用于动态添加的字段。支持的数据类型包括：boolean、double、integer、object、array、date。 dynamic：开启动态映射的模式 date_detection：开启日期检测 dynamic_date_formats：检测的日期格式 numeric_detection: true：开启数值检测 动态模板：又称自定义映射，根据匹配条件应用于动态添加的字段。 match_mapping_type: Elasticsearch检测到的数据类型进行操作 match and unmatch: 使用模式来匹配字段名称 path_match and path_unmatch: 使用字段的路径来匹配 显式映射（Explicit mapping）显式映射以完全控制字段的存储和索引方式。 显式映射的意义： 区分全文本字符串字段和精确值字符串字段 执行特定语言的文本分析 优化字段进行部分匹配 自定义日期格式 无法自动检测到的数据类型，如地理信息 字段数据类型（Field data types）常见的类型： binary：二进制编码为Base64字符串 boolean：布尔值 Keywords：关键字，通常用于过滤、排序和聚合。包括keyword、constant_keyword和wildcard。 Numbers：数值类型，包括long、integer、short、byte、double、float Dates：日期类型，包括date和date_nanos alias：为现有字段定义别名 对象和关系类型： object：JSON对象。扁平的键-值对列表 flattened：将整个JSON对象作为单个字段值 nested：保留其子字段之间关系的JSON对象，维护每个对象的独立性 join：为同一索引中的文档定义父/子关系 结构化数据类型： Range：范围，包括long_range, double_range, date_range, and ip_range ip：IPv4和IPv6地址 version：软件版本。特殊的关键字，用于处理软件版本值并支持它们的专用优先级规则 murmur3：计算和存储值的散列。提供了在索引时计算字段值的哈希并将其存储在索引中的功能 聚合数据类型： aggregate_metric_double：度量值进行聚合 histogram：柱状图，以直方图形式聚合数值 文本搜索类型： text：非结构化文本。配置分词器 annotated_text：注解文本。带有映射器注释的文本插件提供了索引文本的功能。如WWW与World Wide Web同义 completion：补全提示。是一个导航功能，引导用户在输入时查看相关结果，提高搜索精度 search_as_you_type：自定义搜索。类似文本的字段，经过优化提供开箱即用的按需输入搜索服务。如搜索框 token_count：符号计数。分词分析后对数量进行索引 文档排名类型： rank_feature：记录一个数字特性，以便在查询中增强文档。 空间数据类型： geo_point：经纬度 geo_shape：复杂的形状，如多边形 元数据： _index：文档所属的索引 _id：文档ID _doc_count：桶聚合（Bucket）返回字段，显示桶中已聚合和分区的文档数 _source：原始JSON文档 _size：_source字段的大小 _routing：自定义路由 _meta：元数据信息 映射参数（Mapping parameters） analyzer：text字段文本解析器 boots：增强匹配权重，默认1.0。如：title的匹配权重大于content匹配权重 coerce：强行类型转换，默认true。如：如string（“10”）强制转换为integer（10） copy_to：将多个字段的值复制到同一字段中。如：first_name和last_name，复制到full_name doc_values：开启字段的排序、聚合和脚本中访问字段，支持除了text和annotated_text的所有字段类型，默认true。本质上是列式存储，保持与原始文档字段相同的值，关闭可节省空间 dynamic：新检测到的字段添加到映射，默认true。false表示不建立索引，strict表示拒绝文档 eager_global_ordinals：全局序号映射。文档中字段的值仅存储序号，而不存原始内容，用于聚合时提高性能 enabled：尝试为字段建立索引，仅可应用于顶级映射和object类型下，默认true。如果禁用整个映射，意味着可以对其进行获取，但是不会以任何方式对它的内容建立索引 format：自定义日期格式 ignore_above：超过长度的字符串内容将不会被索引 ignore_malformed：忽略错误的数据类型插入到索引中。默认抛出异常并丢弃文档 index：控制是否对字段值建立索引，默认true。未索引的字段不可查询 index_options：控制哪些信息添加到倒排索引已进行搜索并突出显示，仅使用于文本字段 index_phrases：将两个词的组合词索引到一个单独的字段中。默认false index_prefixes：为字段值的前缀编制索引，以加快前缀搜索速度 meta：附加到字段的元数据 fields：为不同的目的以不同的方式对同一字段建立索引 norms：用于计算查询的文档分数，默认true。对于仅用于过滤或聚合的字段，不需要对字段进行打分排序时设置为false null_value：使用指定的值替换为null值，以便可以进行索引和搜索 position_increment_gap：当为具有多个值的文本字段建立索引时，将在值之间添加“假”间隙，以防止大多数短语查询在值之间进行匹配，默认值为100 properties：类型映射，object字段和nested字段包含子字段叫properties search_analyzer：查询时使用指定的分析器 similarity：字段打分的相似性算法，默认BM25 store：单独存储属性值。默认对字段值进行索引以使其可搜索，但不单独存储它们，但是已存储在_source字段中 term_vector：存储分析过程的词矢量（Term vectors）信息。包括：词、位置、偏移量、有效载荷 映射限制设置（mapping limit settings）索引中定义太多字段会导致映射爆炸，从而导致内存不足错误和难以恢复的情况。在动态映射中，如果每个新插入的文档都引入新字段，每个新字段都添加到索引映射中，随着映射的增长，这会成为一个问题。使用映射限制设置可以限制（手动或动态创建的）字段映射的数量，并防止文档引起映射爆炸。 index.mapping.total_fields.limit: 字段最大数限制，默认1000 index.mapping.depth.limit: 字段的最大深度，默认20 index.mapping.nested_fields.limit: 单个索引中嵌套类型（nested）最大数限制，默认50 index.mapping.nested_objects.limit: 单个文档中嵌套JSON对象的最大数限制，默认10000 其他object与nested区别如果需要为对象数组建立索引并保持数组中每个对象的独立性，应该使用nested类型而不是object类型。 1234567891011121314PUT my_index/_doc/1&#123; &quot;group&quot; : &quot;fans&quot;, &quot;user&quot; : [ &#123; &quot;first&quot; : &quot;John&quot;, &quot;last&quot; : &quot;Smith&quot; &#125;, &#123; &quot;first&quot; : &quot;Alice&quot;, &quot;last&quot; : &quot;White&quot; &#125; ]&#125; ES内部会转换成这样的对象： 12345&#123; &quot;group&quot; : &quot;fans&quot;, &quot;user.first&quot; : [ &quot;alice&quot;, &quot;john&quot; ], &quot;user.last&quot; : [ &quot;smith&quot;, &quot;white&quot; ]&#125; multi-fields（多字段不同的目的）为了不同的目的，以不同的方式对同一个字段进行索引https://www.elastic.co/guide/en/elasticsearch/reference/7.11/multi-fields.htmlhttps://stackoverflow.com/questions/42383341/full-text-search-as-well-as-terms-search-on-same-filed-of-elasticsearch 12PUT my_index&#123;&quot;mappings&quot;:&#123;&quot;properties&quot;:&#123;&quot;city&quot;:&#123;&quot;type&quot;:&quot;text&quot;,&quot;fields&quot;:&#123;&quot;raw&quot;:&#123;&quot;type&quot;:&quot;keyword&quot;&#125;&#125;&#125;&#125;&#125;&#125; 示例12345678910111213141516171819202122232425262728293031323334353637383940414243444546PUT my_index&#123; &quot;mappings&quot;: &#123; &quot;dynamic_templates&quot;: [ &#123; &quot;strings_as_keywords&quot;: &#123; &quot;match_mapping_type&quot;: &quot;string&quot;, &quot;mapping&quot;: &#123; &quot;type&quot;: &quot;keyword&quot;, &quot;ignore_above&quot;: 1024 &#125; &#125; &#125;, &#123; &quot;unindexed_longs&quot;: &#123; &quot;match_mapping_type&quot;: &quot;long&quot;, &quot;mapping&quot;: &#123; &quot;type&quot;: &quot;long&quot;, &quot;index&quot;: false &#125; &#125; &#125; ], &quot;properties&quot;: &#123; &quot;@timestamp&quot;: &#123; &quot;type&quot;: &quot;date&quot; &#125;, &quot;message&quot;: &#123; &quot;type&quot;: &quot;text&quot;, &quot;index&quot;: false &#125;, &quot;log&quot;: &#123; &quot;properties&quot;: &#123; &quot;file&quot;: &#123; &quot;properties&quot;: &#123; &quot;path&quot;: &#123; &quot;type&quot;: &quot;text&quot;, &quot;index&quot;: false &#125; &#125; &#125; &#125; &#125; &#125; &#125;&#125;","categories":[{"name":"技术","slug":"技术","permalink":"http://www.lights8080.com/categories/%E6%8A%80%E6%9C%AF/"}],"tags":[{"name":"ELK","slug":"ELK","permalink":"http://www.lights8080.com/tags/ELK/"},{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"http://www.lights8080.com/tags/Elasticsearch/"}]},{"title":"语录-2","slug":"杂谈/语录-2","date":"2021-03-24T16:00:00.000Z","updated":"2021-08-03T01:47:54.000Z","comments":true,"path":"p/3d7fef96.html","link":"","permalink":"http://www.lights8080.com/p/3d7fef96.html","excerpt":"如果你像这样全速开上五分钟，就抵得上某些人碌碌无为的活一辈子 少许的牺牲是必须的 上吊的研究","text":"如果你像这样全速开上五分钟，就抵得上某些人碌碌无为的活一辈子 少许的牺牲是必须的 上吊的研究 1. 如果你像这样全速开上五分钟，就抵得上某些人碌碌无为的活一辈子。《世界上最快的印第安摩托》讲述伯特·孟若的真实故事，为了速度与极限，跨越半个地球前往美国犹他州的盐湖城参加世界机车大赛。他的摩托车甚至没有刹车、没有减速伞、没有灭火器，刷新了当时的世界纪录。此后他又多次刷新世界纪录，最后于1967年创下1000CC以下改装摩托车速度记录，至今无人打破。创纪录时他年近七十。 电影中的语录：活到我这把年纪，在世界上的每一天都是最好的一天。做人如果没有梦想，还不如当颗白菜。荣誉永远都不属于评论家，他们一直在等着别人犯错，然后告诉他们如何改进，荣誉只属于那些真正有行动的人。于我而言，驰骋本身就是回报。这是我这辈子最想做的大事，相比于其他家伙，做的更好，更大的事。很多人都想让我们这些老东西，安安静静的一边等死，但是哥们我告诉你，还不到时候。也许我外表上已经老态龙钟，但我的内心永远都是十八岁，年轻人，我依然可以跟你赛车赌钱。你不知道这对我来说意味着什么，为了今天，我已经等了25年。在追梦者眼里，所爱隔山海，山海皆可平。 解说最后总结的非常好：“””对一些人来说，梦想是人生中可有可无的调味品，如果生活不允许，那就放弃。但对另一部分人来说，梦想就是一辈子的事，是绝对无法妥协的事。所以，有些人是为梦想拼了命，有些人只是为梦想踮了踮脚尖。 我们普通人不是没有梦想，而是没有为梦想孤注一掷的信念，我们会计算得失，计算成本，会计算为梦想付出的代价是否值得，当我们在给梦想做这样的计算时，实际上我们已经做好了随时退缩的准备。“”” 2. 少许的牺牲是必须的奥托·利林塔尔，滑翔机之父，从1891年～1896年，飞行试验次数多达2000次。最后一次飞行过程中因为一股风力，失去控制栽向地面，脊椎骨折，第二天逝世。临终前最后的一句话就是”少许的牺牲是必须的”。 但愿所有的人都能忠于自己，拥有一个梦想，并为之努力奋斗。最终实现不了也没关系，因为你就是被上天选中来凑数的。也但愿梦想也不要像下面这位这么疯狂。 3. 《上吊的研究》尼古拉-米诺维奇。为了更好的研究绞刑，自己进行了十几次的上吊实验，每次做完实验清醒后，做的第一件事就是做记录。最终总结出了200多页的权威报告。","categories":[{"name":"杂谈","slug":"杂谈","permalink":"http://www.lights8080.com/categories/%E6%9D%82%E8%B0%88/"}],"tags":[{"name":"语录","slug":"语录","permalink":"http://www.lights8080.com/tags/%E8%AF%AD%E5%BD%95/"}]},{"title":"Logstash-配置","slug":"技术/ELK/Logstash-配置","date":"2021-03-21T16:00:00.000Z","updated":"2021-08-03T01:47:54.000Z","comments":true,"path":"p/2d82b4af.html","link":"","permalink":"http://www.lights8080.com/p/2d82b4af.html","excerpt":"Logstash配置介绍、插件说明、配置说明、高级配置、命令说明基于7.11版本。https://www.elastic.co/guide/en/logstash/7.11/index.html","text":"Logstash配置介绍、插件说明、配置说明、高级配置、命令说明基于7.11版本。https://www.elastic.co/guide/en/logstash/7.11/index.html 一、配置Logstash配置 Logstash，你需要创建一个配置文件来指定想要使用的插件和每个插件的设置。可以引用配置中的事件字段，并在事件满足某些条件时使用条件来处理它们。运行logstash时使用-f指定配置文件。 每种类型的插件都有一个单独的部分，每个部分都包含一个或多个插件的配置选项。如果指定多个过滤器，则会按照它们在配置文件中出现的顺序进行应用。 logstash-simple.conf 12345678910111213input &#123; stdin &#123; &#125; &#125;filter &#123; grok &#123; match =&gt; &#123; &quot;message&quot; =&gt; &quot;%&#123;COMBINEDAPACHELOG&#125;&quot; &#125; &#125; date &#123; match =&gt; [ &quot;timestamp&quot; , &quot;dd/MMM/yyyy:HH:mm:ss Z&quot; ] &#125;&#125;output &#123; elasticsearch &#123; hosts =&gt; [&quot;localhost:9200&quot;] &#125; stdout &#123; codec =&gt; rubydebug &#125;&#125; 1. 在配置中访问事件数据和字段Logstash中的某些配置选项需要使用事件的字段。因为输入会生成事件，所以输入块中没有要评估的字段，因为它们还不存在。 引用事件数据和字段仅在过滤器和输出块内起作用。基本语法是[fieldname]，引用顶级字段时可以去掉[]，引用嵌套字段时，要指定完成整路径[top-level field][nested field]。 sprintf格式化引用事件字段：increment =&gt; “apache.%{[response][status]}”引用事件日期和类型：path =&gt; “/var/log/%{type}.%{+yyyy.MM.dd.HH}” 条件只想在特定条件下过滤或输出事件，这时可以使用条件。 1234567if EXPRESSION &#123; ...&#125; else if EXPRESSION &#123; ...&#125; else &#123; ...&#125; 比较运算符:equality: ==, !=, &lt;, &gt;, &lt;=, &gt;=regexp: =, ! (checks a pattern on the right against a string value on the left)inclusion: in, not in 布尔运算符:and, or, nand, xor 一元运算符：!（取反） 还可以使用(...)，对表达式进行分组。 @metadata字段一个特殊的字段，在输出时不会成为任何事件的一部分。非常适用于做条件，扩展和构建事件字段等 二、插件说明Input plugins beats file kafka Filter plugins aggregate聚合属于同一任务的多个事件（通常是日志行）中的可用信息，最后将聚合的信息推送到最终任务事件中 clone克隆事件，原始事件保持不变。新事件作为正常的事件插入到管道中，并从生成事件的过滤器开始继续执行。 date解析字段中的日期，然后使用该日期或时间戳作为事件的Logstash的时间戳 Grok filter plugin解析任何文本并将其结构化。适用于文本的结构是逐行变化。%{SYNTAX:SEMANTIC}grok-patternsgrokdebug Dissect filter plugin适用于界定符拆分，不使用正则表达式，而且速度非常快。%{id-&gt;} %{function} %{+ts} %&#123;+ts&#125;：表示前面已经捕获到一个ts字段了，而这次捕获的内容，自动添补到之前ts字段内容的后面 -&gt;：表示忽略它右边的填充 %&#123;+key/2&#125;：表示在有多次捕获内容都填到key字段里的时候，拼接字符串的顺序谁前谁后。/2表示排第2位 %&#123;&#125;：表示是一个空的跳过字段 %&#123;?string&#125;：表示这块只是一个占位，并不会实际生成捕获字段存到事件里面 %&#123;?string&#125; %&#123;&amp;string&#125;：表示当同样捕获名称都是string，但是一个?一个&amp;的时候，表示这是一个键值对 drop删除到达此过滤器的所有内容 elapsed跟踪一对开始/结束事件，并使用它们的时间戳来计算它们之间的经过时间 elasticsearch在Elasticsearch中搜索上一个日志事件，并将其中的某些字段复制到当前事件中 fingerprint创建一个或多个字段的一致哈希（指纹），并将结果存储在新字段中 geoip根据来自Maxmind GeoLite2数据库的数据添加有关IP地址地理位置的信息 http提供了与外部Web服务/ REST API的集成。 java_uuid允许您生成UUID并将其作为字段添加到每个已处理事件 uuiduuid过滤器允许您生成UUID并将其作为字段添加到每个已处理事件。 jdbc_static通过从远程数据库预加载的数据来丰富事件 jdbc_streaming执行SQL查询，并将结果集存储在指定为目标的字段中。它会将结果本地缓存到过期的LRU缓存中 json这是一个JSON解析过滤器。它采用一个包含JSON的现有字段，并将其扩展为Logstash事件内的实际数据结构 kv有助于自动解析foo=bar种类的消息（或特定事件字段） metrics对于汇总指标很有用 mutate可以重命名，删除，替换和修改事件中的字段。需要注意的是，一个mutate块中的命令执行是有序的coerce -&gt; ... -&gt; copy，可以使用多个mutate块控制执行顺序。coerce: 设置空字段的默认值replace: 从事件中的其他部分构件一个新值，替换掉已有字段strip: 删除字段的前后空格update: 用新值更新现有字段，该字段不存在不采取任何操作gsub: 正则表达式匹配，将所有的匹配项更新为替换的字符串，只支持字符串和字符串数组，其他类型不采取任何操作join: 用分隔符连接数组，非数组字段不采取任何操作convert: 将字段的值转换为其他类型，如字符串转整数 prune用于根据字段名称或其值的白名单或黑名单从事件中删除字段（名称和值也可以是正则表达式）。如果使用json/kv过滤器解析出来一些不是事先知道的字段，只想保留其中一部分，这个功能很有用 range用于检查某些字段是否在预期的大小/长度范围内。支持的类型是数字和字符串。当在指定范围内时，执行一个操作。 ruby接受嵌入式Ruby代码或Ruby文件 sleep睡眠一定时间。这将导致logstash在给定的时间内停止运行。这对于速率限制等很有用 throttle节流过滤器用于限制事件数量 translate使用配置的哈希或文件确定替换值的常规搜索和替换工具。当前支持的是YAML，JSON和CSV文件。每个字典项目都是一个键值对 truncate允许您截断长度超过给定长度的字段 urldecode解码经过urlencoded的字段 useragentUserAgent过滤器，添加有关用户代理的信息，例如家族，操作系统，版本和设备 xmlXML过滤器。获取一个包含XML的字段，并将其扩展为实际的数据结构 Output plugins elasticsearch file stdout exec 三、配置说明包括：logstash.yaml、pipelines.yml、jvm.options、log4j2.properties、startup.options logstash.yml Logstash配置选项可以控制Logstash的执行。如：指定管道设置、配置文件位置、日志记录选项等。运行Logstash时，大多数配置可以命令行中指定，并覆盖文件的相关配置。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162node.name: `hostname`path.data: LOGSTASH_HOME/datapath.logs: LOGSTASH_HOME/logs# 指定main pipeline的配置文件路径path.config: # 指定main pipeline的配置数据。语法同配置文件config.string: # 开启后，检查配置是否有效，然后退出config.test_and_exit: false# 开启后，修改配置文件自动加载，过程：暂停管道所有输入；创新新管道并检验配置；检查成功切换到新管道，失败则继续使用老的管道。config.reload.automatic: false# 检查配置文件更新的时间间隔config.reload.interval: 3s# 内部队列模型，memory(default)：内存，persisted：磁盘queue.type: memory# 持久队列的数据文件存储路径（queue.type: persisted时启用）path.queue: path.data/queue# 持久队列的页容量，持久化以页为单位queue.page_capacity: 64mb# 开启后，关闭logstash之前等待持久队列消耗完毕queue.drain: false# 队列中允许的最大事件数，默认0表示无限制queue.max_events: 0# 事件缓冲的内部队列的总容量，达到限制时Logstash将不再接受新事件queue.max_bytes: 1024mb# 强制执行检查点之前的最大ACKed事件数queue.checkpoint.acks: 1024# 强制执行检查点之前，可以写入磁盘的最大事件数queue.checkpoint.writes: 1024# 对每次检查点写入失败将重试一次queue.checkpoint.retry: false# metrics REST endpoint绑定的地址和端口http.host: &quot;127.0.0.1&quot;http.port: 9600# 工作线程IDpipeline.id: main# 控制事件排序，auto：如果`pipeline.workers: 1`开启排序。true：如果有多个工作线程，强制对管道进行排序，并防止Logstash启动。false：禁用排序所需的处理，节省处理成本。pipeline.ordered: auto# 管道筛选和输出阶段的工作线程数，CPU没有饱和可以增加此数字更好的利用机器处理能力。pipeline.workers: `number of cpu cores`# 单个工作线程在发送到filters+workers之前，从输入中获取的最大事件数pipeline.batch.size: 125# 将小批量事件派送到filters+outputs之前，轮询下一个事件等待毫秒时间，可以理解为未到达批处理最大事件数时延迟发送时间pipeline.batch.delay: 50# 开启后，每个pipeline分割为不同的日志，使用pipeline.id作为文件名pipeline.separate_logs: false# 开启后，强行退出可能会导致关机期间丢失数据pipeline.unsafe_shutdown: false# 启用死信队列，默认falsedead_letter_queue.enable: falsedead_letter_queue.max_bytes: 1024mbpath.dead_letter_queue: path.data/dead_letter_queue# 指定自定义插件的位置path.plugins: # 配置模块，遵循yaml结构modules: 四、高级配置1. 多管道配置（multiple pipelines configuration）如果需要在同一个进程中运行多个管道，通过配置pipelines.yml文件来处理，必须放在path.settings文件夹中。并遵循以下结构： 1234567# config/pipelines.yml- pipeline.id: my-pipeline_1 path.config: &quot;/etc/path/to/p1.config&quot; pipeline.workers: 3- pipeline.id: my-other-pipeline path.config: &quot;/etc/different/path/p2.cfg&quot; queue.type: persisted 不带任何参数启动Logstash时，将读取pipelines.yml文件并实例化该文件中指定的所有管道。如果使用-e或-f时，Logstash会忽略pipelines.yml文件并记录相关警告。 如果当前的配置中的事件流不共享相同的输入/过滤器和输出，并且使用标签和条件相互分隔，则使用多个管道特别有用。 在单个实例中具有多个管道还可以使这些事件流具有不同的性能和持久性参数（例如，工作线程数和持久队列的不同设置）。 2. 管道到管道的通信（pipeline-to-pipeline Communication）使用Logstash的多管道功能时，可以在同一Logstash实例中连接多个管道。此配置对于隔离这些管道的执行以及有助于打破复杂管道的逻辑很有用。 3. 重新加载配置文件（Reloading the Config File）如果没有开启自动重新加载（–config.reload.automatic），可以强制Logstash重新加载配置文件并重新启动管道。 1kill -SIGHUP 14175 4. 管理多行事件（Managing Multiline Events）5. Glob模式支持（glob pattern support）注意Logstash不会按照glob表达式中编写的文件顺序执行，是按照字母顺序对其进行排序执行的。 6. Logstash到Logstash通讯（Logstash-to-Logstash Communication）7. Ingest Node解析数据转换到Logstash解析数据（Converting Ingest Node Pipelines）8. 集中配置管理（Centralized Pipeline Management）五、命令说明命令行上设置的所有参数都会覆盖logstash.yml中的相应设置。生产环境建议使用logstash.yml控制Logstash执行。 参数： –node.name NAME：指定Logstash实例的名称，默认当前主机名 -f, –path.config CONFIG_PATH：加载Logstash配置的文件或目录 -e, –config.string CONFIG_STRING：Logstash配置数据，如果未指定输入，则使用input &#123; stdin &#123; type =&gt; stdin &#125; &#125;作为默认的输入，如果未指定输出，则使用output &#123; stdout &#123; codec =&gt; rubydebug &#125; &#125;作为默认的输出。 -M “CONFIG_SETTING=VALUE”：覆盖指定的配置 –config.test_and_exit: 检查配置是否有效，然后退出 –config.reload.automatic: 修改配置文件自动加载 –modules MODULE_NAME：指定运行的模块名称 –setup：是一次性设置步骤，在Elasticsearch中创建索引模式并导入Kibana仪表板和可视化文件。 … 示例： 1bin/logstash -f logstash-simple.conf --config.reload.automatic","categories":[{"name":"技术","slug":"技术","permalink":"http://www.lights8080.com/categories/%E6%8A%80%E6%9C%AF/"}],"tags":[{"name":"ELK","slug":"ELK","permalink":"http://www.lights8080.com/tags/ELK/"},{"name":"Logstash","slug":"Logstash","permalink":"http://www.lights8080.com/tags/Logstash/"}]},{"title":"Logstash-介绍","slug":"技术/ELK/Logstash-介绍","date":"2021-03-18T16:00:00.000Z","updated":"2021-08-03T01:47:54.000Z","comments":true,"path":"p/5e5b28be.html","link":"","permalink":"http://www.lights8080.com/p/5e5b28be.html","excerpt":"本文内容是通过官网学习Logstash的一个总结，阅读本文可以对Logstash有个整体的认识。包括Logstash介绍、如何工作、事件模型、工作原理、弹性数据、持久化队列、性能优化、部署和扩展等基于7.11版本。https://www.elastic.co/guide/en/logstash/7.11/index.html","text":"本文内容是通过官网学习Logstash的一个总结，阅读本文可以对Logstash有个整体的认识。包括Logstash介绍、如何工作、事件模型、工作原理、弹性数据、持久化队列、性能优化、部署和扩展等基于7.11版本。https://www.elastic.co/guide/en/logstash/7.11/index.html 介绍Logstash是具有实时流水线能力的开源的数据收集引擎。Logstash可以动态统一不同来源的数据，并将数据标准化到您选择的目标输出。它提供了大量插件，可帮助我们解析，丰富，转换和缓冲任何类型的数据。 如何工作管道（Logstash Pipeline）是Logstash中独立的运行单元，每个管道都包含两个必须的元素输入（input）和输出（output），和一个可选的元素过滤器（filter），事件处理管道负责协调它们的执行。输入和输出支持编解码器，使您可以在数据进入或退出管道时对其进行编码或解码，而不必使用单独的过滤器。如：json、multiline等 inputs（输入阶段）：会生成事件。包括：file、kafka、beats等 filters（过滤器阶段）：可以将过滤器和条件语句结合使用对事件进行处理。包括：grok、mutate等 outputs（输出阶段）：将事件数据发送到特定的目的地，完成了所以输出处理，改事件就完成了执行。如：elasticsearch、file等 Codecs（解码器）：基本上是流过滤器，作为输入和输出的一部分进行操作，可以轻松地将消息的传输与序列化过程分开。 1. 工作原理Logstash管道中每个输入阶段都运行在自己的线程中，输入将事件写入到内存或磁盘的中央队列。每个管道工作线程（pipeline worker）从队列中获取一批事件，通过配置的过滤器运行这批事件，然后将过滤的事件运行到所有输出。批处理的大小和工作线程数可以通过pipeline.batch.size和pipeline.workers进行配置。 默认Logstash在管道各阶段之间使用内存队列来缓存事件，如果发生意外的终止，则内存中的事件都将丢失。为了防止数据丢失，可以启用Logstash配置queue.type: persisted将正在运行的事件持久保存到磁盘。 2. 事件顺序默认Logstash不保证事件顺序，重新排序可以发送在两个地方： 批处理中的事件可以在过滤器处理期间重新排序 当一个或多个批次的处理速度快于其他批次时，可以对批次重新排序 当维护事件顺序非常重要时，排序设置： 设置pipeline.ordered: auto且设置pipeline.workers: 1，则自动启用排序。 设置pipeline.ordered: true，这种方法可以确保批处理是一个一个的执行，并确保确保事件在批处理中保持其顺序。 设置pipeline.ordered: false则禁用排序处理，但可以节省排序所需的成本。 Logstash 模块Logstash Module提供了一种快速的端到端的解决方案，用于提取数据并使用专用仪表盘对其进行可视化。 每个模块都内置了Logstash配置、Kibana仪表盘和其他元文件。使您可以更轻松地为特定用例或数据源设置Elastic Stack。 为了更轻松的上手，Logstash Module提供了三种基本功能，运行模块时将执行以下步骤： 创建ElasticSearch索引 设置Kibana仪表盘和可视化数据所需要的索引模式，搜索和可视化。 使用配置运行Logstash pipeline 弹性数据当数据流过事件处理管道时，Logstash可能会遇到阻止其事件传递到输出的情况。如：意外的数据类型或异常终止。为了防止数据丢失并确保事件不中断的流过管道，Logstash提供了两种功能。 持久队列（persistent queues） 死信队列（dead letter queues-DLQ） 持久队列默认Logstash在管道阶段（inputs-&gt;pipeline worker）之间使用内存中有边界队列来缓冲事件。这些内存队列的大小是固定的，并且不可配置。如果Logstash遇到暂时的计算机故障，那内存队列中的数据将丢失。 为了防止异常终止期间的数据丢失，Logstash具有持久队列功能，该功能将消息队列存储在磁盘上，提供数据的持久性。持久队列对于需要大缓冲区的Logstash部署也很有用，无需部署和管理消息代理（Kafka、Redis等）以促进缓冲的发布-订阅模型，可以启用持久队列在磁盘上缓冲消息并删除消息代理。 使用queue.max_bytes可配置磁盘上队列的总容量，当队列已满时，Logstash向输入施加压力阻止数据流入，不再接受新事件，这种机制有助于在输入阶段控制数据流速，不会压倒性的到输出。 持久队列的好处： Logstash异常终止或重启启动时避免数据丢失，将消息存储在磁盘上，直到传递至少成功一次。 无需使用Kafka外部缓冲消息代理。应对大缓冲区和吸收突发事件。 无法解决的问题： 永久性机器故障（如磁盘损坏），持久队列无法防止数据丢失。具有确认能力的Beats和http之类的插件，将受到持久队列的良好保护。 不使用请求-响应协议的输入插件（如TCP、UDP），持久队列无法防止数据丢失。 工作原理 队列位于输入和过滤器阶段之间：input → queue → filter + output。 当输入阶段可处理事件时将事件写入队列，成功写入后，输入可以向数据源发送确认（acknowledgement）。 处理队列中的事件时，Logstash仅在过滤器和输出已完全处理该事件时，该事件才记录（队列保留管道已处理的事件记录）为已处理（acknowledged/ACKed）- 这意味着该事件已由所有已配置的过滤器和输出处理。 在正常关闭时，Logstash将停止从队列读取数据，并将完成正在由过滤器和输出处理中的事件。重启后，Logstash将恢复处理持久队列中的事件，并接受来自输入的新事件。 如果Logstash异常终止，任何运行中的事件都不会被记录为ACKed，并且在Logstash重新启动时将被过滤器和输出重新处理。Logstash在批处理事件，当发生异常终止时，可能有一些批处理已经成功完成，但没有记录为 ACKed。 页队列本身就是一个页（page）集合，分为头页（head page）和尾页（tail page），仅有一个头页，达到具体大小（queue.page_capacity）时将变成尾页，并创建一个新的头页。尾页是不可变的，头页是仅追加的。每个页都是一个文件，页中的所有事件确认后，将被删除，如果较旧的页中至少有一个未被确认，整个页将保留在磁盘上，直到成功处理该页上的所有事件为止。 检查点启用持久队列功能后，Logstash通过一种称为检查点（checkpoint）的机制提交到磁盘。检查点文件在单独文件中记录有关自身的详细信息（页信息，确认等）。当记录检查点时，Logstash将调用头页的同步操作和以原子的方式将队列的当前状态写入磁盘。检查点的过程是原子的，意味着如果成功，将保存对文件的任何修改。如果Logstash终止，或者出现硬件级别的故障，则持久队列中缓冲但尚未提交检查点的所有数据都将丢失。可以通过设置queue.checkpoint.writes，强制Logstash更频繁地检查点。为了确保最大的持久性避免丢失数据，可以设置queue.checkpoint.writes为1，在每次事件后强制执行检查点。 死信队列死信队列提供了另一层数据弹性。（当前仅对Elasticsearch输出支持死信队列，用于响应码为400和404的文档，二者均表示无法重试的事件。）默认情况，当Logstash遇到由于数据错误而无法处理事件时，会挂起或删除失败的事件。为了防止数据丢失，可以配置将未成功的事件写入死信队列，而不是丢弃。写入死信队列的每个事件包括原始事件、无法处理的原因、写入事件的插入信息以及事件时间戳。要处理死信队列的事件，需要创建一个管道配置，使用dead_letter_queue插件从死信队列中读取数据。 工作原理 Elasticsearch无法访问的HTTP请求失败，Elasticsearch输出插件将无限期的重试整个请求，这些场景中死信队列不会拦截。 部署和扩展从操作日志和指标分析到企业和应用程序搜索，Elastic Stack可用于大量用例。确保将数据可扩展、持久和安全地传输到Elasticsearch极为重要，尤其是对于关键任务环境。本文重点介绍Logstash的常见体系结构模型，以及如何随着部署的增长而有效的扩展。重点放在操作日志、指标、安全分析用例上，因为它们往往需要大规模部署。 Beats to Elasticsearch使用Filebeat Modules，可以快速的收集、解析和索引流行的日志类型和预建的Kibana仪表盘。这种情况下Beats会将数据直接发送到ES，由摄取节点处理并索引数据。 Beats and Logstash to ElasticsearchBeats和Logstash共同提供了可扩展且具有弹性的全面解决方案。Beats运行在数千台边缘主机服务器上，将日志收集、拖尾和传输到Logstash。Logstash是水平可伸缩的，可以形成运行同一管道的节点组。Logstash的自适应缓冲功能即使在吞吐量变化不定的情况下也有助于流畅的传输。如果Logstash成为瓶颈，只需要添加更多节点即可进行横向扩展。以下是一些建议： 扩展： Beats应该在一组Logstash节点之间实现负载均衡 建议至少使用两个Logstash节点已实现高可用性 通常每个Logstash节点仅部署一个Beats输入，但也可以为每个Logstash节点部署多个Beats输入。 弹性： 使用Filebeat/Winlogbeat进行日志收集时，可以保证至少一次交付 从Filebeat/Winlogbeat到Logstash，以及从Logstash到Elasticsearch这两种通讯协议都是同步且支持确认。其他的Beats不支持。 处理： Logstash通常将使用grok或dissect提取字段，增强地理信息，并可以进一步利用文件、数据库或Elasticsearch查找数据集来丰富事件。 处理复杂性会影响整体吞吐量和CPU利用率，确保检查其他可用的过滤器插件。 Integrating with Messaging Queues如果现有的基础架构中有消息队列，那么将数据放入Elastic Stack会很容易。如果仅使用消息队列用于Logstash缓冲数据，建议使用Logstash持久队列，消除不必要的复杂性。 性能调优包括性能故障排除和调优和分析Logstash性能。 JVM 建议堆的大小应不小于4G且不大于8G，太小会导致JVM不断的进行垃圾回收，造成增加CPU利用率 堆的大小不要超过物理内存量的水平，必须保留一些内存以运行OS和其他进程，一般不要超过物理内存的50-75％。 将最小（Xms）和最大（Xmx）堆分配大小设置为相同的值，以防止在运行时调整堆大小，这是一个非常昂贵的过程。 调优和分析Logstash性能Logstash提供了以下选项来优化管道性能，pipeline.workers，pipeline.batch.size和pipeline.batch.delay。 pipeline.workers此设置确定要运行多少个线程以进行过滤和输出处理。如果发现事件正在备份或者CPU没有饱和可以考虑增加此参数以更好的利用可用的处理能力。 pipeline.batch.size此设置定义单个工作线程在尝试执行过滤器和输出之前收集的最大事件数。较大的批处理大小通常更有效，但会增加内存开销。某些硬件配置要求您增加jvm.options配置文件中的JVM堆空间，以避免性能下降。由于频繁的垃圾收集或与内存不足异常相关的JVM崩溃，超出最佳范围的值会导致性能下降。输出插件可以将每个批次作为逻辑单元进行处理。例如，Elasticsearch输出为收到的每个批次发出批量请求。调整pipeline.batch.size设置可调整发送到Elasticsearch的批量请求的大小。 pipeline.batch.delay很少需要调整。此设置调整Logstash管道的延迟。管道批处理延迟是Logstash在当前管道工作线程中接收到事件后等待新消息的最长时间（以毫秒为单位）。经过这段时间后，Logstash开始执行过滤器和输出。Logstash在接收事件和在过滤器中处理该事件之间等待的最长时间是pipeline.batch.delay和pipeline.batch.size设置的乘积。 管道的配置和优化进行中事件的总数由pipeline.workers和pipeline.batch.size设置的乘积确定。注意在间歇地不规则的接收大型事件的管道，需要足够的内存来处理这些峰值。可以将工作线程数设置高于CPU内核数，因为输出通常度过空闲时间在I/O等待条件下。","categories":[{"name":"技术","slug":"技术","permalink":"http://www.lights8080.com/categories/%E6%8A%80%E6%9C%AF/"}],"tags":[{"name":"ELK","slug":"ELK","permalink":"http://www.lights8080.com/tags/ELK/"},{"name":"Logstash","slug":"Logstash","permalink":"http://www.lights8080.com/tags/Logstash/"}]},{"title":"生产环境的一次JVM调优过程","slug":"技术/JVM/生产环境的一次JVM调优过程","date":"2021-02-04T16:00:00.000Z","updated":"2021-08-03T01:47:54.000Z","comments":true,"path":"p/f53d0b8c.html","link":"","permalink":"http://www.lights8080.com/p/f53d0b8c.html","excerpt":"JVM调优过程。问题背景、分析过程、优化思路","text":"JVM调优过程。问题背景、分析过程、优化思路 问题背景机器负载截图： 进程截图： 启动参数： 1java -jar -Xms2048m -Xmx4096m -Xss256k -XX:MetaspaceSize=128m -XX:MaxMetaspaceSize=4096m -XX:NewSize=1920m -XX:MaxNewSize=4096m -XX:SurvivorRatio=6 -XX:+UseParNewGC -XX:ParallelGCThreads=8 -XX:MaxTenuringThreshold=9 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=60 分析过程 服务器内存8G，java进程最大分配内存4G。但进程显示内存使用率27.4，实际使用2.1G。进程内存并没有达到最大分配内存，而CPU却使用率很高。初步分析肯定是Full GC导致CPU使用率高。 查看GC情况 jstat -gcutil 545592 S0：幸存区1当前使用比例S1：幸存区2当前使用比例E：伊甸园区使用比例O：老年代使用比例M：元数据区使用比例CCS：压缩使用比例YGC：年轻代垃圾回收次数FGC：老年代垃圾回收次数FGCT：老年代垃圾回收消耗时间GCT：垃圾回收消耗总时 通过查看GC使用情况，得出年轻代使用比例不高，回收次数少，老年代使用比例高，回收次数过多。 查看Heap信息 jmap -heap 545592 结果显示老年代分片空间：OldSize = 65536 (0.0625MB)新生代分片空间：MaxNewSize = 4294901760 (4095.9375MB)频繁对老年代进行Full GC，引起CPU资源占用高。 优化思路问题的根源很简单，没有真正了解JVM的内存模型以及参数控制，不合理的分配新生代和老年代空间导致。 暂时去掉参数-XX:NewSize、-XX:MaxNewSize，默认NewRatio=2（表示老年代:新生代 = 1:2）。跑一段时间检查新生代和老年代占用情况以增长速度和回收次数等，然后酌情合理配置参数-XX:NewSize、-XX:MaxNewSize。 运行一段时间后观察 jstat -gcutil 3239169 jmap -heap 3239169 结果显示FGC没有再发生，YGC的次数也很低。实际新生代Survivor区只使用了2M，老年代只使用了56M且不再明显增长。所以优化方案是降低老年代和新生代Survivor区空间，增加新生代Eden区空间。 优化策略 修改-Xms1024m -Xmx2048m：降低初始堆空间大小，因为业务访问量并不高，新生代增长速度不快，遵循不浪费资源、压榨服务器原则。 修改-XX:NewSize=896m -XX:MaxNewSize=1536m：结果显示老年代占用并不大，增长也较慢，所以提高新生代的空间有助于减少YGC，但是太大YGC的时长会增加 修改-XX:SurvivorRatio=8：结果显示Survivor区的使用率也不高，进一步提高Eden区大小 修改–XX:CMSInitiatingOccupancyFraction=80，提升老年代占比触发垃圾回收的阈值，降低CMS次数 修改-XX:MetaspaceSize=128m -XX:MaxMetaspaceSize=512m，永久代不占用堆大小，占用率并不高，遵循不浪费原则 调整后的启动参数如下： 1java -jar -Xms1024m -Xmx2048m -Xss256k -XX:MetaspaceSize=128m -XX:MaxMetaspaceSize=512m -XX:NewSize=896m -XX:MaxNewSize=1536m -XX:SurvivorRatio=8 -XX:+UseParNewGC -XX:MaxTenuringThreshold=9 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=80","categories":[{"name":"技术","slug":"技术","permalink":"http://www.lights8080.com/categories/%E6%8A%80%E6%9C%AF/"}],"tags":[{"name":"JVM","slug":"JVM","permalink":"http://www.lights8080.com/tags/JVM/"},{"name":"线上问题","slug":"线上问题","permalink":"http://www.lights8080.com/tags/%E7%BA%BF%E4%B8%8A%E9%97%AE%E9%A2%98/"}]},{"title":"spring cloud gateway [DataBufferLimitException: Exceeded limit on max bytes to buffer : 262144]","slug":"技术/SpringCloud/spring cloud gateway [DataBufferLimitException: Exceeded limit on max bytes to buffer : 262144]","date":"2021-02-03T16:00:00.000Z","updated":"2021-08-03T01:47:54.000Z","comments":true,"path":"p/178537e0.html","link":"","permalink":"http://www.lights8080.com/p/178537e0.html","excerpt":"DataBufferLimitException: Exceeded limit on max bytes to buffer : 262144问题背景、分析过程、解决办法","text":"DataBufferLimitException: Exceeded limit on max bytes to buffer : 262144问题背景、分析过程、解决办法 问题背景线上环境spring cloud gateway偶尔遇到如下异常：DataBufferLimitException: Exceeded limit on max bytes to buffer : 262144 问题的产生原因很容易查到，REST API接口请求数据超过256K，被网关拦截。正常我们加大缓冲区的配置即可，-1表示不限制。但是并没有解决问题，下面介绍我的分析过程和解决办法。 123spring: codec: max-in-memory-size: 1048576 环境信息： Spring Boot 2.2.1.RELEASE 异常信息： 12345678910111213141516171819202021-02-04 15:39:33.014 [reactor-http-epoll-2] ERROR org.springframework.boot.autoconfigure.web.reactive.error.AbstractErrorWebExceptionHandler - [1513c08e-1] 500 Server Error for HTTP POST &quot;/x-service/path&quot;org.springframework.core.io.buffer.DataBufferLimitException: Exceeded limit on max bytes to buffer : 262144 at org.springframework.core.io.buffer.LimitedDataBufferList.raiseLimitException(LimitedDataBufferList.java:101) Suppressed: reactor.core.publisher.FluxOnAssembly$OnAssemblyException:Error has been observed at the following site(s): |_ checkpoint ⇢ org.springframework.cloud.gateway.filter.WeightCalculatorWebFilter [DefaultWebFilterChain] |_ checkpoint ⇢ org.springframework.boot.actuate.metrics.web.reactive.server.MetricsWebFilter [DefaultWebFilterChain] |_ checkpoint ⇢ HTTP POST &quot;/x-service/path&quot; [ExceptionHandlingWebHandler]Stack trace: at org.springframework.core.io.buffer.LimitedDataBufferList.raiseLimitException(LimitedDataBufferList.java:101) at org.springframework.core.io.buffer.LimitedDataBufferList.updateCount(LimitedDataBufferList.java:94) at org.springframework.core.io.buffer.LimitedDataBufferList.add(LimitedDataBufferList.java:59) at reactor.core.publisher.MonoCollect$CollectSubscriber.onNext(MonoCollect.java:124) at reactor.core.publisher.FluxMap$MapSubscriber.onNext(FluxMap.java:114) at reactor.core.publisher.FluxPeek$PeekSubscriber.onNext(FluxPeek.java:192) at reactor.core.publisher.FluxMap$MapSubscriber.onNext(FluxMap.java:114) at reactor.netty.channel.FluxReceive.onInboundNext(FluxReceive.java:331) at reactor.netty.channel.ChannelOperations.onInboundNext(ChannelOperations.java:352) at reactor.netty.http.server.HttpServerOperations.onInboundNext(HttpServerOperations.java:484)... 分析过程 定位异常触发点org.springframework.core.io.buffer.LimitedDataBufferList.java 查找maxByteCount的设置代码org.springframework.http.codec.support.BaseDefaultCodecs.java 经过调试发现，启动时初始化是正常的，配置此参数有效spring.codec.max-in-memory-size 但业务调用的时候此参数接受值为null，配置并未生效 继续查找调用源头发现，我们的自定义拦截器获取body信息，代码如下 1ServerRequest serverRequest = ServerRequest.create(exchange, HandlerStrategies.withDefaults().messageReaders()); 因为HandlerStrategies.withDefaults()是重新创建的对象，并未使用Spring注入的对象，造成配置不生效 查看gateway的ModifyRequestBodyGatewayFilterFactory，使用注入的messageReaders即可org.springframework.cloud.gateway.filter.factory.rewrite.ModifyRequestBodyGatewayFilterFactory.java 解决办法参考ModifyRequestBodyGatewayFilterFactory调整自定义拦截器，使用注入的配置类。代码如下： 123456789101112131415161718192021222324252627282930313233343536373839@Component@Slf4jpublic class XXXFilter implements GlobalFilter, Ordered &#123; @Autowired ServerCodecConfigurer codecConfigurer; @Override public Mono&lt;Void&gt; filter(ServerWebExchange exchange, GatewayFilterChain chain) &#123; ServerHttpRequest request = exchange.getRequest(); try &#123; if (request.getHeaders().getContentLength() &gt; 0) &#123; ServerRequest serverRequest = ServerRequest.create(exchange, codecConfigurer.getReaders()); Mono&lt;String&gt; bodyToMono = serverRequest.bodyToMono(String.class); return bodyToMono.flatMap(reqBody -&gt; &#123; if (isJson(reqBody)) &#123; String perms = JSONObject.parseObject(reqBody).getString(&quot;perms&quot;); if (StringUtils.isNotEmpty(perms)) &#123; exchange.getAttributes().put(Constants.XXX_REQUEST_PERMS_ATTR, perms); &#125; &#125; ServerHttpRequestDecorator requestDecorator = new ServerHttpRequestDecorator(exchange.getRequest()) &#123; @Override public Flux&lt;DataBuffer&gt; getBody() &#123; NettyDataBufferFactory nettyDataBufferFactory = new NettyDataBufferFactory(ByteBufAllocator.DEFAULT); DataBuffer bodyDataBuffer = nettyDataBufferFactory.wrap(reqBody.getBytes()); return Flux.just(bodyDataBuffer); &#125; &#125;; return chain.filter(exchange.mutate().request(requestDecorator).build()); &#125;); &#125; return chain.filter(exchange); &#125; catch (Exception e) &#123; log.error(&quot;Exception[XXXFilter]:&quot;, e); return chain.filter(exchange); &#125; &#125;","categories":[{"name":"技术","slug":"技术","permalink":"http://www.lights8080.com/categories/%E6%8A%80%E6%9C%AF/"}],"tags":[{"name":"线上问题","slug":"线上问题","permalink":"http://www.lights8080.com/tags/%E7%BA%BF%E4%B8%8A%E9%97%AE%E9%A2%98/"},{"name":"SpringCloud","slug":"SpringCloud","permalink":"http://www.lights8080.com/tags/SpringCloud/"}]},{"title":"语录-1","slug":"杂谈/语录-1","date":"2021-01-30T16:00:00.000Z","updated":"2021-08-03T01:47:54.000Z","comments":true,"path":"p/a476be2c.html","link":"","permalink":"http://www.lights8080.com/p/a476be2c.html","excerpt":"我希望这才是35岁危机的真正原因 正确理解“技术” 如何变强","text":"我希望这才是35岁危机的真正原因 正确理解“技术” 如何变强 1. 我希望这才是35岁危机的真正原因当我35岁时，如果我还奋斗在编码的一线，我希望我被劝退的原因是因为技术能力、业务能力比不过年轻人，而不是因为精力和体力跟不上年轻人。 忘记是从哪里看到的这句话了。最近这一年出现了很多贩卖焦虑的文章，这句话可以说是程序员面对中年危机的正面回击了。 人到中年精力和体力跟不上年轻人这是自然规律，但是技术能力和业务能力比不过年轻人，这只能说明你不是一名优秀的程序员。面对变化，终身成长，持续提升自己的核心竞争力，是每个优秀程序员必须要有的特质。 借鉴这句话，我想给一些年轻程序员该如何涨工资提供一点思路。“我希望你涨薪的原因是因为能力提高了，希望承担更多的责任，而不是因为在同一家公司工作了多少年、加了多少班和我同学都已经拿到多少多少了。” 2. 正确理解“技术”进入职场前的面试环节，由于时间有限，主要还是考察的“技”。当你通过面试，进入职场后，“术”比“技”重要一些。 “技”指的是对代码规范性和设计能力的提高，对组件的掌握程度提高和学习最新的前沿技术等。“术”指的是对业务的深度了解，对问题的理解和分析和对工作认真负责的态度等。 当我认识到“技术是为业务而服务”的时候，才是职场中一大进步。 3. 如何变强你解决的问题越多，你的能力就会越强，你就能解决更多难题，这时候你就已经变强了。– 王岩 我们每个人，每个时候，都是在为自己的简历打工。不管公司能够维持多久，这份简历会一直陪着我们。– 薛兆丰 不想当将军的士兵不是好士兵，但当不好一个士兵的将军，一定不是好将军。– 马云 我最讨厌天天抱怨工作，还不辞职的人。– 马云 在任何行业里，混吃等死的人不被开，都对不起那些努力的人。– 李苦李","categories":[{"name":"杂谈","slug":"杂谈","permalink":"http://www.lights8080.com/categories/%E6%9D%82%E8%B0%88/"}],"tags":[{"name":"语录","slug":"语录","permalink":"http://www.lights8080.com/tags/%E8%AF%AD%E5%BD%95/"}]}],"categories":[{"name":"weekly","slug":"weekly","permalink":"http://www.lights8080.com/categories/weekly/"},{"name":"读书","slug":"读书","permalink":"http://www.lights8080.com/categories/%E8%AF%BB%E4%B9%A6/"},{"name":"技术","slug":"读书/技术","permalink":"http://www.lights8080.com/categories/%E8%AF%BB%E4%B9%A6/%E6%8A%80%E6%9C%AF/"},{"name":"技术","slug":"技术","permalink":"http://www.lights8080.com/categories/%E6%8A%80%E6%9C%AF/"},{"name":"工具","slug":"工具","permalink":"http://www.lights8080.com/categories/%E5%B7%A5%E5%85%B7/"},{"name":"科普","slug":"科普","permalink":"http://www.lights8080.com/categories/%E7%A7%91%E6%99%AE/"},{"name":"杂谈","slug":"杂谈","permalink":"http://www.lights8080.com/categories/%E6%9D%82%E8%B0%88/"}],"tags":[{"name":"weekly","slug":"weekly","permalink":"http://www.lights8080.com/tags/weekly/"},{"name":"凤凰架构","slug":"凤凰架构","permalink":"http://www.lights8080.com/tags/%E5%87%A4%E5%87%B0%E6%9E%B6%E6%9E%84/"},{"name":"事务","slug":"事务","permalink":"http://www.lights8080.com/tags/%E4%BA%8B%E5%8A%A1/"},{"name":"架构","slug":"架构","permalink":"http://www.lights8080.com/tags/%E6%9E%B6%E6%9E%84/"},{"name":"中台","slug":"中台","permalink":"http://www.lights8080.com/tags/%E4%B8%AD%E5%8F%B0/"},{"name":"大数据","slug":"大数据","permalink":"http://www.lights8080.com/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"},{"name":"数据仓库","slug":"数据仓库","permalink":"http://www.lights8080.com/tags/%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/"},{"name":"RPC","slug":"RPC","permalink":"http://www.lights8080.com/tags/RPC/"},{"name":"微服务","slug":"微服务","permalink":"http://www.lights8080.com/tags/%E5%BE%AE%E6%9C%8D%E5%8A%A1/"},{"name":"网络","slug":"网络","permalink":"http://www.lights8080.com/tags/%E7%BD%91%E7%BB%9C/"},{"name":"阿里云","slug":"阿里云","permalink":"http://www.lights8080.com/tags/%E9%98%BF%E9%87%8C%E4%BA%91/"},{"name":"开源产品","slug":"开源产品","permalink":"http://www.lights8080.com/tags/%E5%BC%80%E6%BA%90%E4%BA%A7%E5%93%81/"},{"name":"IDEA","slug":"IDEA","permalink":"http://www.lights8080.com/tags/IDEA/"},{"name":"for Mac","slug":"for-Mac","permalink":"http://www.lights8080.com/tags/for-Mac/"},{"name":"线程池","slug":"线程池","permalink":"http://www.lights8080.com/tags/%E7%BA%BF%E7%A8%8B%E6%B1%A0/"},{"name":"Java","slug":"Java","permalink":"http://www.lights8080.com/tags/Java/"},{"name":"SSH","slug":"SSH","permalink":"http://www.lights8080.com/tags/SSH/"},{"name":"Git","slug":"Git","permalink":"http://www.lights8080.com/tags/Git/"},{"name":"Linux","slug":"Linux","permalink":"http://www.lights8080.com/tags/Linux/"},{"name":"shell","slug":"shell","permalink":"http://www.lights8080.com/tags/shell/"},{"name":"脚本","slug":"脚本","permalink":"http://www.lights8080.com/tags/%E8%84%9A%E6%9C%AC/"},{"name":"ELK","slug":"ELK","permalink":"http://www.lights8080.com/tags/ELK/"},{"name":"JAVA","slug":"JAVA","permalink":"http://www.lights8080.com/tags/JAVA/"},{"name":"编程思想","slug":"编程思想","permalink":"http://www.lights8080.com/tags/%E7%BC%96%E7%A8%8B%E6%80%9D%E6%83%B3/"},{"name":"Maven","slug":"Maven","permalink":"http://www.lights8080.com/tags/Maven/"},{"name":"字符集","slug":"字符集","permalink":"http://www.lights8080.com/tags/%E5%AD%97%E7%AC%A6%E9%9B%86/"},{"name":"多线程","slug":"多线程","permalink":"http://www.lights8080.com/tags/%E5%A4%9A%E7%BA%BF%E7%A8%8B/"},{"name":"Mysql","slug":"Mysql","permalink":"http://www.lights8080.com/tags/Mysql/"},{"name":"Python","slug":"Python","permalink":"http://www.lights8080.com/tags/Python/"},{"name":"ElastAlert","slug":"ElastAlert","permalink":"http://www.lights8080.com/tags/ElastAlert/"},{"name":"Hexo","slug":"Hexo","permalink":"http://www.lights8080.com/tags/Hexo/"},{"name":"前端","slug":"前端","permalink":"http://www.lights8080.com/tags/%E5%89%8D%E7%AB%AF/"},{"name":"NodeJs","slug":"NodeJs","permalink":"http://www.lights8080.com/tags/NodeJs/"},{"name":"Nginx","slug":"Nginx","permalink":"http://www.lights8080.com/tags/Nginx/"},{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"http://www.lights8080.com/tags/Elasticsearch/"},{"name":"李永乐老师","slug":"李永乐老师","permalink":"http://www.lights8080.com/tags/%E6%9D%8E%E6%B0%B8%E4%B9%90%E8%80%81%E5%B8%88/"},{"name":"人体系统","slug":"人体系统","permalink":"http://www.lights8080.com/tags/%E4%BA%BA%E4%BD%93%E7%B3%BB%E7%BB%9F/"},{"name":"语录","slug":"语录","permalink":"http://www.lights8080.com/tags/%E8%AF%AD%E5%BD%95/"},{"name":"线上问题","slug":"线上问题","permalink":"http://www.lights8080.com/tags/%E7%BA%BF%E4%B8%8A%E9%97%AE%E9%A2%98/"},{"name":"SpringCloud","slug":"SpringCloud","permalink":"http://www.lights8080.com/tags/SpringCloud/"},{"name":"杂谈","slug":"杂谈","permalink":"http://www.lights8080.com/tags/%E6%9D%82%E8%B0%88/"},{"name":"美国个人支票","slug":"美国个人支票","permalink":"http://www.lights8080.com/tags/%E7%BE%8E%E5%9B%BD%E4%B8%AA%E4%BA%BA%E6%94%AF%E7%A5%A8/"},{"name":"Filebeat","slug":"Filebeat","permalink":"http://www.lights8080.com/tags/Filebeat/"},{"name":"Kibana","slug":"Kibana","permalink":"http://www.lights8080.com/tags/Kibana/"},{"name":"Logstash","slug":"Logstash","permalink":"http://www.lights8080.com/tags/Logstash/"},{"name":"JVM","slug":"JVM","permalink":"http://www.lights8080.com/tags/JVM/"}]}