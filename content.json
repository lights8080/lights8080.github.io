{"meta":{"title":"七路灯","subtitle":"人的一生应当有许多停靠站，但愿每一个站台都有一盏雾中的灯。","description":"人的一生应当有许多停靠站，但愿每一个站台都有一盏雾中的灯。","author":"七路灯","url":"http://www.lights8080.xyz","root":"/"},"pages":[{"title":"about","date":"2021-05-26T12:06:18.000Z","updated":"2021-05-27T06:17:19.000Z","comments":true,"path":"about/index.html","permalink":"http://www.lights8080.xyz/about/index.html","excerpt":"","text":"从事互联网开发，信奉终身成长的技术人。分享知识和记录成长。"},{"title":"contact","date":"2021-05-26T12:06:42.000Z","updated":"2021-05-26T12:07:00.000Z","comments":true,"path":"contact/index.html","permalink":"http://www.lights8080.xyz/contact/index.html","excerpt":"","text":""},{"title":"categories","date":"2021-05-26T12:04:49.000Z","updated":"2021-05-26T12:05:30.000Z","comments":true,"path":"categories/index.html","permalink":"http://www.lights8080.xyz/categories/index.html","excerpt":"","text":""},{"title":"tags","date":"2021-05-26T12:05:59.000Z","updated":"2021-05-26T12:06:12.000Z","comments":true,"path":"tags/index.html","permalink":"http://www.lights8080.xyz/tags/index.html","excerpt":"","text":""}],"posts":[{"title":"Elasticsearch-映射（Mapping）","slug":"Elasticsearch-搜索（Search-DSL）","date":"2021-04-08T03:27:15.000Z","updated":"2021-05-27T05:54:11.000Z","comments":true,"path":"2021/04/08/Elasticsearch-搜索（Search-DSL）/","link":"","permalink":"http://www.lights8080.xyz/2021/04/08/Elasticsearch-%E6%90%9C%E7%B4%A2%EF%BC%88Search-DSL%EF%BC%89/","excerpt":"Elasticsearch介绍查询搜索请求包含哪些选项，并介绍其中的Query DSL。包括语法说明、查询和过滤上下文、复合查询等和查询示例。 基于7.11版本。","text":"Elasticsearch介绍查询搜索请求包含哪些选项，并介绍其中的Query DSL。包括语法说明、查询和过滤上下文、复合查询等和查询示例。 基于7.11版本。 搜索请求是对Elasticsearch数据流或索引中的数据信息的请求，包括以下自定义选项： Query DSL（查询语法） Aggregations（分组聚合） Search multiple data streams and indices（多数据流和索引搜索） Paginate search results（分页查询） Retrieve selected fields（查询指定字段） Sort search results（排序） Run an async search（异步搜索）本文介绍其中的Query DSL。 查询特定语言（Query DSL - Domain Specific Language）Elasticsearch提供了基于JSON的丰富的查询特定语言来定义查询，包含两种类型的子句组成： Leaf query clauses：页查询。在特定的字段中查找特定值，如match、term和range查询 Compound query clauses：复合查询。包装其他的Leaf和Compound子查询，逻辑组合多个查询（bool、dis_max），或更改其行为（constant_score）。 查询和过滤上下文（Query and filter context）相关性得分（relevance scores）：Elasticsearch按相关性得分对匹配的搜索结果进行排序，该得分衡量每个文档与查询的匹配程度。相关性得分是一个正浮点数，在查询API的_score元数据字段中返回，分值越高，文档越相关。不同的查询类型可以计算不同的相关性得分，计算分数还取决于查询子句是运行在查询上下文还是过滤器上下文中。 查询上下文（Query context）：回答的是文档与该查询子句的匹配程度如何，主要用于计算文档相关性得分。 过滤器上下文（Filter context）：回答的是文档与该查询子句是否匹配，主要用于过滤结构化数据，不计算相关性得分。频繁的使用filter context将会被ES自动缓存，以提升性能。 Compound queries：复合查询 boolean：匹配和过滤，满足条件可以获得更高的得分 boosting：降低文档的得分，而不是排除 constant_score：固定值得分 dis_max：提升多个文档具有相同固定值得分 function_score：根据算法修改查询文档得分 1. boolean query用于匹配和筛选文档。bool查询是采用more-matches-is-better的机制，因此满足must和should子句的文档将获得更高的分值。 must：返回的文档必须满足此查询子句，参与分值计算 filter：返回的文档必须满足此查询子句，不参与分值计算，缓存结果 should：返回的文档可能满足此查询子句，参与分值计算 must_not：该查询子句必须不能出现在匹配的文档中，不参与分值计算，缓存结果 minimum_should_match：指定至少匹配几个should子句，若一个bool查询包含至少一个should子句且无must或filter子句，则默认值为1。 boost：提升权重 2. boosting目的是降低某些文档分值，而不是从结果中排除。 boosting计算相关性得分规则： 从符合positive子句的查询中获得原始的相关性得分 得分 乘上 negative_boost系数，获得最终得分 positive：必填，返回的文档必须匹配此查询。 negative：必填，降低匹配文档的分值。 negative_boost：必填，介于0~1.0之间的数 3. constant_score包装filter query并返回分值，分值等于boost参数。 filter：必填，返回索引文档都必须匹配的查询条件 boost：可选，指定分值，默认为1 4. dis_max用于提升多个字段中包含相同术语的文档分配更高的得分。dis_max计算相关性得分规则： 获得匹配子句中最高得分 其他匹配的子句得分 乘上 tie_treaker系数 将最高得分与其他匹配得分相加，获得最终得分 queries：必填，返回的文档必须匹配一个或多个查询条件，匹配的条件越多则分值越高 tie_breaker：可选，介于0~1.0之间的数，用于增加匹配文档的分值。默认为0 5. function_score允许修改查询文档的相关性得分，通过得分函数（function_score）在过滤后的文档集合上计算，获得最终得分。 query：指定查询条件，默认”match_all”: {} score_mode：计算分值的模式。multiply（默认）、sum、avg、first、max、min boost_mode：计算的分值与查询分值合并模式。multiply（默认）、replace（忽略查询分值）、sum、avg、max、min function_score：计算分值的函数。script_score（函数）、weight（权重）、random_score（0~1随机）、field_value_factor（字段因素） Full text queries：全文检索，查询已分析的文本字段 intervals：根据匹配项的顺序和接近程度返回文档 match：标准的全文查询，模糊匹配和短语接近查询 match_bool_prefix：分析其输入解析构造为bool query，最后一个词在前缀查询中使用 match_phrase：分析其输入解析为短语匹配查询 match_phrase_prefix：分析其输入解析为短语匹配查询，最后一个词在前缀查询中使用 multi_match：多个字段匹配查询 query_string：语法解析器查询 simple_query_string：更简单的语法解析器查询 Geo queries：坐标查询 geo_bounding_box：矩形查询 geo_distance：坐标点范围查询 geo_polygon：多边形查询 geo_shape：包括几何图形查询和指定地理形状相交点查询 Shape queries：像geo_shape一样，支持索引任意二维的几何图形功能Joining queries：连接查询 nested：嵌套类型查询 has_child：匹配子文档的字段，返回父文档。前提是同一索引中建立的父子关系 has_parent：匹配父文档的字段，返回所有子文档。前提是同一索引中建立的父子关系 parent_id：查询指定父文档的所有子文档。 Span queries：区间查询。精准控制多个输入词的先后顺序，已经多个关键词在文档中的前后距离 span_containing：区间列表查询 field_masking_span：允许跨越不同字段查询 span_first：跨度查询，匹配项必须出现在该字段的前N个位置 span_multi：Wraps a term, range, prefix, wildcard, regexp, or fuzzy query. span_near：接受多个跨度查询，顺序相同且指定距离之内 span_not：包装其他span query，排除与该文档匹配的所有文档 span_or：返回任意指定查询匹配的文档 span_term：等同于term query，可以和其他span query一起使用 span_within： Specialized queries：专业的查询 distance_feature：基于时间或坐标查询，越接近原点得分越高 more_like_this：按文本、文档和文档的集合查询 percolate：按存储的指定文档匹配查询 rank_feature：通过定义字段的rank_feature或rank_features属性值提高得分 script：脚本过滤文档 script_score：通过脚本自定义得分 wrapper：接受一个base64字符串查询 pinned：提升特定文档的查询 Term-level queries：术语级查询 exists：返回包含字段的任意文档 fuzzy：返回搜索词的相似词的文档 ids：返回指定ID的文档 prefix：返回字段中指定前缀的文档 range：返回范围内的文档 regexp：返回正则匹配的文档 term：返回字段中包含特定术语的文档 terms：返回字段中包含一个或多个术语的文档 terms_set：返回字段中包含最少数目术语的文档 type：返回指定类型的文档 wildcard：返回通配符匹配文档 查询示例1234567891011121314151617181920212223242526272829303132333435363738394041# 多索引同步搜索GET &#x2F;my-index-000001,my-index-000002&#x2F;_search&#123; # 指定查询超时时间 &quot;timeout&quot;: &quot;2s&quot;, # 字段匹配，相当于query context &quot;query&quot;: &#123; &quot;bool&quot;: &#123; &quot;must&quot;: [ &#123; &quot;match_all&quot;: &#123;&#125;&#125;, &#123; &quot;match&quot;: &#123; &quot;title&quot;: &quot;Search&quot; &#125;&#125; ], # 相当于filter context &quot;filter&quot;: [ &#123; &quot;term&quot;: &#123; &quot;status&quot;: &quot;published&quot; , &quot;_name&quot; : &quot;status_pub&quot;&#125;&#125;, &#123; &quot;range&quot;: &#123; &quot;publish_date&quot;: &#123; &quot;gte&quot;: &quot;2015-01-01&quot; &#125;&#125;&#125; ] &#125; &#125;, # 排序 &quot;sort&quot;: [ &#123; &quot;account_number&quot;: &quot;asc&quot; &#125;, &#123; &quot;post_date&quot; : &#123;&quot;order&quot; : &quot;asc&quot;&#125;&#125;, &quot;_score&quot; ], # 范围搜索 &quot;range&quot;: &#123; &quot;gmtCreate&quot;: &#123; &quot;gte&quot;: &quot;2020-10-01 00:00:00&quot;, &quot;lte&quot;: &quot;2020-10-31 23:59:59&quot;, &quot;format&quot;: &quot;yyyy-MM-dd HH:mm:ss&quot;, &quot;time_zone&quot;:&quot;+08:00&quot; &#125; &#125; # 查询指定字段 &quot;fields&quot;: [&quot;user.id&quot;, &quot;@timestamp&quot;], &quot;_source&quot;: false # 分页 &quot;from&quot;: 10, &quot;size&quot;: 10&#125;","categories":[{"name":"技术","slug":"技术","permalink":"http://www.lights8080.xyz/categories/%E6%8A%80%E6%9C%AF/"},{"name":"ELK","slug":"技术/ELK","permalink":"http://www.lights8080.xyz/categories/%E6%8A%80%E6%9C%AF/ELK/"}],"tags":[{"name":"技术","slug":"技术","permalink":"http://www.lights8080.xyz/tags/%E6%8A%80%E6%9C%AF/"},{"name":"ELK","slug":"ELK","permalink":"http://www.lights8080.xyz/tags/ELK/"},{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"http://www.lights8080.xyz/tags/Elasticsearch/"}]},{"title":"Elasticsearch-索引（Index）","slug":"Elasticsearch-索引（Index）","date":"2021-04-07T03:27:15.000Z","updated":"2021-05-27T05:53:35.000Z","comments":true,"path":"2021/04/07/Elasticsearch-索引（Index）/","link":"","permalink":"http://www.lights8080.xyz/2021/04/07/Elasticsearch-%E7%B4%A2%E5%BC%95%EF%BC%88Index%EF%BC%89/","excerpt":"Elasticsearch索引介绍，包括索引设置、索引模板、索引生命周期管理、翻滚索引。 基于7.11版本。","text":"Elasticsearch索引介绍，包括索引设置、索引模板、索引生命周期管理、翻滚索引。 基于7.11版本。 索引设置（Index Settings）static只能在创建索引时或关闭的索引上设置 index.number_of_shards：主分片数量，默认1 index.number_of_routing_shards：拆分索引的路由分片数量，默认值位于2~1024之间，依赖索引主分片数量 index.shard.check_on_startup：打开前检查分片是否损坏，默认false index.codec：压缩存储算法，默认LZ4 index.routing_partition_size：自定义路由可以到达的分片数量，默认1 dynamic可以使用API实时对索引进行操作 index.number_of_replicas：主分片的副本数，默认1 index.auto_expand_replicas：根据集群中数据节点的数量自动扩展副本的数量，默认false index.search.idle.after：搜索空闲之前不能接收搜索和获取请求的时间，默认30s index.refresh_interval：刷新操作频率，最近对索引的更改既可见，默认1s。-1关闭刷新操作 index.max_result_window：查询索引结果的最大数量，默认10000 index.max_inner_result_window：内部或聚合命中最大数量，默认100 index.max_rescore_window：打分请求的最大索引数量，默认10000（同index.max_result_window） index.max_docvalue_fields_search：查询中允许的最大字段数，默认100 index.max_script_fields：查询中允许的最大脚本字段数，默认32 index.query.default_field：查询返回的默认字段，默认*（表示所有） 索引模板（Index Templates）索引模板是告诉Elasticsearch在创建索引时如何配置索引的一种方法。对于数据流（data stream），索引模板配置是创建他们的后备索引。在创建索引之前先配置模板，模板设置将用作创建索引的基础。 模板有两种类型，索引模板（index templates）和组件模板（component templates）。 组件模板是可重用的构建块，用于配置映射（mappings）、设置（settings）和别名（alias）。使用组件模板来构造索引模板，但它们不会直接应用于索引。索引模板可以包含组件模板的集合，也可以直接指定设置，映射和别名。如果匹配多个模板，优先使用优先级最高的模板。 可以使用模拟API创建索引，确定最终的索引设置。POST /_index_template/_simulate。 注意事项： 如果新数据流或索引与多个索引模板匹配，则使用优先级最高的索引模板。 Elasticsearch内置了许多索引模板（如：metrics--,logs-*-*），每个模板的优先级是100。如果不想使用内置模板，请为您的模板分配更高的优先级。 如果显式设置创建索引，并且该索引与索引模板匹配，则创建索引请求中的设置将优先于索引模板中指定的设置。 索引模板仅在创建索引期间应用。索引模板的更改不会影响现有索引。 当可组合模板匹配给定索引时，它始终优先于旧模板。如果没有可组合模板匹配，则旧版模板可能仍匹配并被应用。 索引生命周期管理（Index Lifecycle Manager - ILM）配置索引生命周期管理策略，能够随着时间推移根据性能、弹性和保留要求自动的管理索引。 索引生命周期策略可以触发以下操作： 翻转（Rollover）：当现有索引达到一定分片大小，文档数或使用年限时，为翻转目标创建新索引。翻转目标可以是索引别名或数据流。 收缩（Shrink）：减少索引中主碎片的数量。 强制合并（Force merge）：手动触发合并以减少索引每个分片中的段数，并释放已删除文档所使用的空间。 冻结（Freeze）：将索引设为只读，并最大程度地减少其内存占用量。 删除（Delete）：永久删除索引，包括其所有数据和元数据。 使用ILM可以更轻松地管理热-温-冷体系结构中的索引，在使用时间序列数据时很常见（如日志和指标）。 索引生命周期（Index lifecycle）ILM定义了以下四个阶段（Phases） Hot：频繁的写入和查询 Warm：索引不在更新，仍然在查询 Cold：不再更新的索引，很少查询仍然可以搜索，查询较慢也没关系 Delete：不再需要的索引，可以安全的删除 索引的生命周期策略指定了应用于哪些阶段，每个阶段中执行什么操作，以及何时在两个阶段之间进行转换。 创建索引时可以手动应用生命周期策略。对于时间序列索引，需要将生命周期策略与用于在序列中创建新索引的索引模板相关联。当索引滚动时，不会自动将手动应用的策略应用于新索引。 阶段转换（phase transitions）ILM根据其年龄在整个生命周期中移动索引。要控制这些翻转的时间，请为每个阶段设置一个最小年龄。为了使索引移至下一阶段，当前阶段中的所有操作都必须完成，并且索引必须早于下一阶段的最小年龄。 最小年龄默认为0，这会导致ILM在当前阶段中的所有操作完成后立即将索引移至下一阶段。 如果索引具有未分配的分片并且集群运行状况为黄色，则索引仍可以根据其索引生命周期管理策略过渡到下一阶段。但是，由于Elasticsearch只能在绿色集群上执行某些清理任务，因此可能会有意外的副作用。 阶段执行（phase execution）ILM控制阶段中的动作的执行的顺序，以及哪些步骤是执行每个动作的必要索引操作。 当索引进入阶段后，ILM将阶段定义信息缓存在索引元数据中，这样可以确保索引政策更新不会将索引置于永远不退出阶段的状态。 ILM定期运行，检查索引是否符合策略标准，并执行所需的步骤。为了避免竞争情况，ILM可能需要运行多次执行，完成一项动作所需的所有步骤。这意味着即使indexs.lifecycle.poll_interval设置为10分钟并且索引满足翻转条件，也可能需要20分钟才能完成翻转。 阶段动作（phase actions）参考https://www.elastic.co/guide/en/elasticsearch/reference/7.11/ilm-index-lifecycle.html#ilm-phase-actions 索引生命周期动作（Index Lifecycle Actions） Allocate：将分片移动到具有不同性能特征的节点，并减少副本的数量。 Delete：永久删除索引。 Force merge：减少索引段的数量并清除已删除的文档。将索引设为只读。 Freeze：冻结索引以最大程度地减少其内存占用量。 Migrate：将索引分片移动到对应于当前 ILM 阶段的数据层。 Read only：阻止对索引的写操作。 Rollover：移动索引作为滚动别名的写索引，并开始索引到新索引。 Searchable snapshot：为配置库中的管理索引拍摄快照，并将其作为可搜索快照。 Set priority：降低索引在生命周期中的优先级，以确保首先恢复热索引。 Shrink：通过将索引缩小为新索引来减少主碎片的数量。 Unfollow：将关注者索引转换为常规索引。在Rollover、Shrink和Searchable snapshot操作之前自动执行。 Wait for snapshot：删除索引之前，请确保快照已存在。 ILM更新（Lifecycle policy updates）您可以通过修改当前策略或切换到其他策略的方式来更改管理索引或滚动索引集合的生命周期。 为确保策略更新不会将索引置于无法退出当前阶段的状态，进入这个阶段时，阶段定义会缓存在索引元数据中。如果策略更新可以安全的应用，ILM更新缓冲的阶段定义；如果不能，则使用缓冲阶段定义完成该阶段。 Rollover（翻转）在为日志或指标等时间序列数据编制索引时，不能无限期地写入单个索引。为了满足索引和搜索性能要求并管理资源使用，可以写入索引直到达到某个阈值，然后创建一个新索引并开始写入该索引。 使用滚动索引能够： 优化活跃的索引，以在高性能热节点上获得高接收速率。 针对热节点上的搜索性能进行优化。 将较旧的，访问频率较低的数据转移到价格较低的冷节点上。 根据您的保留政策，通过删除整个索引来删除数据。 我们建议使用数据流来管理时间序列数据。数据流自动跟踪写入索引，同时将配置保持在最低水平。数据流设计用于仅追加数据，其中数据流名称可用作操作（读取，写入，翻转，收缩等）目标。如果您的用例需要就地更新数据，则可以使用索引别名来管理时间序列数据。 自动翻转（automatic rollover）：ILM使您能够根据索引大小，文档数或使用年限自动翻转到新索引。触发翻转后，将创建一个新索引，将写入别名更新为指向新索引，并将所有后续更新写入新索引。与基于时间的过渡相比，基于大小，文档数或使用年限翻转至新索引更可取。在任意时间滚动通常会导致许多小的索引，这可能会对性能和资源使用产生负面影响。 数据流（Data streams）数据流用于跨多个索引存储仅追加的时间序列数据，同时提供一个用于请求的数据流名称。可以将索引和搜索请求直接提交到数据流。流自动将请求路由到存储流数据的索引。同样可以使用索引生命周期管理（ILM）来自动管理这些后备索引。数据流非常适合日志，事件，指标和其他连续生成的数据。 滚动索引（rollover index）当现有的索引满足您提供的条件（a list of conditions）时，滚动索引API会为滚动目标（rollover target）创建一个新的索引。当滚动目标是别名（alias）时，别名会指向新索引（当指向多个索引时，必须有一个索引设置is_write_index=true）当滚动目标是数据流（data stream）时，新索引会成为数据流的写索引，并生成一个增量 Rollover request12POST &#x2F;&lt;rollover-target&gt;&#x2F;_rollover&#x2F;&lt;target-index&gt;POST &#x2F;&lt;rollover-target&gt;&#x2F;_rollover&#x2F; Path parameters ：必填，现有的分配给目标索引的索引别名或数据流名称。 ：可选，用于创建和分配索引别名的目标索引名称。如果是数据流，则不允许使用此参数。如果是索引别名，则分配给以”-“和数字结尾的索引名称，如logs-000001。 Request body aliases：可选，索引别名 conditions：可选，滚动操作的触发条件 max_age：最大年龄 max_docs：最大文档数 max_size：最大索引大小 mappings：可选，映射配置 settings：可选，索引配置","categories":[{"name":"技术","slug":"技术","permalink":"http://www.lights8080.xyz/categories/%E6%8A%80%E6%9C%AF/"},{"name":"ELK","slug":"技术/ELK","permalink":"http://www.lights8080.xyz/categories/%E6%8A%80%E6%9C%AF/ELK/"}],"tags":[{"name":"技术","slug":"技术","permalink":"http://www.lights8080.xyz/tags/%E6%8A%80%E6%9C%AF/"},{"name":"ELK","slug":"ELK","permalink":"http://www.lights8080.xyz/tags/ELK/"},{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"http://www.lights8080.xyz/tags/Elasticsearch/"}]},{"title":"Elasticsearch-介绍","slug":"Elasticsearch-介绍","date":"2021-04-01T03:27:15.000Z","updated":"2021-05-27T05:51:51.000Z","comments":true,"path":"2021/04/01/Elasticsearch-介绍/","link":"","permalink":"http://www.lights8080.xyz/2021/04/01/Elasticsearch-%E4%BB%8B%E7%BB%8D/","excerpt":"Elasticsearch介绍，包括文档与索引、倒排索引、搜索和分析、可伸缩和弹性（节点、分片、跨集群复制）。 内容大部分源自官方文档第一节“What is Elasticsearch?” 基于7.11版本。","text":"Elasticsearch介绍，包括文档与索引、倒排索引、搜索和分析、可伸缩和弹性（节点、分片、跨集群复制）。 内容大部分源自官方文档第一节“What is Elasticsearch?” 基于7.11版本。 Elasticsearch是一个分布式搜索和分析引擎，为所有类型的数据提供了近实时的搜索和分析。不仅可以进行简单的数据探索，还可以汇总信息来发现数据中的趋势和模式。随着数据和查询量的增长，分布式特性可以使部署顺畅的无缝的增长。 1. 文档和索引文档（Document）Elasticsearch是分布式文档存储，它不会将信息存储为“行-列”数据结构，而是存储为JSON文档的数据结构。当一个集群中有多个节点时，存储的文档分布在集群中，并且可以从任何节点访问。 Elasticsearch使用倒排索引的数据结构，支持非常快速的全文本搜索。一个倒排索引由文档中所有不重复词的列表构成，对于其中每个词，有一个包含它的文档列表。 索引（Index）索引是一个逻辑命名空间，可以看作是文档的优化集合，每个文档都是字段的集合。默认Elasticsearch对每个字段中的所有数据建立索引，并且每个索引字段都具有专用的优化数据结构。如：文本字段存储在倒排索引中，数字、地理字段存储在BKD树中。 Elasticsearch还支持动态映射（dynamic mapping），自动检测并向索引添加新字段。可以定义规则来控制动态映射，也可以显式定义映射以完全控制字段的存储和索引方式。 显式定义映射的意义： 区分全文本字符串字段和精确值字符串字段 执行特定语言的文本分析 优化字段进行部分匹配 自定义日期格式 无法自动检测到的数据类型，如地理信息 为不同的目的以不同的方式对同一字段建立索引通常很有用。 倒排索引（inverted index）倒排索引的结构，适用于快速的全文（Text）搜索。一个倒排索引由文档中所有不重复词的列表构成，对于其中每个词，有一个包含它的文档列表。 倒排索引建立的是分词（Term）和文档（Document）之间的映射关系，在倒排索引中，数据是面向词而不是面向文档的。 Term（词）：精准值，foo、Foo是不相同的词 Text（文本）：非结构化文本，默认文本会被解析为词，这是索引中实际存储的内容 Elasticsearch索引存储结构图： 一个索引包含很多分片，一个分片是一个Lucene索引，它本身就是一个完整的搜索引擎，可以独立执行建立索引和搜索任务。 Lucene索引又由很多分段组成，每个分段都是一个倒排索引。 Elasticsearch每次refresh都会生成一个新的分段，其中包含若干文档的数据。 每个分段（Segment）内部，文档的不同字段被单独建立索引。每个字段的值由若干词（Term）组成。 词（Term）是原文本内容经过分词器处理和语言处理后的最终结果。 2. 搜索和分析搜索（Search）Elasticsearch提供了一个简单，一致的REST API，用于管理集群以及建立索引和搜索数据。REST APIs支持结构化查询、全文本查询和结合了两者的复杂查询。 结构化查询：类似于SQL构建的查询，按索引中搜索字段，然后按字段对匹配项进行排序。 全文本查询：查找到所有与查询字符串匹配的文档，按相关性对它们进行排序。 可以使用Elasticsearch的全面JSON风格的查询语言(Query DSL)访问所有这些搜索功能。还可以构造SQL风格的查询来在Elasticsearch内部本地搜索和聚合数据。 分析（Analyze）聚合使您能够构建数据的复杂摘要，并深入了解关键指标，模式和趋势。聚合利用了用于搜索的相同数据结构，速度很快，可以实时分析和可视化数据。聚合操作和搜索的请求在一起运行，可以在单个请求中同一时间相同的数据进行搜索文档，过滤结果并执行分析。 3. 可伸缩和弹性集群（Cluster）一个Elasticsearch集群由一个或多个节点（Node）组成，每个集群都有一个cluster name作为标识。 集群的三种状态： Green：所有主分片和副本分片都准备就绪。数据不会丢失 Yellow：所有主分片准备就绪，但至少一个主分片对应的副本分片没有就绪。意味着高可用和容灾能力下降 Red：至少有一个主分片没有就绪。此时查询的结果会出现数据丢失 Elasticsearch可以根据需要进行扩展，它天生就实现了分布式。你可以向集群添加节点以增加容量，它会自动将数据和查询负载分布到所有可用节点，不需要大改应用程序。 Elasticsearch索引只是一个或多个物理分片的逻辑分组，其中每个分片实际上是一个独立的的索引。通过将文档分布在索引中的多个分片上，并将这些分片分布在多个节点上，当集群增长(或缩小)时，Elasticsearch自动迁移分片以平衡集群。 节点（Node）节点是属于集群的运行实例，测试时可以一个服务器上启动多个节点，线上通常一个服务器只有一个实例。 启动时，节点将使用单播来发现具有相同集群名称的现有集群，并尝试加入。 候选主节点（Master-eligible Node） 主节点（Master Node） 数据节点（Data Node） 协调节点（Coordinating Node） 热节点（Hot Node） 冷节点（Warm Node） 预处理节点（Ingest Node） 分片（Shard）分片是单个Lucene实例。这是一个低级的工作单元，由ElasticSearch自动管理，在节点发生故障或新增时，可以自动的将分片从一个节点移动到另一个节点。分为主分片（primaries）和副本分片（replicas）两种类型。 主分片：建立索引时默认会有一个主分片，每个文档都属于一个主分片，创建索引后，主分片数量是固定的无法更改。 副本分片：每个主分片可以有零个或多个副本，可以随时更改，副本永远不会和主分片在同一节点上启动。副本提供数据的冗余副本，以防止硬件故障，并增加处理像搜索或检索文档这样的读请求的能力。 跨集群复制（Cross-cluster replication）出于性能原因，群集中的节点必须位于同一网络上，跨不同数据中心中的节点在群集中平衡分片的时间太长了。但是这不符合高可用架构的要求，跨群集复制(CCR)可以解决单个集群重大故障问题。 CCR提供了一种将主集群的索引自动同步到次要远程集群的方法，次要远程集群可以作为热备份。如果主集群失败，次要集群可以接管。您还可以使用CCR创建次要集群，以满足用户在地理位置上接近的读请求。主集群上的索引是Leader，负责所有写请求，复制到次要集群上的索引是只读的Follower。","categories":[{"name":"技术","slug":"技术","permalink":"http://www.lights8080.xyz/categories/%E6%8A%80%E6%9C%AF/"},{"name":"ELK","slug":"技术/ELK","permalink":"http://www.lights8080.xyz/categories/%E6%8A%80%E6%9C%AF/ELK/"}],"tags":[{"name":"技术","slug":"技术","permalink":"http://www.lights8080.xyz/tags/%E6%8A%80%E6%9C%AF/"},{"name":"ELK","slug":"ELK","permalink":"http://www.lights8080.xyz/tags/ELK/"},{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"http://www.lights8080.xyz/tags/Elasticsearch/"}]},{"title":"Elasticsearch-映射（Mapping）","slug":"Elasticsearch-映射（Mapping）","date":"2021-04-01T03:27:15.000Z","updated":"2021-05-27T05:52:27.000Z","comments":true,"path":"2021/04/01/Elasticsearch-映射（Mapping）/","link":"","permalink":"http://www.lights8080.xyz/2021/04/01/Elasticsearch-%E6%98%A0%E5%B0%84%EF%BC%88Mapping%EF%BC%89/","excerpt":"Elasticsearch映射介绍，包括动态映射、显式映射、字段数据类型、映射参数、映射限制设置。 内容大纲源自官方文档“Mapping”模块 基于7.11版本。","text":"Elasticsearch映射介绍，包括动态映射、显式映射、字段数据类型、映射参数、映射限制设置。 内容大纲源自官方文档“Mapping”模块 基于7.11版本。 映射（Mapping）是定义文档及其包含的字段如何存储和索引的过程。每个文档都是字段的集合，每个字段都有自己的数据类型。为数据创建一个映射定义，包含与文档相关的字段列表。 动态映射（Dynamic mapping）当Elasticsearch在文档中检测到新字段时，会动态将该字段添加到映射中称为动态映射。 动态字段映射：根据数据类型规则应用于动态添加的字段。支持的数据类型包括：boolean、double、integer、object、array、date。 dynamic：开启动态映射的模式 date_detection：开启日期检测 dynamic_date_formats：检测的日期格式 numeric_detection: true：开启数值检测 动态模板：又称自定义映射，根据匹配条件应用于动态添加的字段。 match_mapping_type: Elasticsearch检测到的数据类型进行操作 match and unmatch: 使用模式来匹配字段名称 path_match and path_unmatch: 使用字段的路径来匹配 显式映射（Explicit mapping）显式映射以完全控制字段的存储和索引方式。 显式映射的意义： 区分全文本字符串字段和精确值字符串字段 执行特定语言的文本分析 优化字段进行部分匹配 自定义日期格式 无法自动检测到的数据类型，如地理信息 字段数据类型（Field data types）常见的类型： binary：二进制编码为Base64字符串 boolean：布尔值 Keywords：关键字，通常用于过滤、排序和聚合。包括keyword、constant_keyword和wildcard。 Numbers：数值类型，包括long、integer、short、byte、double、float Dates：日期类型，包括date和date_nanos alias：为现有字段定义别名 对象和关系类型： object：JSON对象。扁平的键-值对列表 flattened：将整个JSON对象作为单个字段值 nested：保留其子字段之间关系的JSON对象，维护每个对象的独立性 join：为同一索引中的文档定义父/子关系 结构化数据类型： Range：范围，包括long_range, double_range, date_range, and ip_range ip：IPv4和IPv6地址 version：软件版本。特殊的关键字，用于处理软件版本值并支持它们的专用优先级规则 murmur3：计算和存储值的散列。提供了在索引时计算字段值的哈希并将其存储在索引中的功能 聚合数据类型： aggregate_metric_double：度量值进行聚合 histogram：柱状图，以直方图形式聚合数值 文本搜索类型： text：非结构化文本。配置分词器 annotated_text：注解文本。带有映射器注释的文本插件提供了索引文本的功能。如WWW与World Wide Web同义 completion：补全提示。是一个导航功能，引导用户在输入时查看相关结果，提高搜索精度 search_as_you_type：自定义搜索。类似文本的字段，经过优化提供开箱即用的按需输入搜索服务。如搜索框 token_count：符号计数。分词分析后对数量进行索引 文档排名类型： rank_feature：记录一个数字特性，以便在查询中增强文档。 空间数据类型： geo_point：经纬度 geo_shape：复杂的形状，如多边形 元数据： _index：文档所属的索引 _id：文档ID _doc_count：桶聚合（Bucket）返回字段，显示桶中已聚合和分区的文档数 _source：原始JSON文档 _size：_source字段的大小 _routing：自定义路由 _meta：元数据信息 映射参数（Mapping parameters） analyzer：text字段文本解析器 boots：增强匹配权重，默认1.0。如：title的匹配权重大于content匹配权重 coerce：强行类型转换，默认true。如：如string（“10”）强制转换为integer（10） copy_to：将多个字段的值复制到同一字段中。如：first_name和last_name，复制到full_name doc_values：开启字段的排序、聚合和脚本中访问字段，支持除了text和annotated_text的所有字段类型，默认true。本质上是列式存储，保持与原始文档字段相同的值，关闭可节省空间 dynamic：新检测到的字段添加到映射，默认true。false表示不建立索引，strict表示拒绝文档 eager_global_ordinals：全局序号映射。文档中字段的值仅存储序号，而不存原始内容，用于聚合时提高性能 enabled：尝试为字段建立索引，仅可应用于顶级映射和object类型下，默认true。如果禁用整个映射，意味着可以对其进行获取，但是不会以任何方式对它的内容建立索引 format：自定义日期格式 ignore_above：超过长度的字符串内容将不会被索引 ignore_malformed：忽略错误的数据类型插入到索引中。默认抛出异常并丢弃文档 index：控制是否对字段值建立索引，默认true。未索引的字段不可查询 index_options：控制哪些信息添加到倒排索引已进行搜索并突出显示，仅使用于文本字段 index_phrases：将两个词的组合词索引到一个单独的字段中。默认false index_prefixes：为字段值的前缀编制索引，以加快前缀搜索速度 meta：附加到字段的元数据 fields：为不同的目的以不同的方式对同一字段建立索引 norms：用于计算查询的文档分数，默认true。对于仅用于过滤或聚合的字段，不需要对字段进行打分排序时设置为false null_value：使用指定的值替换为null值，以便可以进行索引和搜索 position_increment_gap：当为具有多个值的文本字段建立索引时，将在值之间添加“假”间隙，以防止大多数短语查询在值之间进行匹配，默认值为100 properties：类型映射，object字段和nested字段包含子字段叫properties search_analyzer：查询时使用指定的分析器 similarity：字段打分的相似性算法，默认BM25 store：单独存储属性值。默认对字段值进行索引以使其可搜索，但不单独存储它们，但是已存储在_source字段中 term_vector：存储分析过程的词矢量（Term vectors）信息。包括：词、位置、偏移量、有效载荷 映射限制设置（mapping limit settings）索引中定义太多字段会导致映射爆炸，从而导致内存不足错误和难以恢复的情况。在动态映射中，如果每个新插入的文档都引入新字段，每个新字段都添加到索引映射中，随着映射的增长，这会成为一个问题。使用映射限制设置可以限制（手动或动态创建的）字段映射的数量，并防止文档引起映射爆炸。 index.mapping.total_fields.limit: 字段最大数限制，默认1000 index.mapping.depth.limit: 字段的最大深度，默认20 index.mapping.nested_fields.limit: 单个索引中嵌套类型（nested）最大数限制，默认50 index.mapping.nested_objects.limit: 单个文档中嵌套JSON对象的最大数限制，默认10000","categories":[{"name":"技术","slug":"技术","permalink":"http://www.lights8080.xyz/categories/%E6%8A%80%E6%9C%AF/"},{"name":"ELK","slug":"技术/ELK","permalink":"http://www.lights8080.xyz/categories/%E6%8A%80%E6%9C%AF/ELK/"}],"tags":[{"name":"技术","slug":"技术","permalink":"http://www.lights8080.xyz/tags/%E6%8A%80%E6%9C%AF/"},{"name":"ELK","slug":"ELK","permalink":"http://www.lights8080.xyz/tags/ELK/"},{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"http://www.lights8080.xyz/tags/Elasticsearch/"}]},{"title":"Logstash-配置","slug":"Logstash-配置","date":"2021-03-22T03:27:15.000Z","updated":"2021-05-27T03:47:46.000Z","comments":true,"path":"2021/03/22/Logstash-配置/","link":"","permalink":"http://www.lights8080.xyz/2021/03/22/Logstash-%E9%85%8D%E7%BD%AE/","excerpt":"Logstash配置介绍、插件说明、配置说明、高级配置、命令说明基于7.11版本。https://www.elastic.co/guide/en/logstash/7.11/index.html","text":"Logstash配置介绍、插件说明、配置说明、高级配置、命令说明基于7.11版本。https://www.elastic.co/guide/en/logstash/7.11/index.html 一、配置Logstash配置 Logstash，你需要创建一个配置文件来指定想要使用的插件和每个插件的设置。可以引用配置中的事件字段，并在事件满足某些条件时使用条件来处理它们。运行logstash时使用-f指定配置文件。 每种类型的插件都有一个单独的部分，每个部分都包含一个或多个插件的配置选项。如果指定多个过滤器，则会按照它们在配置文件中出现的顺序进行应用。 logstash-simple.conf 12345678910111213input &#123; stdin &#123; &#125; &#125;filter &#123; grok &#123; match =&gt; &#123; &quot;message&quot; =&gt; &quot;%&#123;COMBINEDAPACHELOG&#125;&quot; &#125; &#125; date &#123; match =&gt; [ &quot;timestamp&quot; , &quot;dd/MMM/yyyy:HH:mm:ss Z&quot; ] &#125;&#125;output &#123; elasticsearch &#123; hosts =&gt; [&quot;localhost:9200&quot;] &#125; stdout &#123; codec =&gt; rubydebug &#125;&#125; 1. 在配置中访问事件数据和字段Logstash中的某些配置选项需要使用事件的字段。因为输入会生成事件，所以输入块中没有要评估的字段，因为它们还不存在。 引用事件数据和字段仅在过滤器和输出块内起作用。基本语法是[fieldname]，引用顶级字段时可以去掉[]，引用嵌套字段时，要指定完成整路径[top-level field][nested field]。 sprintf格式化引用事件字段：increment =&gt; “apache.%{[response][status]}”引用事件日期和类型：path =&gt; “/var/log/%{type}.%{+yyyy.MM.dd.HH}” 条件只想在特定条件下过滤或输出事件，这时可以使用条件。 1234567if EXPRESSION &#123; ...&#125; else if EXPRESSION &#123; ...&#125; else &#123; ...&#125; 比较运算符:equality: ==, !=, &lt;, &gt;, &lt;=, &gt;=regexp: =, ! (checks a pattern on the right against a string value on the left)inclusion: in, not in 布尔运算符:and, or, nand, xor 一元运算符：!（取反） 还可以使用(...)，对表达式进行分组。 @metadata字段一个特殊的字段，在输出时不会成为任何事件的一部分。非常适用于做条件，扩展和构建事件字段等 二、插件说明Input plugins beats file kafka Filter plugins aggregate聚合属于同一任务的多个事件（通常是日志行）中的可用信息，最后将聚合的信息推送到最终任务事件中 clone克隆事件，原始事件保持不变。新事件作为正常的事件插入到管道中，并从生成事件的过滤器开始继续执行。 date解析字段中的日期，然后使用该日期或时间戳作为事件的Logstash的时间戳 Grok filter plugin解析任何文本并将其结构化。适用于文本的结构是逐行变化。%{SYNTAX:SEMANTIC}grok-patterns Dissect filter plugin适用于界定符拆分，不使用正则表达式，而且速度非常快。%{id-&gt;} %{function} %{+ts} %&#123;+ts&#125;：表示前面已经捕获到一个ts字段了，而这次捕获的内容，自动添补到之前ts字段内容的后面 -&gt;：表示忽略它右边的填充 %&#123;+key/2&#125;：表示在有多次捕获内容都填到key字段里的时候，拼接字符串的顺序谁前谁后。/2表示排第2位 %&#123;&#125;：表示是一个空的跳过字段 %&#123;?string&#125;：表示这块只是一个占位，并不会实际生成捕获字段存到事件里面 %&#123;?string&#125; %&#123;&amp;string&#125;：表示当同样捕获名称都是string，但是一个?一个&amp;的时候，表示这是一个键值对 drop删除到达此过滤器的所有内容 elapsed跟踪一对开始/结束事件，并使用它们的时间戳来计算它们之间的经过时间 elasticsearch在Elasticsearch中搜索上一个日志事件，并将其中的某些字段复制到当前事件中 fingerprint创建一个或多个字段的一致哈希（指纹），并将结果存储在新字段中 geoip根据来自Maxmind GeoLite2数据库的数据添加有关IP地址地理位置的信息 http提供了与外部Web服务/ REST API的集成。 java_uuid允许您生成UUID并将其作为字段添加到每个已处理事件 uuiduuid过滤器允许您生成UUID并将其作为字段添加到每个已处理事件。 jdbc_static通过从远程数据库预加载的数据来丰富事件 jdbc_streaming执行SQL查询，并将结果集存储在指定为目标的字段中。它会将结果本地缓存到过期的LRU缓存中 json这是一个JSON解析过滤器。它采用一个包含JSON的现有字段，并将其扩展为Logstash事件内的实际数据结构 kv有助于自动解析foo=bar种类的消息（或特定事件字段） metrics对于汇总指标很有用 mutate可以重命名，删除，替换和修改事件中的字段。需要注意的是，一个mutate块中的命令执行是有序的coerce -&gt; ... -&gt; copy，可以使用多个mutate块控制执行顺序。coerce: 设置空字段的默认值replace: 从事件中的其他部分构件一个新值，替换掉已有字段strip: 删除字段的前后空格update: 用新值更新现有字段，该字段不存在不采取任何操作gsub: 正则表达式匹配，将所有的匹配项更新为替换的字符串，只支持字符串和字符串数组，其他类型不采取任何操作join: 用分隔符连接数组，非数组字段不采取任何操作convert: 将字段的值转换为其他类型，如字符串转整数 prune用于根据字段名称或其值的白名单或黑名单从事件中删除字段（名称和值也可以是正则表达式）。如果使用json/kv过滤器解析出来一些不是事先知道的字段，只想保留其中一部分，这个功能很有用 range用于检查某些字段是否在预期的大小/长度范围内。支持的类型是数字和字符串。当在指定范围内时，执行一个操作。 ruby接受嵌入式Ruby代码或Ruby文件 sleep睡眠一定时间。这将导致logstash在给定的时间内停止运行。这对于速率限制等很有用 throttle节流过滤器用于限制事件数量 translate使用配置的哈希或文件确定替换值的常规搜索和替换工具。当前支持的是YAML，JSON和CSV文件。每个字典项目都是一个键值对 truncate允许您截断长度超过给定长度的字段 urldecode解码经过urlencoded的字段 useragentUserAgent过滤器，添加有关用户代理的信息，例如家族，操作系统，版本和设备 xmlXML过滤器。获取一个包含XML的字段，并将其扩展为实际的数据结构 Output plugins elasticsearch file stdout exec 三、配置说明包括：logstash.yaml、pipelines.yml、jvm.options、log4j2.properties、startup.options logstash.yml Logstash配置选项可以控制Logstash的执行。如：指定管道设置、配置文件位置、日志记录选项等。运行Logstash时，大多数配置可以命令行中指定，并覆盖文件的相关配置。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162node.name: `hostname`path.data: LOGSTASH_HOME/datapath.logs: LOGSTASH_HOME/logs# 指定main pipeline的配置文件路径path.config: # 指定main pipeline的配置数据。语法同配置文件config.string: # 开启后，检查配置是否有效，然后退出config.test_and_exit: false# 开启后，修改配置文件自动加载，过程：暂停管道所有输入；创新新管道并检验配置；检查成功切换到新管道，失败则继续使用老的管道。config.reload.automatic: false# 检查配置文件更新的时间间隔config.reload.interval: 3s# 内部队列模型，memory(default)：内存，persisted：磁盘queue.type: memory# 持久队列的数据文件存储路径（queue.type: persisted时启用）path.queue: path.data/queue# 持久队列的页容量，持久化以页为单位queue.page_capacity: 64mb# 开启后，关闭logstash之前等待持久队列消耗完毕queue.drain: false# 队列中允许的最大事件数，默认0表示无限制queue.max_events: 0# 事件缓冲的内部队列的总容量，达到限制时Logstash将不再接受新事件queue.max_bytes: 1024mb# 强制执行检查点之前的最大ACKed事件数queue.checkpoint.acks: 1024# 强制执行检查点之前，可以写入磁盘的最大事件数queue.checkpoint.writes: 1024# 对每次检查点写入失败将重试一次queue.checkpoint.retry: false# metrics REST endpoint绑定的地址和端口http.host: &quot;127.0.0.1&quot;http.port: 9600# 工作线程IDpipeline.id: main# 控制事件排序，auto：如果`pipeline.workers: 1`开启排序。true：如果有多个工作线程，强制对管道进行排序，并防止Logstash启动。false：禁用排序所需的处理，节省处理成本。pipeline.ordered: auto# 管道筛选和输出阶段的工作线程数，CPU没有饱和可以增加此数字更好的利用机器处理能力。pipeline.workers: `number of cpu cores`# 单个工作线程在发送到filters+workers之前，从输入中获取的最大事件数pipeline.batch.size: 125# 将小批量事件派送到filters+outputs之前，轮询下一个事件等待毫秒时间，可以理解为未到达批处理最大事件数时延迟发送时间pipeline.batch.delay: 50# 开启后，每个pipeline分割为不同的日志，使用pipeline.id作为文件名pipeline.separate_logs: false# 开启后，强行退出可能会导致关机期间丢失数据pipeline.unsafe_shutdown: false# 启用死信队列，默认falsedead_letter_queue.enable: falsedead_letter_queue.max_bytes: 1024mbpath.dead_letter_queue: path.data/dead_letter_queue# 指定自定义插件的位置path.plugins: # 配置模块，遵循yaml结构modules: 四、高级配置1. 多管道配置（multiple pipelines configuration）如果需要在同一个进程中运行多个管道，通过配置pipelines.yml文件来处理，必须放在path.settings文件夹中。并遵循以下结构： 1234567# config/pipelines.yml- pipeline.id: my-pipeline_1 path.config: &quot;/etc/path/to/p1.config&quot; pipeline.workers: 3- pipeline.id: my-other-pipeline path.config: &quot;/etc/different/path/p2.cfg&quot; queue.type: persisted 不带任何参数启动Logstash时，将读取pipelines.yml文件并实例化该文件中指定的所有管道。如果使用-e或-f时，Logstash会忽略pipelines.yml文件并记录相关警告。 如果当前的配置中的事件流不共享相同的输入/过滤器和输出，并且使用标签和条件相互分隔，则使用多个管道特别有用。 在单个实例中具有多个管道还可以使这些事件流具有不同的性能和持久性参数（例如，工作线程数和持久队列的不同设置）。 2. 管道到管道的通信（pipeline-to-pipeline Communication）使用Logstash的多管道功能时，可以在同一Logstash实例中连接多个管道。此配置对于隔离这些管道的执行以及有助于打破复杂管道的逻辑很有用。 3. 重新加载配置文件（Reloading the Config File）如果没有开启自动重新加载（–config.reload.automatic），可以强制Logstash重新加载配置文件并重新启动管道。 1kill -SIGHUP 14175 4. 管理多行事件（Managing Multiline Events）5. Glob模式支持（glob pattern support）注意Logstash不会按照glob表达式中编写的文件顺序执行，是按照字母顺序对其进行排序执行的。 6. Logstash到Logstash通讯（Logstash-to-Logstash Communication）7. Ingest Node解析数据转换到Logstash解析数据（Converting Ingest Node Pipelines）8. 集中配置管理（Centralized Pipeline Management）五、命令说明命令行上设置的所有参数都会覆盖logstash.yml中的相应设置。生产环境建议使用logstash.yml控制Logstash执行。 参数： –node.name NAME：指定Logstash实例的名称，默认当前主机名 -f, –path.config CONFIG_PATH：加载Logstash配置的文件或目录 -e, –config.string CONFIG_STRING：Logstash配置数据，如果未指定输入，则使用input &#123; stdin &#123; type =&gt; stdin &#125; &#125;作为默认的输入，如果未指定输出，则使用output &#123; stdout &#123; codec =&gt; rubydebug &#125; &#125;作为默认的输出。 -M “CONFIG_SETTING=VALUE”：覆盖指定的配置 –config.test_and_exit: 检查配置是否有效，然后退出 –config.reload.automatic: 修改配置文件自动加载 –modules MODULE_NAME：指定运行的模块名称 –setup：是一次性设置步骤，在Elasticsearch中创建索引模式并导入Kibana仪表板和可视化文件。 … 示例： 1bin/logstash -f logstash-simple.conf --config.reload.automatic","categories":[{"name":"技术","slug":"技术","permalink":"http://www.lights8080.xyz/categories/%E6%8A%80%E6%9C%AF/"},{"name":"ELK","slug":"技术/ELK","permalink":"http://www.lights8080.xyz/categories/%E6%8A%80%E6%9C%AF/ELK/"}],"tags":[{"name":"技术","slug":"技术","permalink":"http://www.lights8080.xyz/tags/%E6%8A%80%E6%9C%AF/"},{"name":"ELK","slug":"ELK","permalink":"http://www.lights8080.xyz/tags/ELK/"},{"name":"Logstash","slug":"Logstash","permalink":"http://www.lights8080.xyz/tags/Logstash/"}]},{"title":"Logstash-介绍","slug":"Logstash-介绍","date":"2021-03-19T03:27:15.000Z","updated":"2021-05-27T03:45:55.000Z","comments":true,"path":"2021/03/19/Logstash-介绍/","link":"","permalink":"http://www.lights8080.xyz/2021/03/19/Logstash-%E4%BB%8B%E7%BB%8D/","excerpt":"本文内容是通过官网学习Logstash的一个总结，阅读本文可以对Logstash有个整体的认识。包括Logstash介绍、如何工作、事件模型、工作原理、弹性数据、持久化队列、性能优化、部署和扩展等基于7.11版本。https://www.elastic.co/guide/en/logstash/7.11/index.html","text":"本文内容是通过官网学习Logstash的一个总结，阅读本文可以对Logstash有个整体的认识。包括Logstash介绍、如何工作、事件模型、工作原理、弹性数据、持久化队列、性能优化、部署和扩展等基于7.11版本。https://www.elastic.co/guide/en/logstash/7.11/index.html 介绍Logstash是具有实时流水线能力的开源的数据收集引擎。Logstash可以动态统一不同来源的数据，并将数据标准化到您选择的目标输出。它提供了大量插件，可帮助我们解析，丰富，转换和缓冲任何类型的数据。 如何工作管道（Logstash Pipeline）是Logstash中独立的运行单元，每个管道都包含两个必须的元素输入（input）和输出（output），和一个可选的元素过滤器（filter），事件处理管道负责协调它们的执行。输入和输出支持编解码器，使您可以在数据进入或退出管道时对其进行编码或解码，而不必使用单独的过滤器。如：json、multiline等 inputs（输入阶段）：会生成事件。包括：file、kafka、beats等 filters（过滤器阶段）：可以将过滤器和条件语句结合使用对事件进行处理。包括：grok、mutate等 outputs（输出阶段）：将事件数据发送到特定的目的地，完成了所以输出处理，改事件就完成了执行。如：elasticsearch、file等 Codecs（解码器）：基本上是流过滤器，作为输入和输出的一部分进行操作，可以轻松地将消息的传输与序列化过程分开。 1. 工作原理Logstash管道中每个输入阶段都运行在自己的线程中，输入将事件写入到内存或磁盘的中央队列。每个管道工作线程（pipeline worker）从队列中获取一批事件，通过配置的过滤器运行这批事件，然后将过滤的事件运行到所有输出。批处理的大小和工作线程数可以通过pipeline.batch.size和pipeline.workers进行配置。 默认Logstash在管道各阶段之间使用内存队列来缓存事件，如果发生意外的终止，则内存中的事件都将丢失。为了防止数据丢失，可以启用Logstash配置queue.type: persisted将正在运行的事件持久保存到磁盘。 2. 事件顺序默认Logstash不保证事件顺序，重新排序可以发送在两个地方： 批处理中的事件可以在过滤器处理期间重新排序 当一个或多个批次的处理速度快于其他批次时，可以对批次重新排序 当维护事件顺序非常重要时，排序设置： 设置pipeline.ordered: auto且设置pipeline.workers: 1，则自动启用排序。 设置pipeline.ordered: true，这种方法可以确保批处理是一个一个的执行，并确保确保事件在批处理中保持其顺序。 设置pipeline.ordered: false则禁用排序处理，但可以节省排序所需的成本。 Logstash 模块Logstash Module提供了一种快速的端到端的解决方案，用于提取数据并使用专用仪表盘对其进行可视化。 每个模块都内置了Logstash配置、Kibana仪表盘和其他元文件。使您可以更轻松地为特定用例或数据源设置Elastic Stack。 为了更轻松的上手，Logstash Module提供了三种基本功能，运行模块时将执行以下步骤： 创建ElasticSearch索引 设置Kibana仪表盘和可视化数据所需要的索引模式，搜索和可视化。 使用配置运行Logstash pipeline 弹性数据当数据流过事件处理管道时，Logstash可能会遇到阻止其事件传递到输出的情况。如：意外的数据类型或异常终止。为了防止数据丢失并确保事件不中断的流过管道，Logstash提供了两种功能。 持久队列（persistent queues） 死信队列（dead letter queues-DLQ） 持久队列默认Logstash在管道阶段（inputs-&gt;pipeline worker）之间使用内存中有边界队列来缓冲事件。这些内存队列的大小是固定的，并且不可配置。如果Logstash遇到暂时的计算机故障，那内存队列中的数据将丢失。 为了防止异常终止期间的数据丢失，Logstash具有持久队列功能，该功能将消息队列存储在磁盘上，提供数据的持久性。持久队列对于需要大缓冲区的Logstash部署也很有用，无需部署和管理消息代理（Kafka、Redis等）以促进缓冲的发布-订阅模型，可以启用持久队列在磁盘上缓冲消息并删除消息代理。 使用queue.max_bytes可配置磁盘上队列的总容量，当队列已满时，Logstash向输入施加压力阻止数据流入，不再接受新事件，这种机制有助于在输入阶段控制数据流速，不会压倒性的到输出。 持久队列的好处： Logstash异常终止或重启启动时避免数据丢失，将消息存储在磁盘上，直到传递至少成功一次。 无需使用Kafka外部缓冲消息代理。应对大缓冲区和吸收突发事件。 无法解决的问题： 永久性机器故障（如磁盘损坏），持久队列无法防止数据丢失。具有确认能力的Beats和http之类的插件，将受到持久队列的良好保护。 不使用请求-响应协议的输入插件（如TCP、UDP），持久队列无法防止数据丢失。 工作原理 队列位于输入和过滤器阶段之间：input → queue → filter + output。 当输入阶段可处理事件时将事件写入队列，成功写入后，输入可以向数据源发送确认（acknowledgement）。 处理队列中的事件时，Logstash仅在过滤器和输出已完全处理该事件时，该事件才记录（队列保留管道已处理的事件记录）为已处理（acknowledged/ACKed）- 这意味着该事件已由所有已配置的过滤器和输出处理。 在正常关闭时，Logstash将停止从队列读取数据，并将完成正在由过滤器和输出处理中的事件。重启后，Logstash将恢复处理持久队列中的事件，并接受来自输入的新事件。 如果Logstash异常终止，任何运行中的事件都不会被记录为ACKed，并且在Logstash重新启动时将被过滤器和输出重新处理。Logstash在批处理事件，当发生异常终止时，可能有一些批处理已经成功完成，但没有记录为 ACKed。 页队列本身就是一个页（page）集合，分为头页（head page）和尾页（tail page），仅有一个头页，达到具体大小（queue.page_capacity）时将变成尾页，并创建一个新的头页。尾页是不可变的，头页是仅追加的。每个页都是一个文件，页中的所有事件确认后，将被删除，如果较旧的页中至少有一个未被确认，整个页将保留在磁盘上，直到成功处理该页上的所有事件为止。 检查点启用持久队列功能后，Logstash通过一种称为检查点（checkpoint）的机制提交到磁盘。检查点文件在单独文件中记录有关自身的详细信息（页信息，确认等）。当记录检查点时，Logstash将调用头页的同步操作和以原子的方式将队列的当前状态写入磁盘。检查点的过程是原子的，意味着如果成功，将保存对文件的任何修改。如果Logstash终止，或者出现硬件级别的故障，则持久队列中缓冲但尚未提交检查点的所有数据都将丢失。可以通过设置queue.checkpoint.writes，强制Logstash更频繁地检查点。为了确保最大的持久性避免丢失数据，可以设置queue.checkpoint.writes为1，在每次事件后强制执行检查点。 死信队列死信队列提供了另一层数据弹性。（当前仅对Elasticsearch输出支持死信队列，用于响应码为400和404的文档，二者均表示无法重试的事件。）默认情况，当Logstash遇到由于数据错误而无法处理事件时，会挂起或删除失败的事件。为了防止数据丢失，可以配置将未成功的事件写入死信队列，而不是丢弃。写入死信队列的每个事件包括原始事件、无法处理的原因、写入事件的插入信息以及事件时间戳。要处理死信队列的事件，需要创建一个管道配置，使用dead_letter_queue插件从死信队列中读取数据。 工作原理 Elasticsearch无法访问的HTTP请求失败，Elasticsearch输出插件将无限期的重试整个请求，这些场景中死信队列不会拦截。 部署和扩展从操作日志和指标分析到企业和应用程序搜索，Elastic Stack可用于大量用例。确保将数据可扩展、持久和安全地传输到Elasticsearch极为重要，尤其是对于关键任务环境。本文重点介绍Logstash的常见体系结构模型，以及如何随着部署的增长而有效的扩展。重点放在操作日志、指标、安全分析用例上，因为它们往往需要大规模部署。 Beats to Elasticsearch使用Filebeat Modules，可以快速的收集、解析和索引流行的日志类型和预建的Kibana仪表盘。这种情况下Beats会将数据直接发送到ES，由摄取节点处理并索引数据。 Beats and Logstash to ElasticsearchBeats和Logstash共同提供了可扩展且具有弹性的全面解决方案。Beats运行在数千台边缘主机服务器上，将日志收集、拖尾和传输到Logstash。Logstash是水平可伸缩的，可以形成运行同一管道的节点组。Logstash的自适应缓冲功能即使在吞吐量变化不定的情况下也有助于流畅的传输。如果Logstash成为瓶颈，只需要添加更多节点即可进行横向扩展。以下是一些建议： 扩展： Beats应该在一组Logstash节点之间实现负载均衡 建议至少使用两个Logstash节点已实现高可用性 通常每个Logstash节点仅部署一个Beats输入，但也可以为每个Logstash节点部署多个Beats输入。 弹性： 使用Filebeat/Winlogbeat进行日志收集时，可以保证至少一次交付 从Filebeat/Winlogbeat到Logstash，以及从Logstash到Elasticsearch这两种通讯协议都是同步且支持确认。其他的Beats不支持。 处理： Logstash通常将使用grok或dissect提取字段，增强地理信息，并可以进一步利用文件、数据库或Elasticsearch查找数据集来丰富事件。 处理复杂性会影响整体吞吐量和CPU利用率，确保检查其他可用的过滤器插件。 Integrating with Messaging Queues如果现有的基础架构中有消息队列，那么将数据放入Elastic Stack会很容易。如果仅使用消息队列用于Logstash缓冲数据，建议使用Logstash持久队列，消除不必要的复杂性。 性能调优包括性能故障排除和调优和分析Logstash性能。 JVM 建议堆的大小应不小于4G且不大于8G，太小会导致JVM不断的进行垃圾回收，造成增加CPU利用率 堆的大小不要超过物理内存量的水平，必须保留一些内存以运行OS和其他进程，一般不要超过物理内存的50-75％。 将最小（Xms）和最大（Xmx）堆分配大小设置为相同的值，以防止在运行时调整堆大小，这是一个非常昂贵的过程。 调优和分析Logstash性能Logstash提供了以下选项来优化管道性能，pipeline.workers，pipeline.batch.size和pipeline.batch.delay。 pipeline.workers此设置确定要运行多少个线程以进行过滤和输出处理。如果发现事件正在备份或者CPU没有饱和可以考虑增加此参数以更好的利用可用的处理能力。 pipeline.batch.size此设置定义单个工作线程在尝试执行过滤器和输出之前收集的最大事件数。较大的批处理大小通常更有效，但会增加内存开销。某些硬件配置要求您增加jvm.options配置文件中的JVM堆空间，以避免性能下降。由于频繁的垃圾收集或与内存不足异常相关的JVM崩溃，超出最佳范围的值会导致性能下降。输出插件可以将每个批次作为逻辑单元进行处理。例如，Elasticsearch输出为收到的每个批次发出批量请求。调整pipeline.batch.size设置可调整发送到Elasticsearch的批量请求的大小。 pipeline.batch.delay很少需要调整。此设置调整Logstash管道的延迟。管道批处理延迟是Logstash在当前管道工作线程中接收到事件后等待新消息的最长时间（以毫秒为单位）。经过这段时间后，Logstash开始执行过滤器和输出。Logstash在接收事件和在过滤器中处理该事件之间等待的最长时间是pipeline.batch.delay和pipeline.batch.size设置的乘积。 管道的配置和优化进行中事件的总数由pipeline.workers和pipeline.batch.size设置的乘积确定。注意在间歇地不规则的接收大型事件的管道，需要足够的内存来处理这些峰值。可以将工作线程数设置高于CPU内核数，因为输出通常度过空闲时间在I/O等待条件下。","categories":[{"name":"技术","slug":"技术","permalink":"http://www.lights8080.xyz/categories/%E6%8A%80%E6%9C%AF/"},{"name":"ELK","slug":"技术/ELK","permalink":"http://www.lights8080.xyz/categories/%E6%8A%80%E6%9C%AF/ELK/"}],"tags":[{"name":"技术","slug":"技术","permalink":"http://www.lights8080.xyz/tags/%E6%8A%80%E6%9C%AF/"},{"name":"ELK","slug":"ELK","permalink":"http://www.lights8080.xyz/tags/ELK/"},{"name":"Logstash","slug":"Logstash","permalink":"http://www.lights8080.xyz/tags/Logstash/"}]}],"categories":[{"name":"技术","slug":"技术","permalink":"http://www.lights8080.xyz/categories/%E6%8A%80%E6%9C%AF/"},{"name":"ELK","slug":"技术/ELK","permalink":"http://www.lights8080.xyz/categories/%E6%8A%80%E6%9C%AF/ELK/"}],"tags":[{"name":"技术","slug":"技术","permalink":"http://www.lights8080.xyz/tags/%E6%8A%80%E6%9C%AF/"},{"name":"ELK","slug":"ELK","permalink":"http://www.lights8080.xyz/tags/ELK/"},{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"http://www.lights8080.xyz/tags/Elasticsearch/"},{"name":"Logstash","slug":"Logstash","permalink":"http://www.lights8080.xyz/tags/Logstash/"}]}